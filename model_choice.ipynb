{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASHION MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./fashion-mnist_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      2       0       0       0       0       0       0       0       0   \n",
       "1      9       0       0       0       0       0       0       0       0   \n",
       "2      6       0       0       0       0       0       0       0       5   \n",
       "3      0       0       0       0       1       2       0       0       0   \n",
       "4      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9    ...     pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0        30        43   \n",
       "3       0    ...            3         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         1         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Columns: 785 entries, label to pixel784\n",
      "dtypes: int64(785)\n",
      "memory usage: 359.3 MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./fashion-mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>103</td>\n",
       "      <td>87</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>53</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>53</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>137</td>\n",
       "      <td>126</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>224</td>\n",
       "      <td>222</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      0       0       0       0       0       0       0       0       9   \n",
       "1      1       0       0       0       0       0       0       0       0   \n",
       "2      2       0       0       0       0       0       0      14      53   \n",
       "3      2       0       0       0       0       0       0       0       0   \n",
       "4      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9    ...     pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       8    ...          103        87        56         0         0   \n",
       "1       0    ...           34         0         0         0         0   \n",
       "2      99    ...            0         0         0         0        63   \n",
       "3       0    ...          137       126       140         0       133   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2        53        31         0         0         0  \n",
       "3       224       222        56         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.iloc[:, 1:].values\n",
    "y_train = df_train.iloc[:, 0].values\n",
    "X_test = df_test.iloc[:, 1:].values\n",
    "y_test = df_test.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(x):\n",
    "    x = x.reshape(28, 28)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(x, cmap=plt.cm.Blues, interpolation='nearest')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some pretty examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112cc9208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n60tRpXhUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112af8320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSIVQA5DK/wP+/6zFOyZFqQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cf24f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 0, 7, 109 me gusta \n",
    "\n",
    "for i in [0, 7, 109]:\n",
    "    show_image(X_train[i, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the data is uniform distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBoAkdcoAkKROGQCS1CkDQJI69X/U+JMP5yW85QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106348208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_train, rwidth=0.5)\n",
    "plt.show()SS,."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some image transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_clothes(k):\n",
    "    k_index = y_train == k\n",
    "    mean_cl = np.mean(X_train[k_index, :], axis=0)\n",
    "    show_image(mean_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10634ea20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAEpSlnoyViiKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107ea4a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107e9a630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107f0e908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112ad4f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ceecdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1080257b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noQagFEINQCmEGoBSCDUApRBqAEr5X3GDj2/VRmw6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cf11a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ni0WBlyoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105186588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107e43ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(10):    \n",
    "    mean_clothes(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_image(x):\n",
    "    x = x.reshape(28, 28)\n",
    "    x = np.fliplr(x)\n",
    "    return x.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8f16zj0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11124c748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11124fd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11577b5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlEKoASiFUANQCqEGoJT/Az8Hard+IZvRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113984400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCDUAqRBqAFIh1ACkQqgBSOX/AeTFLFTkzvl9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112d17668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11173e710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in [0, 1, 1264]:\n",
    "    show_image(flip_image(X_train[i, :]))\n",
    "    show_image(X_train[i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/fmder/e28813c1e8721830ff9c\n",
    "\n",
    "import numpy\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "def elastic_transform(image, alpha, sigma, random_state=None):\n",
    "    \"\"\"Elastic deformation of images as described in [Simard2003]_.\n",
    "    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n",
    "       Convolutional Neural Networks applied to Visual Document Analysis\", in\n",
    "       Proc. of the International Conference on Document Analysis and\n",
    "       Recognition, 2003.\n",
    "    \"\"\"\n",
    "    if random_state is None:\n",
    "        random_state = numpy.random.RandomState(None)\n",
    "    else:\n",
    "        random_state = numpy.random.RandomState(random_state)\n",
    "\n",
    "    shape = image.shape\n",
    "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "\n",
    "    x, y = numpy.meshgrid(numpy.arange(shape[0]), numpy.arange(shape[1]))\n",
    "    indices = numpy.reshape(y+dy, (-1, 1)), numpy.reshape(x+dx, (-1, 1))\n",
    "\n",
    "    return map_coordinates(image, indices, order=1).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAFEpJREFUeJzt3X2MnWWZx/Hf1Zm+Mi20nVL7Rluh\naJDVUkZQQayAim8BNyvKuoTdJVtDIAsrayToRnazRtYA8s/GWJdGTBCjCwhGorBYF2WxMC219MVC\nKW8t084M0Feg7bTX/jEHM5AO99VzzsyZc/H9JM2ceeaa+7nPPDO/Puc5930/5u4CgCxGNboDAFBP\nhBqAVAg1AKkQagBSIdQApEKoAUiFUAOQCqEGIBVCDUAqrcO5s/b2dp87d95w7hJAEqtWrex192ml\numENtblz5+mhFZ3DuUsASYwfbc9G6mp6+Wlm55nZRjPbZGbX1NIWANRD1aFmZi2S/lPSJyWdJOki\nMzupXh0DgGrUcqZ2mqRN7r7Z3fdL+omk8+vTLQCoTi2hNkvS8wM+31LZ9gZmtsTMOs2ss6e3p4bd\nAUDZkA/pcPel7t7h7h3T2otvXABATWoJta2S5gz4fHZlGwA0TC2h9qikBWY238zGSPqipHvq0y0A\nqE7V49Tcvc/MrpD0a0ktkpa5+7q69QwAqlDT4Ft3v1fSvXXqCwDUjLmfAFIh1ACkQqgBSIVQA5AK\noQYgFUINQCqEGoBUCDUAqRBqAFIh1ACkQqgBSIVQA5AKoQYgFUINQCqEGoBUCDUAqRBqAFIh1ACk\nQqgBSIVQA5AKoQYgFUINQCqEGoBUCDUAqRBqAFIh1ACkQqgBSIVQA5AKoQYgFUINQCqEGoBUCDUA\nqRBqAFIh1ACkQqgBSIVQA5AKoQYgFUINQCqEGoBUWmv5ZjN7RtJuSQcl9bl7Rz06BQDVqinUKj7q\n7r11aAcAasbLTwCp1BpqLuk+M1tpZksOV2BmS8ys08w6e3p7atwdALy1WkPtTHdfJOmTki43s7Pe\nXODuS929w907prVPq3F3APDWago1d99a+dgt6S5Jp9WjUwBQrapDzcyOMrOJrz+W9HFJa+vVMQCo\nRi3vfk6XdJeZvd7Oj939V3XpFQBUqepQc/fNkt5Xx74AQM0Y0gEgFUINQCqEGoBUCDUAqRBqAFIh\n1ACkQqgBSIVQA5AKoQYglXosEokR4NAhD9WNGmWhuoPB9loC7f1i7QuhtpY/tSNUd/H7ZhZr2ieO\nCbU1/ehxoboxrfX7/9899rOtTEGsW3v1FO1b5Pcy+jsZxZkagFQINQCpEGoAUiHUAKRCqAFIhVAD\nkAqhBiAVQg1AKoQagFSYUdAE6jlifN+Bg6G6saNb6rbP7//+2VDdc8/tDNV1buwu1tx12QdDbb28\nd3+oLjJzQpLaJ44t1kRH40dF2qv3jJOoercX2uew7xEAhhChBiAVQg1AKoQagFQINQCpEGoAUiHU\nAKRCqAFIhcG3SUTHc9ZzUK0kLb7hf4s175k/JdRWX9+hUF1ksOmLe2KDak+Y3haqi9rcvbdYc8yE\n0aG2prTFliSPqPcg2Ot+vTFU19pS3u83zj2x1u68AWdqAFIh1ACkQqgBSIVQA5AKoQYgFUINQCqE\nGoBUCDUAqRBqAFJhRkETqOdyzXVeSVrHzz66WLPu6ZdCbX361FmhuvVdu4s1l/5oZait/ftjy5uf\nfvL0UN1Xzpwfqovo3rUvVPfKvr5izbRJ5WXGJelna7aE6n78q9iMgmsuem+orp44UwOQSjHUzGyZ\nmXWb2doB26aY2f1m9mTl4+Sh7SYAxETO1H4o6bw3bbtG0gPuvkDSA5XPAaDhiqHm7g9KevNFkfMl\n3Vp5fKukC+rcLwCoSrXX1Ka7e1fl8TZJg15FNbMlZtZpZp09vT1V7g4AYmp+o8D777Q76Ftv7r7U\n3TvcvWNa+7RadwcAb6naUNtuZjMkqfKxfMtsABgG1YbaPZIuqTy+RNLd9ekOANQmMqTjdkkPS3qX\nmW0xs0slXS/pY2b2pKRzK58DQMMVZxS4+0WDfOmcOvcljXqP7o/MKKj3GvTnfPfBUN261c8Va+Yt\nmBFq64/P7wzVjR9Tnggz5x0TQ229uOu1UN2B4P0TPvHt3xRrPtQxO9TW1xafEKob01p+wbVp255Q\nW7f/YWuobvacY0J1W3fG7hVRT8woAJAKoQYgFUINQCqEGoBUCDUAqRBqAFIh1ACkQqgBSIVQA5DK\nsN6jwCX1L+pRu8go+yMRnQVwKND/1paR+3/FpT9ZHarbuP6FUN1/ffXsYs1nTp4Zauus7/w2VHfs\n1AnFmme37Aq1teQTx4fq/uEDsXsPvHDugmLNyZfdHmprzcbYUl19gdkOu3e+Empr7vypobqxY2PR\n8csVzxdrLu2YE2orauT+9QFAFQg1AKkQagBSIdQApEKoAUiFUAOQCqEGIBVCDUAqwzr41hQbNBsZ\noBsdxLs/uAxzdMBspO7F3ftCbd23aXuo7uu3dBZr2o+dFGqrtzs2KPWsM8uDSCXpxKnlZbOjS2F/\n/0uLQnUfuuDaYs23bv5KqK3ooNqomZPHF2v++sLTQm39buWWUN2Zi2YVa147cDDU1tMvxH4/ooPV\ntzxbHkB8MNhWFGdqAFIh1ACkQqgBSIVQA5AKoQYgFUINQCqEGoBUCDUAqRBqAFIZ1hkFUZERxtEZ\nAGNHt9TanTf41/s2Fmu6d+8PtfWPH5wXqjvj9HJd945XQ21NmVIe8S5JPcH2bnhwc7HmnBMnh9r6\nwinHheq+dv2VxZqvf/vuUFsn3/Q3obqZk2I/twtu/l2x5uWeHaG25sw/NlS37umXijWTjx4Xaitq\n+uTykuqS9NT4scWal/ceqLU7b8CZGoBUCDUAqRBqAFIh1ACkQqgBSIVQA5AKoQYgFUINQCqEGoBU\nRuSMguhsgYg1z+0M1X317rWhus1P9RZr1t3w2VBbo8q3a5Akff/C9xZrlm8qrwUvSf/+8w2hura2\nMaG6VRu6izVnnXBMqK3bH3suVHfNOeX7J/x6dex+B+f/049Cdce+68RQ3TGBkfZtbbGZAjt3vhaq\na2kp73Nn8L4ZkyfFZh68tCfW3ktPPFGs6X3ljFBbUcX0MLNlZtZtZmsHbLvOzLaa2erKv0/VtVcA\nUKXIKdEPJZ13mO3fdfeFlX/31rdbAFCdYqi5+4OSyjNmAWAEqOXi1RVmtqby8nTQZRjMbImZdZpZ\nZ09v7LoPAFSr2lD7nqTjJS2U1CXpxsEK3X2pu3e4e8e09mlV7g4AYqoKNXff7u4H3f2QpB9Iit1y\nGgCGWFWhZmYzBnz6OUmx8RAAMMSK49TM7HZJiyW1m9kWSd+UtNjMFkpySc9I+vIQ9hEAwoqh5u4X\nHWbzLUPQlz/71fquYs11d64PtWUWG+G6a1dsoOMHO+aE6iJeO3AoVLfvwMFiTfu48rLJkrR/f1+o\nzn10qG7q1PLAz1t//3yorcs+Oi9U99T2PcWa5Vd/JNTWtMeeCdW1T2sL1e0ILIPeFziekjR+QmwA\n9IQJ5WM1I3CcJKknOOC3t/eVUF3r1HeUa6Kj0IOYJgUgFUINQCqEGoBUCDUAqRBqAFIh1ACkQqgB\nSIVQA5AKoQYglWFdzrvvkGvnKweKdX9/w/Jizec/+77QPrftiI18fqE1lu+jAjMUDvTVb6aAJI0f\n01KsmRwYVS5JXzr7naG6Ox+OzQKIjGbfu3d/qK0/9cSO1fruct03pseW377pqsWhum/+cGWoLjKD\nZez42OyPo46KzSgYFRiRv/vV8t+dFD9WkybF+tbXV57BEpz0E8aZGoBUCDUAqRBqAFIh1ACkQqgB\nSIVQA5AKoQYgFUINQCqEGoBUhnVGwebevfrCskeKdfMXzCjWXP3h+aF9XnrbqlBda2tsWPPRkXXj\ngyOkR9dxFsPUttgI75OPja21/z+TYqPe+wKzJ9qCfXvs2Z2hus8vKq9737n55VBbCybHfh7jjxoX\nqovcf8DdY20FZ6a857hB7yX+Z4/+qTvU1muvxe5hsXNnrG7W/JnFml8+0RtqK4ozNQCpEGoAUiHU\nAKRCqAFIhVADkAqhBiAVQg1AKoQagFQINQCpDOuMgtaWUZrcVh6pvm3bnmLNH7a8GNrn7t37QnUv\nvxxbH7+ra3ex5p/Pis12mDQ+dl+BnYH15VsD69RL0sy28aG6V4Nr2u/bVx5Bv2dPbN37Rx8p35tC\nkl7YfmaxZunFp4ba+mP3rtg+V8buUdA6/bhyUXDGybYny38HktS9bUexZsdj/xfb6ax3h8r+4v0L\nQnXjxpUjZk/gd+hIcKYGIBVCDUAqhBqAVAg1AKkQagBSIdQApEKoAUiFUAOQikWXFq6HUxZ1+G8f\nWlGsO+Hy/y7WTJg4IbTPcz80L1R33OTYcs2/eGRLsebmCxeG2up4Z3kZZknasbc8eLUtMMhRkna+\nGluG+T9++1So7pX95YGTl58eGJAqaUxwefP2ieUB3EdPiA1sjuo7GFtau2vHa8Wa3t2xwcgtwQHV\nkb/hqYGfmSTNnhIbnP3wptjg98/8yz3Fmp//22dCbX38pGNXuntHqY4zNQCpFEPNzOaY2XIzW29m\n68zsysr2KWZ2v5k9WfkYO+0AgCEUOVPrk3S1u58k6QOSLjezkyRdI+kBd18g6YHK5wDQUMVQc/cu\nd19Vebxb0gZJsySdL+nWStmtki4Yqk4CQNQRXVMzs3mSTpG0QtJ0d++qfGmbpOmDfM8SM+s0s84X\ne3tq6CoAlIVDzczaJN0h6Sp3f8N6Ld7/9sth34Jx96Xu3uHuHVPbp9XUWQAoCYWamY1Wf6Dd5u53\nVjZvN7MZla/PkBS7BTQADKHIu58m6RZJG9z9pgFfukfSJZXHl0i6u/7dA4AjExmxeYakiyU9bmar\nK9uulXS9pJ+a2aWSnpV04dB0EQDihnVGwamndvhDKzqLdb2BJbgX/NVNxRpJGjXlHaG6vzx/Uaju\niwvL7c2cGBuVfdTY2CyAsaPLVwnGjW4JtRUcpB726oHySPvVW18OtfWbzbG6p7vLy1w/0vlcqK1x\nE2IzSd570rGhur87fXax5iMnxNoaPyZ2TCOefzG2XP1lP/tjqO6hn90Xqnv32R8u1jz89bNDbY0f\nbcwoAPD2Q6gBSIVQA5AKoQYgFUINQCqEGoBUCDUAqRBqAFIh1ACkMiJnFNRTdG35O9aU7z0gSXev\nKS+f1LPj1VBbu3aVZ05I0t7APQpGB2YdSFLv9h2hunr+Xhw3PzaCfu7MSaG6988/plgzP3jPiaiN\nPbFjunx9eV2Hjeu7ijWSdGD/gVBdxL5XY79riz5wQqju06fMCNVd+eHjQ3URzCgA8LZEqAFIhVAD\nkAqhBiAVQg1AKoQagFQINQCpEGoAUkk/+BZADgy+BfC2RKgBSIVQA5AKoQYgFUINQCqEGoBUCDUA\nqRBqAFIh1ACkQqgBSIVQA5AKoQYgFUINQCqEGoBUCDUAqRBqAFIh1ACkQqgBSIVQA5AKoQYglWKo\nmdkcM1tuZuvNbJ2ZXVnZfp2ZbTWz1ZV/nxr67gLAW2sN1PRJutrdV5nZREkrzez+yte+6+43DF33\nAODIFEPN3bskdVUe7zazDZJmDXXHAKAaR3RNzczmSTpF0orKpivMbI2ZLTOzyXXuGwAcsXComVmb\npDskXeXuuyR9T9Lxkhaq/0zuxkG+b4mZdZpZZ09vTx26DACDC4WamY1Wf6Dd5u53SpK7b3f3g+5+\nSNIPJJ12uO9196Xu3uHuHdPap9Wr3wBwWJF3P03SLZI2uPtNA7bPGFD2OUlr6989ADgykXc/z5B0\nsaTHzWx1Zdu1ki4ys4WSXNIzkr48JD0EgCMQeffz95LsMF+6t/7dAYDaMKMAQCqEGoBUCDUAqRBq\nAFIh1ACkQqgBSIVQA5AKoQYgFUINQCqEGoBUCDUAqRBqAFIh1ACkQqgBSIVQA5AKoQYgFUINQCqE\nGoBUCDUAqZi7D9/OzHokPfumze2SeoetE/XX7P2Xmv85NHv/peZ/DsPR/7nuXrzP5rCG2mE7YNbp\n7h0N7UQNmr3/UvM/h2bvv9T8z2Ek9Z+XnwBSIdQApDISQm1poztQo2bvv9T8z6HZ+y81/3MYMf1v\n+DU1AKinkXCmBgB1Q6gBSKVhoWZm55nZRjPbZGbXNKoftTCzZ8zscTNbbWadje5PhJktM7NuM1s7\nYNsUM7vfzJ6sfJzcyD6+lUH6f52Zba0ch9Vm9qlG9vGtmNkcM1tuZuvNbJ2ZXVnZ3kzHYLDnMCKO\nQ0OuqZlZi6QnJH1M0hZJj0q6yN3XD3tnamBmz0jqcPemGTRpZmdJ2iPpR+5+cmXbdyS95O7XV/6D\nmezuX2tkPwczSP+vk7TH3W9oZN8izGyGpBnuvsrMJkpaKekCSX+r5jkGgz2HCzUCjkOjztROk7TJ\n3Te7+35JP5F0foP68rbi7g9KeulNm8+XdGvl8a3q/wUdkQbpf9Nw9y53X1V5vFvSBkmz1FzHYLDn\nMCI0KtRmSXp+wOdbNIJ+KEfAJd1nZivNbEmjO1OD6e7eVXm8TdL0RnamSleY2ZrKy9MR+9JtIDOb\nJ+kUSSvUpMfgTc9BGgHHgTcKanOmuy+S9ElJl1deGjU1778e0WzjfL4n6XhJCyV1Sbqxsd0pM7M2\nSXdIusrddw38WrMcg8M8hxFxHBoValslzRnw+ezKtqbi7lsrH7sl3aX+l9XNaHvlOsnr10u6G9yf\nI+Lu2939oLsfkvQDjfDjYGaj1R8Gt7n7nZXNTXUMDvccRspxaFSoPSppgZnNN7Mxkr4o6Z4G9aUq\nZnZU5SKpzOwoSR+XtPatv2vEukfSJZXHl0i6u4F9OWKvh0HF5zSCj4OZmaRbJG1w95sGfKlpjsFg\nz2GkHIeGzSiovN17s6QWScvc/VsN6UiVzOyd6j87k6RWST9uhudgZrdLWqz+pWK2S/qmpJ9L+qmk\n49S/NNSF7j4iL8YP0v/F6n/J45KekfTlAdenRhQzO1PS7yQ9LulQZfO16r8m1SzHYLDncJFGwHFg\nmhSAVHijAEAqhBqAVAg1AKkQagBSIdQApEKoAUiFUAOQyv8DuGlLVYh+yI4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111258eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nt1NeikDAQSoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11567a8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112cfad68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115674048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121f9b2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1157bf438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in [5, 40]:   \n",
    "    show_image(X_train[i, :])\n",
    "    for params in [\n",
    "        {'alpha': 21, 'sigma': 4, 'random_state': 73},\n",
    "        {'alpha': 26, 'sigma': 4, 'random_state': 34},\n",
    "    ]:\n",
    "        g = elastic_transform(X_train[i, :].reshape(28, 28), **params)\n",
    "        show_image(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nagCyQqgByAqhBiArhBqArBBqALLy/2glApm07xr2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11394f860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111103d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy import ndimage\n",
    "g = ndimage.interpolation.rotate(X_train[9809, :].reshape(28, 28), 18, reshape=False)\n",
    "show_image(g)\n",
    "show_image(X_train[9809, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test / 255.0\n",
    "X_train = X_train / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1337, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]0.85275\n"
     ]
    }
   ],
   "source": [
    "#logistic reg\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(verbose=2)\n",
    "clf.fit(X_train, y_train)\n",
    "score1 = clf.score(X_val, y_val)\n",
    "print(score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]"
     ]
    }
   ],
   "source": [
    "# verschiedene C for logistic reg\n",
    "\n",
    "score_log = [0] * 9\n",
    "C_train = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]\n",
    "\n",
    "for i in range(9):\n",
    "    clf_train = LogisticRegression(verbose=2, C=C_train[i])\n",
    "    clf_train.fit(X_train, y_train)\n",
    "    score_log[i] = clf_train.score(X_val, y_val)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1080376d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(C_train, score_log)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.80241666666666667,\n",
       " 0.82791666666666663,\n",
       " 0.84191666666666665,\n",
       " 0.84924999999999995,\n",
       " 0.85383333333333333,\n",
       " 0.85524999999999995,\n",
       " 0.85275000000000001,\n",
       " 0.85016666666666663,\n",
       " 0.84866666666666668]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZgY4EMzMLOVAMDMzAP4/YnIwmFPc02gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ba34860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score_log2 = [0 for i in range(7)]\n",
    "C_train2 = [1/0.001, 1/0.003, 1/0.01, 1/0.03, 1/0.1, 1/0.3, 1/3.0]\n",
    "\n",
    "for i in range(7):\n",
    "    clf_train2 = LogisticRegression(verbose=2, C=C_train2[i])\n",
    "    clf_train2.fit(X_train, y_train)\n",
    "    score_log2[i] = clf_train2.score(X_val, y_val)\n",
    "    \n",
    "plt.plot(C_train2, score_log2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.84216666666666662,\n",
       " 0.84366666666666668,\n",
       " 0.84483333333333333,\n",
       " 0.84591666666666665,\n",
       " 0.84866666666666668,\n",
       " 0.84983333333333333,\n",
       " 0.85541666666666671]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_log2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the best score: `0.8552` with `C=0.3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.61301231\n",
      "Iteration 2, loss = 0.43381343\n",
      "Iteration 3, loss = 0.39176189\n",
      "Iteration 4, loss = 0.36228112\n",
      "Iteration 5, loss = 0.34084181\n",
      "Iteration 6, loss = 0.32772855\n",
      "Iteration 7, loss = 0.31227195\n",
      "Iteration 8, loss = 0.30744296\n",
      "Iteration 9, loss = 0.29194805\n",
      "Iteration 10, loss = 0.28119689\n",
      "Iteration 11, loss = 0.27448659\n",
      "Iteration 12, loss = 0.26694888\n",
      "Iteration 13, loss = 0.26110808\n",
      "Iteration 14, loss = 0.25246797\n",
      "Iteration 15, loss = 0.24862335\n",
      "Iteration 16, loss = 0.23995806\n",
      "Iteration 17, loss = 0.23531671\n",
      "Iteration 18, loss = 0.22990680\n",
      "Iteration 19, loss = 0.22743079\n",
      "Iteration 20, loss = 0.21702368\n",
      "Iteration 21, loss = 0.21382564\n",
      "Iteration 22, loss = 0.20998514\n",
      "Iteration 23, loss = 0.20580884\n",
      "Iteration 24, loss = 0.20244359\n",
      "Iteration 25, loss = 0.19796346\n",
      "Iteration 26, loss = 0.19469655\n",
      "Iteration 27, loss = 0.18910745\n",
      "Iteration 28, loss = 0.18266466\n",
      "Iteration 29, loss = 0.18154347\n",
      "Iteration 30, loss = 0.17499738\n",
      "Iteration 31, loss = 0.17281727\n",
      "Iteration 32, loss = 0.17171225\n",
      "Iteration 33, loss = 0.16607581\n",
      "Iteration 34, loss = 0.16306034\n",
      "Iteration 35, loss = 0.16080691\n",
      "Iteration 36, loss = 0.15587172\n",
      "Iteration 37, loss = 0.15273741\n",
      "Iteration 38, loss = 0.15061659\n",
      "Iteration 39, loss = 0.14803762\n",
      "Iteration 40, loss = 0.14733446\n",
      "Iteration 41, loss = 0.14071040\n",
      "Iteration 42, loss = 0.14114480\n",
      "Iteration 43, loss = 0.13610945\n",
      "Iteration 44, loss = 0.13570351\n",
      "Iteration 45, loss = 0.13593591\n",
      "Iteration 46, loss = 0.12742999\n",
      "Iteration 47, loss = 0.12589906\n",
      "Iteration 48, loss = 0.12482961\n",
      "Iteration 49, loss = 0.12339228\n",
      "Iteration 50, loss = 0.12228963\n",
      "0.886416666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ann/venv/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# NN\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf2 = MLPClassifier(solver='adam', hidden_layer_sizes=(128,), random_state=1, max_iter=50, verbose=True)\n",
    "clf2.fit(X_train, y_train)\n",
    "score2 = clf2.score(X_val, y_val)\n",
    "print(score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.append(X_train, X_val, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.append(y_train, y_val, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.81653268\n",
      "Iteration 2, loss = 0.50826639\n",
      "Iteration 3, loss = 0.45902357\n",
      "Iteration 4, loss = 0.43393461\n",
      "Iteration 5, loss = 0.41540326\n",
      "Iteration 6, loss = 0.40037597\n",
      "Iteration 7, loss = 0.39290784\n",
      "Iteration 8, loss = 0.38285540\n",
      "Iteration 9, loss = 0.37569244\n",
      "Iteration 10, loss = 0.36987943\n",
      "Iteration 11, loss = 0.36156442\n",
      "Iteration 12, loss = 0.35741629\n",
      "Iteration 13, loss = 0.35467650\n",
      "Iteration 14, loss = 0.34816917\n",
      "Iteration 15, loss = 0.34271170\n",
      "Iteration 16, loss = 0.33971454\n",
      "Iteration 17, loss = 0.33435930\n",
      "Iteration 18, loss = 0.33192773\n",
      "Iteration 19, loss = 0.33087955\n",
      "Iteration 20, loss = 0.32607434\n",
      "Iteration 21, loss = 0.31973596\n",
      "Iteration 22, loss = 0.31755432\n",
      "Iteration 23, loss = 0.31513732\n",
      "Iteration 24, loss = 0.31325547\n",
      "Iteration 25, loss = 0.30865476\n",
      "Iteration 26, loss = 0.30635886\n",
      "Iteration 27, loss = 0.30231516\n",
      "Iteration 28, loss = 0.30083831\n",
      "Iteration 29, loss = 0.29850482\n",
      "Iteration 30, loss = 0.29585127\n",
      "Iteration 31, loss = 0.29225474\n",
      "Iteration 32, loss = 0.29128306\n",
      "Iteration 33, loss = 0.28791724\n",
      "Iteration 34, loss = 0.28668218\n",
      "Iteration 35, loss = 0.28410006\n",
      "Iteration 36, loss = 0.28238668\n",
      "Iteration 37, loss = 0.27998497\n",
      "Iteration 38, loss = 0.27904564\n",
      "Iteration 39, loss = 0.27842632\n",
      "Iteration 40, loss = 0.27722108\n",
      "Iteration 41, loss = 0.27505493\n",
      "Iteration 42, loss = 0.27095612\n",
      "Iteration 43, loss = 0.27134910\n",
      "Iteration 44, loss = 0.26931952\n",
      "Iteration 45, loss = 0.26652890\n",
      "Iteration 46, loss = 0.26629862\n",
      "Iteration 47, loss = 0.26336490\n",
      "Iteration 48, loss = 0.26397716\n",
      "Iteration 49, loss = 0.26379396\n",
      "Iteration 50, loss = 0.25907746\n",
      "Iteration 51, loss = 0.25893941\n",
      "Iteration 52, loss = 0.25710458\n",
      "Iteration 53, loss = 0.25485503\n",
      "Iteration 54, loss = 0.25370980\n",
      "Iteration 55, loss = 0.25265823\n",
      "Iteration 56, loss = 0.25231524\n",
      "Iteration 57, loss = 0.24884362\n",
      "Iteration 58, loss = 0.24859144\n",
      "Iteration 59, loss = 0.24966435\n",
      "Iteration 60, loss = 0.24773110\n",
      "Iteration 61, loss = 0.24687963\n",
      "Iteration 62, loss = 0.24547602\n",
      "Iteration 63, loss = 0.24346968\n",
      "Iteration 64, loss = 0.24116871\n",
      "Iteration 65, loss = 0.23898088\n",
      "Iteration 66, loss = 0.24093414\n",
      "Iteration 67, loss = 0.24135546\n",
      "Iteration 68, loss = 0.23767673\n",
      "Iteration 69, loss = 0.23890495\n",
      "Iteration 70, loss = 0.23625544\n",
      "Iteration 71, loss = 0.23667840\n",
      "Iteration 72, loss = 0.23572447\n",
      "Iteration 73, loss = 0.23524882\n",
      "Iteration 74, loss = 0.23663047\n",
      "Iteration 75, loss = 0.23236214\n",
      "Iteration 76, loss = 0.23145828\n",
      "Iteration 77, loss = 0.23201361\n",
      "Iteration 78, loss = 0.22775290\n",
      "Iteration 79, loss = 0.22896431\n",
      "Iteration 80, loss = 0.22941483\n",
      "Iteration 81, loss = 0.22648195\n",
      "Iteration 82, loss = 0.22541387\n",
      "Iteration 83, loss = 0.22671447\n",
      "Iteration 84, loss = 0.22618916\n",
      "Iteration 85, loss = 0.22555905\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.82127168\n",
      "Iteration 2, loss = 0.51216960\n",
      "Iteration 3, loss = 0.46420688\n",
      "Iteration 4, loss = 0.43728118\n",
      "Iteration 5, loss = 0.42031763\n",
      "Iteration 6, loss = 0.40985318\n",
      "Iteration 7, loss = 0.39778951\n",
      "Iteration 8, loss = 0.38825662\n",
      "Iteration 9, loss = 0.38224664\n",
      "Iteration 10, loss = 0.37466140\n",
      "Iteration 11, loss = 0.36584844\n",
      "Iteration 12, loss = 0.36068496\n",
      "Iteration 13, loss = 0.35629914\n",
      "Iteration 14, loss = 0.35517015\n",
      "Iteration 15, loss = 0.34635120\n",
      "Iteration 16, loss = 0.34225030\n",
      "Iteration 17, loss = 0.33875155\n",
      "Iteration 18, loss = 0.33557472\n",
      "Iteration 19, loss = 0.33156560\n",
      "Iteration 20, loss = 0.32663772\n",
      "Iteration 21, loss = 0.32202827\n",
      "Iteration 22, loss = 0.31848997\n",
      "Iteration 23, loss = 0.31681656\n",
      "Iteration 24, loss = 0.31309756\n",
      "Iteration 25, loss = 0.30925925\n",
      "Iteration 26, loss = 0.30775985\n",
      "Iteration 27, loss = 0.30609280\n",
      "Iteration 28, loss = 0.30200315\n",
      "Iteration 29, loss = 0.30164765\n",
      "Iteration 30, loss = 0.29650022\n",
      "Iteration 31, loss = 0.29283775\n",
      "Iteration 32, loss = 0.29408091\n",
      "Iteration 33, loss = 0.28955119\n",
      "Iteration 34, loss = 0.28600914\n",
      "Iteration 35, loss = 0.28630078\n",
      "Iteration 36, loss = 0.28424594\n",
      "Iteration 37, loss = 0.28145159\n",
      "Iteration 38, loss = 0.28013191\n",
      "Iteration 39, loss = 0.27868147\n",
      "Iteration 40, loss = 0.27986469\n",
      "Iteration 41, loss = 0.27551263\n",
      "Iteration 42, loss = 0.27128721\n",
      "Iteration 43, loss = 0.27323583\n",
      "Iteration 44, loss = 0.27045638\n",
      "Iteration 45, loss = 0.27080126\n",
      "Iteration 46, loss = 0.26607431\n",
      "Iteration 47, loss = 0.26464868\n",
      "Iteration 48, loss = 0.26538299\n",
      "Iteration 49, loss = 0.26268752\n",
      "Iteration 50, loss = 0.26055165\n",
      "Iteration 51, loss = 0.25832568\n",
      "Iteration 52, loss = 0.25760177\n",
      "Iteration 53, loss = 0.25667376\n",
      "Iteration 54, loss = 0.25767878\n",
      "Iteration 55, loss = 0.25368967\n",
      "Iteration 56, loss = 0.25313527\n",
      "Iteration 57, loss = 0.25235838\n",
      "Iteration 58, loss = 0.25384910\n",
      "Iteration 59, loss = 0.24953638\n",
      "Iteration 60, loss = 0.25041701\n",
      "Iteration 61, loss = 0.24888997\n",
      "Iteration 62, loss = 0.24809983\n",
      "Iteration 63, loss = 0.24324999\n",
      "Iteration 64, loss = 0.24358377\n",
      "Iteration 65, loss = 0.24237811\n",
      "Iteration 66, loss = 0.24557561\n",
      "Iteration 67, loss = 0.24143690\n",
      "Iteration 68, loss = 0.23896739\n",
      "Iteration 69, loss = 0.24032136\n",
      "Iteration 70, loss = 0.23751484\n",
      "Iteration 71, loss = 0.23718548\n",
      "Iteration 72, loss = 0.23598377\n",
      "Iteration 73, loss = 0.23319166\n",
      "Iteration 74, loss = 0.23570173\n",
      "Iteration 75, loss = 0.23611268\n",
      "Iteration 76, loss = 0.23259768\n",
      "Iteration 77, loss = 0.23103821\n",
      "Iteration 78, loss = 0.22857041\n",
      "Iteration 79, loss = 0.22705362\n",
      "Iteration 80, loss = 0.23148343\n",
      "Iteration 81, loss = 0.22783085\n",
      "Iteration 82, loss = 0.22708039\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.83077967\n",
      "Iteration 2, loss = 0.50930045\n",
      "Iteration 3, loss = 0.46089195\n",
      "Iteration 4, loss = 0.43541404\n",
      "Iteration 5, loss = 0.42121944\n",
      "Iteration 6, loss = 0.40911871\n",
      "Iteration 7, loss = 0.39787523\n",
      "Iteration 8, loss = 0.38738369\n",
      "Iteration 9, loss = 0.38271868\n",
      "Iteration 10, loss = 0.37769365\n",
      "Iteration 11, loss = 0.36782551\n",
      "Iteration 12, loss = 0.36351320\n",
      "Iteration 13, loss = 0.35741933\n",
      "Iteration 14, loss = 0.35285549\n",
      "Iteration 15, loss = 0.34893066\n",
      "Iteration 16, loss = 0.34342785\n",
      "Iteration 17, loss = 0.34141848\n",
      "Iteration 18, loss = 0.33543609\n",
      "Iteration 19, loss = 0.32827318\n",
      "Iteration 20, loss = 0.32597434\n",
      "Iteration 21, loss = 0.32481470\n",
      "Iteration 22, loss = 0.31932459\n",
      "Iteration 23, loss = 0.31952579\n",
      "Iteration 24, loss = 0.31518378\n",
      "Iteration 25, loss = 0.31075841\n",
      "Iteration 26, loss = 0.30809609\n",
      "Iteration 27, loss = 0.30359307\n",
      "Iteration 28, loss = 0.30377245\n",
      "Iteration 29, loss = 0.30043391\n",
      "Iteration 30, loss = 0.29824074\n",
      "Iteration 31, loss = 0.29461039\n",
      "Iteration 32, loss = 0.29246175\n",
      "Iteration 33, loss = 0.29303624\n",
      "Iteration 34, loss = 0.29186657\n",
      "Iteration 35, loss = 0.28601787\n",
      "Iteration 36, loss = 0.28547385\n",
      "Iteration 37, loss = 0.28152253\n",
      "Iteration 38, loss = 0.28100642\n",
      "Iteration 39, loss = 0.27759393\n",
      "Iteration 40, loss = 0.27937388\n",
      "Iteration 41, loss = 0.27370138\n",
      "Iteration 42, loss = 0.27520406\n",
      "Iteration 43, loss = 0.27146507\n",
      "Iteration 44, loss = 0.26988473\n",
      "Iteration 45, loss = 0.26881393\n",
      "Iteration 46, loss = 0.26627843\n",
      "Iteration 47, loss = 0.26391359\n",
      "Iteration 48, loss = 0.26389489\n",
      "Iteration 49, loss = 0.26026375\n",
      "Iteration 50, loss = 0.26200634\n",
      "Iteration 51, loss = 0.26110181\n",
      "Iteration 52, loss = 0.25730678\n",
      "Iteration 53, loss = 0.25719007\n",
      "Iteration 54, loss = 0.25482439\n",
      "Iteration 55, loss = 0.25252617\n",
      "Iteration 56, loss = 0.25537738\n",
      "Iteration 57, loss = 0.25228312\n",
      "Iteration 58, loss = 0.25222188\n",
      "Iteration 59, loss = 0.25036002\n",
      "Iteration 60, loss = 0.24564061\n",
      "Iteration 61, loss = 0.24514484\n",
      "Iteration 62, loss = 0.24742528\n",
      "Iteration 63, loss = 0.24587864\n",
      "Iteration 64, loss = 0.24272990\n",
      "Iteration 65, loss = 0.24248056\n",
      "Iteration 66, loss = 0.24299199\n",
      "Iteration 67, loss = 0.24157012\n",
      "Iteration 68, loss = 0.23956070\n",
      "Iteration 69, loss = 0.24124286\n",
      "Iteration 70, loss = 0.23634189\n",
      "Iteration 71, loss = 0.23547644\n",
      "Iteration 72, loss = 0.23555694\n",
      "Iteration 73, loss = 0.23225502\n",
      "Iteration 74, loss = 0.23483967\n",
      "Iteration 75, loss = 0.23188862\n",
      "Iteration 76, loss = 0.23061393\n",
      "Iteration 77, loss = 0.23047459\n",
      "Iteration 78, loss = 0.23076130\n",
      "Iteration 79, loss = 0.22913619\n",
      "Iteration 80, loss = 0.23201727\n",
      "Iteration 81, loss = 0.22790106\n",
      "Iteration 82, loss = 0.22832706\n",
      "Iteration 83, loss = 0.22919345\n",
      "Iteration 84, loss = 0.22831496\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.94532279\n",
      "Iteration 2, loss = 0.54824213\n",
      "Iteration 3, loss = 0.47715532\n",
      "Iteration 4, loss = 0.44294432\n",
      "Iteration 5, loss = 0.42212322\n",
      "Iteration 6, loss = 0.40706399\n",
      "Iteration 7, loss = 0.39557740\n",
      "Iteration 8, loss = 0.38690336\n",
      "Iteration 9, loss = 0.37752396\n",
      "Iteration 10, loss = 0.37035334\n",
      "Iteration 11, loss = 0.36400888\n",
      "Iteration 12, loss = 0.35870205\n",
      "Iteration 13, loss = 0.35309811\n",
      "Iteration 14, loss = 0.34836281\n",
      "Iteration 15, loss = 0.34279593\n",
      "Iteration 16, loss = 0.33876843\n",
      "Iteration 17, loss = 0.33357955\n",
      "Iteration 18, loss = 0.33169435\n",
      "Iteration 19, loss = 0.32949931\n",
      "Iteration 20, loss = 0.32306944\n",
      "Iteration 21, loss = 0.32067107\n",
      "Iteration 22, loss = 0.31856690\n",
      "Iteration 23, loss = 0.31226767\n",
      "Iteration 24, loss = 0.31191380\n",
      "Iteration 25, loss = 0.30978098\n",
      "Iteration 26, loss = 0.30575023\n",
      "Iteration 27, loss = 0.30223335\n",
      "Iteration 28, loss = 0.30254223\n",
      "Iteration 29, loss = 0.29824828\n",
      "Iteration 30, loss = 0.29565078\n",
      "Iteration 31, loss = 0.29335245\n",
      "Iteration 32, loss = 0.29095866\n",
      "Iteration 33, loss = 0.28720464\n",
      "Iteration 34, loss = 0.28701846\n",
      "Iteration 35, loss = 0.28421107\n",
      "Iteration 36, loss = 0.28157099\n",
      "Iteration 37, loss = 0.28030801\n",
      "Iteration 38, loss = 0.27829645\n",
      "Iteration 39, loss = 0.27517209\n",
      "Iteration 40, loss = 0.27573839\n",
      "Iteration 41, loss = 0.27213818\n",
      "Iteration 42, loss = 0.27013666\n",
      "Iteration 43, loss = 0.26878231\n",
      "Iteration 44, loss = 0.26828180\n",
      "Iteration 45, loss = 0.26696321\n",
      "Iteration 46, loss = 0.26424852\n",
      "Iteration 47, loss = 0.26357081\n",
      "Iteration 48, loss = 0.26198043\n",
      "Iteration 49, loss = 0.26128418\n",
      "Iteration 50, loss = 0.25975357\n",
      "Iteration 51, loss = 0.25608082\n",
      "Iteration 52, loss = 0.25727549\n",
      "Iteration 53, loss = 0.25288211\n",
      "Iteration 54, loss = 0.25124721\n",
      "Iteration 55, loss = 0.25244026\n",
      "Iteration 56, loss = 0.25215235\n",
      "Iteration 57, loss = 0.24763522\n",
      "Iteration 58, loss = 0.24737788\n",
      "Iteration 59, loss = 0.24741350\n",
      "Iteration 60, loss = 0.24642485\n",
      "Iteration 61, loss = 0.24467410\n",
      "Iteration 62, loss = 0.24311129\n",
      "Iteration 63, loss = 0.24079134\n",
      "Iteration 64, loss = 0.24037212\n",
      "Iteration 65, loss = 0.23907354\n",
      "Iteration 66, loss = 0.24018952\n",
      "Iteration 67, loss = 0.23704728\n",
      "Iteration 68, loss = 0.23549703\n",
      "Iteration 69, loss = 0.23476101\n",
      "Iteration 70, loss = 0.23552541\n",
      "Iteration 71, loss = 0.23758714\n",
      "Iteration 72, loss = 0.23383519\n",
      "Iteration 73, loss = 0.23392488\n",
      "Iteration 74, loss = 0.23040261\n",
      "Iteration 75, loss = 0.23221134\n",
      "Iteration 76, loss = 0.22891992\n",
      "Iteration 77, loss = 0.22975032\n",
      "Iteration 78, loss = 0.22594258\n",
      "Iteration 79, loss = 0.22995368\n",
      "Iteration 80, loss = 0.22745691\n",
      "Iteration 81, loss = 0.22643135\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94467916\n",
      "Iteration 2, loss = 0.54895308\n",
      "Iteration 3, loss = 0.47925394\n",
      "Iteration 4, loss = 0.44613103\n",
      "Iteration 5, loss = 0.42611585\n",
      "Iteration 6, loss = 0.41121197\n",
      "Iteration 7, loss = 0.39954523\n",
      "Iteration 8, loss = 0.38925867\n",
      "Iteration 9, loss = 0.38249532\n",
      "Iteration 10, loss = 0.37434265\n",
      "Iteration 11, loss = 0.36837195\n",
      "Iteration 12, loss = 0.36273538\n",
      "Iteration 13, loss = 0.35816247\n",
      "Iteration 14, loss = 0.35207237\n",
      "Iteration 15, loss = 0.34780441\n",
      "Iteration 16, loss = 0.34224650\n",
      "Iteration 17, loss = 0.33896651\n",
      "Iteration 18, loss = 0.33574135\n",
      "Iteration 19, loss = 0.33125548\n",
      "Iteration 20, loss = 0.32859926\n",
      "Iteration 21, loss = 0.32442878\n",
      "Iteration 22, loss = 0.32232705\n",
      "Iteration 23, loss = 0.32023369\n",
      "Iteration 24, loss = 0.31446251\n",
      "Iteration 25, loss = 0.31095781\n",
      "Iteration 26, loss = 0.30851978\n",
      "Iteration 27, loss = 0.30429872\n",
      "Iteration 28, loss = 0.30462337\n",
      "Iteration 29, loss = 0.30117954\n",
      "Iteration 30, loss = 0.29896410\n",
      "Iteration 31, loss = 0.29744749\n",
      "Iteration 32, loss = 0.29442990\n",
      "Iteration 33, loss = 0.29241417\n",
      "Iteration 34, loss = 0.29040278\n",
      "Iteration 35, loss = 0.28765120\n",
      "Iteration 36, loss = 0.28665810\n",
      "Iteration 37, loss = 0.28386857\n",
      "Iteration 38, loss = 0.27969865\n",
      "Iteration 39, loss = 0.27846458\n",
      "Iteration 40, loss = 0.27839174\n",
      "Iteration 41, loss = 0.27512322\n",
      "Iteration 42, loss = 0.27346395\n",
      "Iteration 43, loss = 0.27239631\n",
      "Iteration 44, loss = 0.27017320\n",
      "Iteration 45, loss = 0.26940563\n",
      "Iteration 46, loss = 0.26826471\n",
      "Iteration 47, loss = 0.26480559\n",
      "Iteration 48, loss = 0.26416459\n",
      "Iteration 49, loss = 0.26246145\n",
      "Iteration 50, loss = 0.26239610\n",
      "Iteration 51, loss = 0.26030526\n",
      "Iteration 52, loss = 0.25975643\n",
      "Iteration 53, loss = 0.25739598\n",
      "Iteration 54, loss = 0.25792429\n",
      "Iteration 55, loss = 0.25401004\n",
      "Iteration 56, loss = 0.25503968\n",
      "Iteration 57, loss = 0.25122903\n",
      "Iteration 58, loss = 0.25204159\n",
      "Iteration 59, loss = 0.25005574\n",
      "Iteration 60, loss = 0.24764914\n",
      "Iteration 61, loss = 0.24630591\n",
      "Iteration 62, loss = 0.24636227\n",
      "Iteration 63, loss = 0.24445283\n",
      "Iteration 64, loss = 0.24244015\n",
      "Iteration 65, loss = 0.24234062\n",
      "Iteration 66, loss = 0.24304548\n",
      "Iteration 67, loss = 0.24075951\n",
      "Iteration 68, loss = 0.24069574\n",
      "Iteration 69, loss = 0.23932048\n",
      "Iteration 70, loss = 0.23688293\n",
      "Iteration 71, loss = 0.23680320\n",
      "Iteration 72, loss = 0.23471951\n",
      "Iteration 73, loss = 0.23559469\n",
      "Iteration 74, loss = 0.23311684\n",
      "Iteration 75, loss = 0.23451268\n",
      "Iteration 76, loss = 0.23243178\n",
      "Iteration 77, loss = 0.23231754\n",
      "Iteration 78, loss = 0.23173217\n",
      "Iteration 79, loss = 0.22854683\n",
      "Iteration 80, loss = 0.22982156\n",
      "Iteration 81, loss = 0.22659470\n",
      "Iteration 82, loss = 0.22695557\n",
      "Iteration 83, loss = 0.22984923\n",
      "Iteration 84, loss = 0.22483354\n",
      "Iteration 85, loss = 0.22680667\n",
      "Iteration 86, loss = 0.22295021\n",
      "Iteration 87, loss = 0.22258186\n",
      "Iteration 88, loss = 0.22262083\n",
      "Iteration 89, loss = 0.22078357\n",
      "Iteration 90, loss = 0.22290867\n",
      "Iteration 91, loss = 0.22296105\n",
      "Iteration 92, loss = 0.21868539\n",
      "Iteration 93, loss = 0.21852418\n",
      "Iteration 94, loss = 0.21956043\n",
      "Iteration 95, loss = 0.21718518\n",
      "Iteration 96, loss = 0.21880166\n",
      "Iteration 97, loss = 0.21773060\n",
      "Iteration 98, loss = 0.21802564\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94653519\n",
      "Iteration 2, loss = 0.54757086\n",
      "Iteration 3, loss = 0.47682229\n",
      "Iteration 4, loss = 0.44617427\n",
      "Iteration 5, loss = 0.42601300\n",
      "Iteration 6, loss = 0.41220180\n",
      "Iteration 7, loss = 0.40077613\n",
      "Iteration 8, loss = 0.39219848\n",
      "Iteration 9, loss = 0.38369432\n",
      "Iteration 10, loss = 0.37562326\n",
      "Iteration 11, loss = 0.36995682\n",
      "Iteration 12, loss = 0.36497471\n",
      "Iteration 13, loss = 0.35799378\n",
      "Iteration 14, loss = 0.35326421\n",
      "Iteration 15, loss = 0.34879553\n",
      "Iteration 16, loss = 0.34403636\n",
      "Iteration 17, loss = 0.34118614\n",
      "Iteration 18, loss = 0.33552853\n",
      "Iteration 19, loss = 0.33299263\n",
      "Iteration 20, loss = 0.33001422\n",
      "Iteration 21, loss = 0.32618246\n",
      "Iteration 22, loss = 0.32277708\n",
      "Iteration 23, loss = 0.32059209\n",
      "Iteration 24, loss = 0.31495941\n",
      "Iteration 25, loss = 0.31247463\n",
      "Iteration 26, loss = 0.30930587\n",
      "Iteration 27, loss = 0.30729598\n",
      "Iteration 28, loss = 0.30461285\n",
      "Iteration 29, loss = 0.30231581\n",
      "Iteration 30, loss = 0.29875200\n",
      "Iteration 31, loss = 0.29769138\n",
      "Iteration 32, loss = 0.29373142\n",
      "Iteration 33, loss = 0.29272449\n",
      "Iteration 34, loss = 0.29053308\n",
      "Iteration 35, loss = 0.28917537\n",
      "Iteration 36, loss = 0.28762267\n",
      "Iteration 37, loss = 0.28433692\n",
      "Iteration 38, loss = 0.28051377\n",
      "Iteration 39, loss = 0.27962984\n",
      "Iteration 40, loss = 0.27924504\n",
      "Iteration 41, loss = 0.27530775\n",
      "Iteration 42, loss = 0.27351832\n",
      "Iteration 43, loss = 0.27470092\n",
      "Iteration 44, loss = 0.27251293\n",
      "Iteration 45, loss = 0.26816112\n",
      "Iteration 46, loss = 0.26714643\n",
      "Iteration 47, loss = 0.26504996\n",
      "Iteration 48, loss = 0.26726573\n",
      "Iteration 49, loss = 0.26418444\n",
      "Iteration 50, loss = 0.25961865\n",
      "Iteration 51, loss = 0.25937667\n",
      "Iteration 52, loss = 0.25988490\n",
      "Iteration 53, loss = 0.25623392\n",
      "Iteration 54, loss = 0.25624122\n",
      "Iteration 55, loss = 0.25576942\n",
      "Iteration 56, loss = 0.25690574\n",
      "Iteration 57, loss = 0.25282030\n",
      "Iteration 58, loss = 0.25020579\n",
      "Iteration 59, loss = 0.24939911\n",
      "Iteration 60, loss = 0.24818173\n",
      "Iteration 61, loss = 0.24616178\n",
      "Iteration 62, loss = 0.24559186\n",
      "Iteration 63, loss = 0.24490456\n",
      "Iteration 64, loss = 0.24565537\n",
      "Iteration 65, loss = 0.24212797\n",
      "Iteration 66, loss = 0.24136792\n",
      "Iteration 67, loss = 0.24156884\n",
      "Iteration 68, loss = 0.23976005\n",
      "Iteration 69, loss = 0.24075824\n",
      "Iteration 70, loss = 0.23752362\n",
      "Iteration 71, loss = 0.23562962\n",
      "Iteration 72, loss = 0.23564225\n",
      "Iteration 73, loss = 0.23521769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 74, loss = 0.23276285\n",
      "Iteration 75, loss = 0.23557755\n",
      "Iteration 76, loss = 0.23262302\n",
      "Iteration 77, loss = 0.23194026\n",
      "Iteration 78, loss = 0.23089339\n",
      "Iteration 79, loss = 0.22918587\n",
      "Iteration 80, loss = 0.22790643\n",
      "Iteration 81, loss = 0.22822511\n",
      "Iteration 82, loss = 0.22759219\n",
      "Iteration 83, loss = 0.22651003\n",
      "Iteration 84, loss = 0.22469883\n",
      "Iteration 85, loss = 0.22346005\n",
      "Iteration 86, loss = 0.22418780\n",
      "Iteration 87, loss = 0.22286609\n",
      "Iteration 88, loss = 0.22289579\n",
      "Iteration 89, loss = 0.22166395\n",
      "Iteration 90, loss = 0.22099247\n",
      "Iteration 91, loss = 0.21980612\n",
      "Iteration 92, loss = 0.22008050\n",
      "Iteration 93, loss = 0.21884483\n",
      "Iteration 94, loss = 0.21720186\n",
      "Iteration 95, loss = 0.21800710\n",
      "Iteration 96, loss = 0.21802817\n",
      "Iteration 97, loss = 0.21842210\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.99819487\n",
      "Iteration 2, loss = 0.53166804\n",
      "Iteration 3, loss = 0.46531132\n",
      "Iteration 4, loss = 0.43569107\n",
      "Iteration 5, loss = 0.41266845\n",
      "Iteration 6, loss = 0.40166606\n",
      "Iteration 7, loss = 0.39555571\n",
      "Iteration 8, loss = 0.38035337\n",
      "Iteration 9, loss = 0.37307336\n",
      "Iteration 10, loss = 0.36794646\n",
      "Iteration 11, loss = 0.36107659\n",
      "Iteration 12, loss = 0.35810429\n",
      "Iteration 13, loss = 0.35062135\n",
      "Iteration 14, loss = 0.34704094\n",
      "Iteration 15, loss = 0.34433494\n",
      "Iteration 16, loss = 0.34155631\n",
      "Iteration 17, loss = 0.33534946\n",
      "Iteration 18, loss = 0.33192987\n",
      "Iteration 19, loss = 0.33129327\n",
      "Iteration 20, loss = 0.32026438\n",
      "Iteration 21, loss = 0.31970938\n",
      "Iteration 22, loss = 0.31385836\n",
      "Iteration 23, loss = 0.31367413\n",
      "Iteration 24, loss = 0.30602230\n",
      "Iteration 25, loss = 0.30427679\n",
      "Iteration 26, loss = 0.29981593\n",
      "Iteration 27, loss = 0.29675206\n",
      "Iteration 28, loss = 0.29950916\n",
      "Iteration 29, loss = 0.29370607\n",
      "Iteration 30, loss = 0.28987642\n",
      "Iteration 31, loss = 0.28894890\n",
      "Iteration 32, loss = 0.28559586\n",
      "Iteration 33, loss = 0.28323786\n",
      "Iteration 34, loss = 0.27860556\n",
      "Iteration 35, loss = 0.28093555\n",
      "Iteration 36, loss = 0.27301890\n",
      "Iteration 37, loss = 0.27045069\n",
      "Iteration 38, loss = 0.26844862\n",
      "Iteration 39, loss = 0.27426306\n",
      "Iteration 40, loss = 0.26895782\n",
      "Iteration 41, loss = 0.26291802\n",
      "Iteration 42, loss = 0.26274866\n",
      "Iteration 43, loss = 0.25966441\n",
      "Iteration 44, loss = 0.25735809\n",
      "Iteration 45, loss = 0.25772426\n",
      "Iteration 46, loss = 0.24994724\n",
      "Iteration 47, loss = 0.25329032\n",
      "Iteration 48, loss = 0.25159055\n",
      "Iteration 49, loss = 0.24878053\n",
      "Iteration 50, loss = 0.24756028\n",
      "Iteration 51, loss = 0.24292599\n",
      "Iteration 52, loss = 0.23980053\n",
      "Iteration 53, loss = 0.24141760\n",
      "Iteration 54, loss = 0.24607063\n",
      "Iteration 55, loss = 0.24486468\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.99745592\n",
      "Iteration 2, loss = 0.53196307\n",
      "Iteration 3, loss = 0.46321885\n",
      "Iteration 4, loss = 0.43783360\n",
      "Iteration 5, loss = 0.41721403\n",
      "Iteration 6, loss = 0.40429513\n",
      "Iteration 7, loss = 0.40242163\n",
      "Iteration 8, loss = 0.38614411\n",
      "Iteration 9, loss = 0.37952605\n",
      "Iteration 10, loss = 0.37199063\n",
      "Iteration 11, loss = 0.36443855\n",
      "Iteration 12, loss = 0.36002638\n",
      "Iteration 13, loss = 0.35430821\n",
      "Iteration 14, loss = 0.35263801\n",
      "Iteration 15, loss = 0.34611700\n",
      "Iteration 16, loss = 0.34343275\n",
      "Iteration 17, loss = 0.33157689\n",
      "Iteration 18, loss = 0.33168719\n",
      "Iteration 19, loss = 0.33072515\n",
      "Iteration 20, loss = 0.32363178\n",
      "Iteration 21, loss = 0.32232579\n",
      "Iteration 22, loss = 0.31621387\n",
      "Iteration 23, loss = 0.31423152\n",
      "Iteration 24, loss = 0.31115437\n",
      "Iteration 25, loss = 0.30676417\n",
      "Iteration 26, loss = 0.30488587\n",
      "Iteration 27, loss = 0.30157114\n",
      "Iteration 28, loss = 0.30596295\n",
      "Iteration 29, loss = 0.29916621\n",
      "Iteration 30, loss = 0.29205928\n",
      "Iteration 31, loss = 0.28820133\n",
      "Iteration 32, loss = 0.29215086\n",
      "Iteration 33, loss = 0.28584792\n",
      "Iteration 34, loss = 0.27962604\n",
      "Iteration 35, loss = 0.27970552\n",
      "Iteration 36, loss = 0.27848694\n",
      "Iteration 37, loss = 0.27205275\n",
      "Iteration 38, loss = 0.27723442\n",
      "Iteration 39, loss = 0.27034483\n",
      "Iteration 40, loss = 0.26535151\n",
      "Iteration 41, loss = 0.26854224\n",
      "Iteration 42, loss = 0.26515173\n",
      "Iteration 43, loss = 0.26169237\n",
      "Iteration 44, loss = 0.26029238\n",
      "Iteration 45, loss = 0.25932511\n",
      "Iteration 46, loss = 0.25486663\n",
      "Iteration 47, loss = 0.25594384\n",
      "Iteration 48, loss = 0.25437390\n",
      "Iteration 49, loss = 0.25457312\n",
      "Iteration 50, loss = 0.25029272\n",
      "Iteration 51, loss = 0.25219306\n",
      "Iteration 52, loss = 0.24946535\n",
      "Iteration 53, loss = 0.25062038\n",
      "Iteration 54, loss = 0.24591908\n",
      "Iteration 55, loss = 0.24149654\n",
      "Iteration 56, loss = 0.24320797\n",
      "Iteration 57, loss = 0.24349774\n",
      "Iteration 58, loss = 0.23474055\n",
      "Iteration 59, loss = 0.23403845\n",
      "Iteration 60, loss = 0.23379897\n",
      "Iteration 61, loss = 0.23108367\n",
      "Iteration 62, loss = 0.23252967\n",
      "Iteration 63, loss = 0.23297601\n",
      "Iteration 64, loss = 0.22677399\n",
      "Iteration 65, loss = 0.22886438\n",
      "Iteration 66, loss = 0.22681353\n",
      "Iteration 67, loss = 0.22479435\n",
      "Iteration 68, loss = 0.22265163\n",
      "Iteration 69, loss = 0.22194754\n",
      "Iteration 70, loss = 0.22198849\n",
      "Iteration 71, loss = 0.21854503\n",
      "Iteration 72, loss = 0.21742552\n",
      "Iteration 73, loss = 0.21652595\n",
      "Iteration 74, loss = 0.21992798\n",
      "Iteration 75, loss = 0.22136073\n",
      "Iteration 76, loss = 0.21008710\n",
      "Iteration 77, loss = 0.21423008\n",
      "Iteration 78, loss = 0.21622976\n",
      "Iteration 79, loss = 0.21156695\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.99709190\n",
      "Iteration 2, loss = 0.52834108\n",
      "Iteration 3, loss = 0.46432403\n",
      "Iteration 4, loss = 0.43543806\n",
      "Iteration 5, loss = 0.41842950\n",
      "Iteration 6, loss = 0.40443232\n",
      "Iteration 7, loss = 0.39492476\n",
      "Iteration 8, loss = 0.39081982\n",
      "Iteration 9, loss = 0.37958737\n",
      "Iteration 10, loss = 0.37159350\n",
      "Iteration 11, loss = 0.36785506\n",
      "Iteration 12, loss = 0.36019560\n",
      "Iteration 13, loss = 0.35376009\n",
      "Iteration 14, loss = 0.35209114\n",
      "Iteration 15, loss = 0.34302104\n",
      "Iteration 16, loss = 0.33981391\n",
      "Iteration 17, loss = 0.33537058\n",
      "Iteration 18, loss = 0.32820803\n",
      "Iteration 19, loss = 0.32707345\n",
      "Iteration 20, loss = 0.32061782\n",
      "Iteration 21, loss = 0.32229343\n",
      "Iteration 22, loss = 0.31838658\n",
      "Iteration 23, loss = 0.31357425\n",
      "Iteration 24, loss = 0.30877804\n",
      "Iteration 25, loss = 0.30509287\n",
      "Iteration 26, loss = 0.30102807\n",
      "Iteration 27, loss = 0.30241308\n",
      "Iteration 28, loss = 0.29746736\n",
      "Iteration 29, loss = 0.29311165\n",
      "Iteration 30, loss = 0.29145003\n",
      "Iteration 31, loss = 0.28859387\n",
      "Iteration 32, loss = 0.28989375\n",
      "Iteration 33, loss = 0.28082457\n",
      "Iteration 34, loss = 0.28173235\n",
      "Iteration 35, loss = 0.28383944\n",
      "Iteration 36, loss = 0.27476953\n",
      "Iteration 37, loss = 0.27122291\n",
      "Iteration 38, loss = 0.27010042\n",
      "Iteration 39, loss = 0.27070377\n",
      "Iteration 40, loss = 0.26720566\n",
      "Iteration 41, loss = 0.26715593\n",
      "Iteration 42, loss = 0.26583496\n",
      "Iteration 43, loss = 0.26116905\n",
      "Iteration 44, loss = 0.25681940\n",
      "Iteration 45, loss = 0.25769737\n",
      "Iteration 46, loss = 0.25916832\n",
      "Iteration 47, loss = 0.25985287\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.21525006\n",
      "Iteration 2, loss = 0.59363783\n",
      "Iteration 3, loss = 0.48892310\n",
      "Iteration 4, loss = 0.44532378\n",
      "Iteration 5, loss = 0.41960219\n",
      "Iteration 6, loss = 0.40388354\n",
      "Iteration 7, loss = 0.39301884\n",
      "Iteration 8, loss = 0.38375570\n",
      "Iteration 9, loss = 0.37191284\n",
      "Iteration 10, loss = 0.36371546\n",
      "Iteration 11, loss = 0.35904665\n",
      "Iteration 12, loss = 0.35292914\n",
      "Iteration 13, loss = 0.34639711\n",
      "Iteration 14, loss = 0.34353131\n",
      "Iteration 15, loss = 0.33672068\n",
      "Iteration 16, loss = 0.33399920\n",
      "Iteration 17, loss = 0.32850342\n",
      "Iteration 18, loss = 0.32663044\n",
      "Iteration 19, loss = 0.32228598\n",
      "Iteration 20, loss = 0.31886173\n",
      "Iteration 21, loss = 0.31420031\n",
      "Iteration 22, loss = 0.30781408\n",
      "Iteration 23, loss = 0.30893008\n",
      "Iteration 24, loss = 0.30486304\n",
      "Iteration 25, loss = 0.30110906\n",
      "Iteration 26, loss = 0.29890673\n",
      "Iteration 27, loss = 0.29151747\n",
      "Iteration 28, loss = 0.29098289\n",
      "Iteration 29, loss = 0.28701928\n",
      "Iteration 30, loss = 0.28638823\n",
      "Iteration 31, loss = 0.28051441\n",
      "Iteration 32, loss = 0.28072244\n",
      "Iteration 33, loss = 0.27882115\n",
      "Iteration 34, loss = 0.27454743\n",
      "Iteration 35, loss = 0.27171088\n",
      "Iteration 36, loss = 0.26832311\n",
      "Iteration 37, loss = 0.26758836\n",
      "Iteration 38, loss = 0.26541908\n",
      "Iteration 39, loss = 0.26577681\n",
      "Iteration 40, loss = 0.26147827\n",
      "Iteration 41, loss = 0.26005107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42, loss = 0.25518020\n",
      "Iteration 43, loss = 0.25465320\n",
      "Iteration 44, loss = 0.25171108\n",
      "Iteration 45, loss = 0.25012021\n",
      "Iteration 46, loss = 0.24934334\n",
      "Iteration 47, loss = 0.24800080\n",
      "Iteration 48, loss = 0.24571619\n",
      "Iteration 49, loss = 0.24233213\n",
      "Iteration 50, loss = 0.24466261\n",
      "Iteration 51, loss = 0.24158930\n",
      "Iteration 52, loss = 0.23992036\n",
      "Iteration 53, loss = 0.23779202\n",
      "Iteration 54, loss = 0.23824059\n",
      "Iteration 55, loss = 0.23505030\n",
      "Iteration 56, loss = 0.23162621\n",
      "Iteration 57, loss = 0.23109587\n",
      "Iteration 58, loss = 0.22738177\n",
      "Iteration 59, loss = 0.22814940\n",
      "Iteration 60, loss = 0.22636745\n",
      "Iteration 61, loss = 0.22695505\n",
      "Iteration 62, loss = 0.22206001\n",
      "Iteration 63, loss = 0.22109753\n",
      "Iteration 64, loss = 0.22459168\n",
      "Iteration 65, loss = 0.21607834\n",
      "Iteration 66, loss = 0.21916458\n",
      "Iteration 67, loss = 0.21879796\n",
      "Iteration 68, loss = 0.21552129\n",
      "Iteration 69, loss = 0.21350656\n",
      "Iteration 70, loss = 0.21309964\n",
      "Iteration 71, loss = 0.21018654\n",
      "Iteration 72, loss = 0.21291117\n",
      "Iteration 73, loss = 0.20803610\n",
      "Iteration 74, loss = 0.20684882\n",
      "Iteration 75, loss = 0.20569643\n",
      "Iteration 76, loss = 0.20906070\n",
      "Iteration 77, loss = 0.20274573\n",
      "Iteration 78, loss = 0.20419339\n",
      "Iteration 79, loss = 0.19790829\n",
      "Iteration 80, loss = 0.20408755\n",
      "Iteration 81, loss = 0.20100063\n",
      "Iteration 82, loss = 0.19928726\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.20216996\n",
      "Iteration 2, loss = 0.59698110\n",
      "Iteration 3, loss = 0.49374961\n",
      "Iteration 4, loss = 0.44917731\n",
      "Iteration 5, loss = 0.42498562\n",
      "Iteration 6, loss = 0.40747017\n",
      "Iteration 7, loss = 0.39614766\n",
      "Iteration 8, loss = 0.38554354\n",
      "Iteration 9, loss = 0.37530686\n",
      "Iteration 10, loss = 0.36807117\n",
      "Iteration 11, loss = 0.36164603\n",
      "Iteration 12, loss = 0.35482199\n",
      "Iteration 13, loss = 0.35153475\n",
      "Iteration 14, loss = 0.34547752\n",
      "Iteration 15, loss = 0.34011597\n",
      "Iteration 16, loss = 0.33521153\n",
      "Iteration 17, loss = 0.33222456\n",
      "Iteration 18, loss = 0.32479894\n",
      "Iteration 19, loss = 0.32729936\n",
      "Iteration 20, loss = 0.32179573\n",
      "Iteration 21, loss = 0.31644861\n",
      "Iteration 22, loss = 0.31111119\n",
      "Iteration 23, loss = 0.30798818\n",
      "Iteration 24, loss = 0.30405897\n",
      "Iteration 25, loss = 0.30231634\n",
      "Iteration 26, loss = 0.29808675\n",
      "Iteration 27, loss = 0.29263069\n",
      "Iteration 28, loss = 0.28947537\n",
      "Iteration 29, loss = 0.28945640\n",
      "Iteration 30, loss = 0.28817251\n",
      "Iteration 31, loss = 0.28512088\n",
      "Iteration 32, loss = 0.27942612\n",
      "Iteration 33, loss = 0.27913472\n",
      "Iteration 34, loss = 0.27440359\n",
      "Iteration 35, loss = 0.27169502\n",
      "Iteration 36, loss = 0.27218318\n",
      "Iteration 37, loss = 0.26705173\n",
      "Iteration 38, loss = 0.26443299\n",
      "Iteration 39, loss = 0.26483337\n",
      "Iteration 40, loss = 0.26074079\n",
      "Iteration 41, loss = 0.26059822\n",
      "Iteration 42, loss = 0.25653063\n",
      "Iteration 43, loss = 0.25524150\n",
      "Iteration 44, loss = 0.25542178\n",
      "Iteration 45, loss = 0.25347867\n",
      "Iteration 46, loss = 0.24934599\n",
      "Iteration 47, loss = 0.24845172\n",
      "Iteration 48, loss = 0.24534924\n",
      "Iteration 49, loss = 0.24411212\n",
      "Iteration 50, loss = 0.24168733\n",
      "Iteration 51, loss = 0.24077599\n",
      "Iteration 52, loss = 0.23896338\n",
      "Iteration 53, loss = 0.23808403\n",
      "Iteration 54, loss = 0.23734285\n",
      "Iteration 55, loss = 0.23324159\n",
      "Iteration 56, loss = 0.23305239\n",
      "Iteration 57, loss = 0.23193015\n",
      "Iteration 58, loss = 0.23421644\n",
      "Iteration 59, loss = 0.23114701\n",
      "Iteration 60, loss = 0.22576873\n",
      "Iteration 61, loss = 0.22777489\n",
      "Iteration 62, loss = 0.22871598\n",
      "Iteration 63, loss = 0.22312465\n",
      "Iteration 64, loss = 0.22282950\n",
      "Iteration 65, loss = 0.22133974\n",
      "Iteration 66, loss = 0.22207463\n",
      "Iteration 67, loss = 0.21639598\n",
      "Iteration 68, loss = 0.21589077\n",
      "Iteration 69, loss = 0.21378025\n",
      "Iteration 70, loss = 0.21545162\n",
      "Iteration 71, loss = 0.21274604\n",
      "Iteration 72, loss = 0.21601387\n",
      "Iteration 73, loss = 0.21472896\n",
      "Iteration 74, loss = 0.20945916\n",
      "Iteration 75, loss = 0.20782720\n",
      "Iteration 76, loss = 0.20717288\n",
      "Iteration 77, loss = 0.20598584\n",
      "Iteration 78, loss = 0.20666194\n",
      "Iteration 79, loss = 0.20132949\n",
      "Iteration 80, loss = 0.20570546\n",
      "Iteration 81, loss = 0.20000246\n",
      "Iteration 82, loss = 0.20175427\n",
      "Iteration 83, loss = 0.20055976\n",
      "Iteration 84, loss = 0.19830904\n",
      "Iteration 85, loss = 0.19926625\n",
      "Iteration 86, loss = 0.19619709\n",
      "Iteration 87, loss = 0.20071103\n",
      "Iteration 88, loss = 0.19648814\n",
      "Iteration 89, loss = 0.19440398\n",
      "Iteration 90, loss = 0.19258449\n",
      "Iteration 91, loss = 0.19026079\n",
      "Iteration 92, loss = 0.19102325\n",
      "Iteration 93, loss = 0.19018273\n",
      "Iteration 94, loss = 0.18531755\n",
      "Iteration 95, loss = 0.18599297\n",
      "Iteration 96, loss = 0.19323979\n",
      "Iteration 97, loss = 0.18787530\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.20627143\n",
      "Iteration 2, loss = 0.60313749\n",
      "Iteration 3, loss = 0.49606980\n",
      "Iteration 4, loss = 0.45361535\n",
      "Iteration 5, loss = 0.42470265\n",
      "Iteration 6, loss = 0.40960916\n",
      "Iteration 7, loss = 0.39717899\n",
      "Iteration 8, loss = 0.38693221\n",
      "Iteration 9, loss = 0.37640245\n",
      "Iteration 10, loss = 0.36862961\n",
      "Iteration 11, loss = 0.36185358\n",
      "Iteration 12, loss = 0.35917338\n",
      "Iteration 13, loss = 0.35453212\n",
      "Iteration 14, loss = 0.34793731\n",
      "Iteration 15, loss = 0.34285265\n",
      "Iteration 16, loss = 0.33844885\n",
      "Iteration 17, loss = 0.33185825\n",
      "Iteration 18, loss = 0.32832797\n",
      "Iteration 19, loss = 0.32398344\n",
      "Iteration 20, loss = 0.32191494\n",
      "Iteration 21, loss = 0.31739915\n",
      "Iteration 22, loss = 0.31380479\n",
      "Iteration 23, loss = 0.31075177\n",
      "Iteration 24, loss = 0.30724545\n",
      "Iteration 25, loss = 0.30409424\n",
      "Iteration 26, loss = 0.29906557\n",
      "Iteration 27, loss = 0.29643459\n",
      "Iteration 28, loss = 0.29545044\n",
      "Iteration 29, loss = 0.28947496\n",
      "Iteration 30, loss = 0.28635434\n",
      "Iteration 31, loss = 0.28582603\n",
      "Iteration 32, loss = 0.27931633\n",
      "Iteration 33, loss = 0.28082024\n",
      "Iteration 34, loss = 0.27897948\n",
      "Iteration 35, loss = 0.27550386\n",
      "Iteration 36, loss = 0.27356697\n",
      "Iteration 37, loss = 0.27066882\n",
      "Iteration 38, loss = 0.26698310\n",
      "Iteration 39, loss = 0.26431162\n",
      "Iteration 40, loss = 0.26359541\n",
      "Iteration 41, loss = 0.25866152\n",
      "Iteration 42, loss = 0.25774685\n",
      "Iteration 43, loss = 0.25621437\n",
      "Iteration 44, loss = 0.25611027\n",
      "Iteration 45, loss = 0.25273809\n",
      "Iteration 46, loss = 0.25595705\n",
      "Iteration 47, loss = 0.24938446\n",
      "Iteration 48, loss = 0.24979379\n",
      "Iteration 49, loss = 0.24422776\n",
      "Iteration 50, loss = 0.24416818\n",
      "Iteration 51, loss = 0.24172541\n",
      "Iteration 52, loss = 0.24064961\n",
      "Iteration 53, loss = 0.24041423\n",
      "Iteration 54, loss = 0.23766012\n",
      "Iteration 55, loss = 0.23754468\n",
      "Iteration 56, loss = 0.23655105\n",
      "Iteration 57, loss = 0.23344694\n",
      "Iteration 58, loss = 0.23061556\n",
      "Iteration 59, loss = 0.23266766\n",
      "Iteration 60, loss = 0.23034508\n",
      "Iteration 61, loss = 0.22704120\n",
      "Iteration 62, loss = 0.22695251\n",
      "Iteration 63, loss = 0.22289625\n",
      "Iteration 64, loss = 0.22396468\n",
      "Iteration 65, loss = 0.22170379\n",
      "Iteration 66, loss = 0.22326162\n",
      "Iteration 67, loss = 0.21887185\n",
      "Iteration 68, loss = 0.21676236\n",
      "Iteration 69, loss = 0.21594851\n",
      "Iteration 70, loss = 0.21340442\n",
      "Iteration 71, loss = 0.21388734\n",
      "Iteration 72, loss = 0.21428470\n",
      "Iteration 73, loss = 0.21709257\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.07489527\n",
      "Iteration 2, loss = 0.54145496\n",
      "Iteration 3, loss = 0.46832893\n",
      "Iteration 4, loss = 0.43353242\n",
      "Iteration 5, loss = 0.41730294\n",
      "Iteration 6, loss = 0.40101539\n",
      "Iteration 7, loss = 0.38859210\n",
      "Iteration 8, loss = 0.38089699\n",
      "Iteration 9, loss = 0.37305358\n",
      "Iteration 10, loss = 0.36538130\n",
      "Iteration 11, loss = 0.36125006\n",
      "Iteration 12, loss = 0.35128484\n",
      "Iteration 13, loss = 0.34721920\n",
      "Iteration 14, loss = 0.34087509\n",
      "Iteration 15, loss = 0.33923984\n",
      "Iteration 16, loss = 0.33658128\n",
      "Iteration 17, loss = 0.32970692\n",
      "Iteration 18, loss = 0.32600003\n",
      "Iteration 19, loss = 0.32254493\n",
      "Iteration 20, loss = 0.32112249\n",
      "Iteration 21, loss = 0.31712810\n",
      "Iteration 22, loss = 0.31100395\n",
      "Iteration 23, loss = 0.30830228\n",
      "Iteration 24, loss = 0.30494216\n",
      "Iteration 25, loss = 0.30321490\n",
      "Iteration 26, loss = 0.30019401\n",
      "Iteration 27, loss = 0.29766636\n",
      "Iteration 28, loss = 0.29716176\n",
      "Iteration 29, loss = 0.29169419\n",
      "Iteration 30, loss = 0.29101581\n",
      "Iteration 31, loss = 0.29042332\n",
      "Iteration 32, loss = 0.28944016\n",
      "Iteration 33, loss = 0.27749153\n",
      "Iteration 34, loss = 0.27871988\n",
      "Iteration 35, loss = 0.28047719\n",
      "Iteration 36, loss = 0.27157281\n",
      "Iteration 37, loss = 0.27365382\n",
      "Iteration 38, loss = 0.26834068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39, loss = 0.26755722\n",
      "Iteration 40, loss = 0.26377271\n",
      "Iteration 41, loss = 0.26466830\n",
      "Iteration 42, loss = 0.26519801\n",
      "Iteration 43, loss = 0.26095450\n",
      "Iteration 44, loss = 0.25832248\n",
      "Iteration 45, loss = 0.25447497\n",
      "Iteration 46, loss = 0.25547531\n",
      "Iteration 47, loss = 0.25457106\n",
      "Iteration 48, loss = 0.25313848\n",
      "Iteration 49, loss = 0.24645470\n",
      "Iteration 50, loss = 0.24407614\n",
      "Iteration 51, loss = 0.24671040\n",
      "Iteration 52, loss = 0.24642780\n",
      "Iteration 53, loss = 0.24097239\n",
      "Iteration 54, loss = 0.23992061\n",
      "Iteration 55, loss = 0.23538977\n",
      "Iteration 56, loss = 0.23698233\n",
      "Iteration 57, loss = 0.23397529\n",
      "Iteration 58, loss = 0.23365507\n",
      "Iteration 59, loss = 0.23175638\n",
      "Iteration 60, loss = 0.23922486\n",
      "Iteration 61, loss = 0.22743438\n",
      "Iteration 62, loss = 0.23208796\n",
      "Iteration 63, loss = 0.22474999\n",
      "Iteration 64, loss = 0.22769805\n",
      "Iteration 65, loss = 0.22709549\n",
      "Iteration 66, loss = 0.22076761\n",
      "Iteration 67, loss = 0.22387154\n",
      "Iteration 68, loss = 0.21769123\n",
      "Iteration 69, loss = 0.21867484\n",
      "Iteration 70, loss = 0.21833278\n",
      "Iteration 71, loss = 0.21492155\n",
      "Iteration 72, loss = 0.21428642\n",
      "Iteration 73, loss = 0.21543016\n",
      "Iteration 74, loss = 0.21384124\n",
      "Iteration 75, loss = 0.20839900\n",
      "Iteration 76, loss = 0.20911679\n",
      "Iteration 77, loss = 0.20202633\n",
      "Iteration 78, loss = 0.21162538\n",
      "Iteration 79, loss = 0.20681288\n",
      "Iteration 80, loss = 0.20875942\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08562814\n",
      "Iteration 2, loss = 0.55115601\n",
      "Iteration 3, loss = 0.47361231\n",
      "Iteration 4, loss = 0.43998650\n",
      "Iteration 5, loss = 0.42075674\n",
      "Iteration 6, loss = 0.40731948\n",
      "Iteration 7, loss = 0.39181105\n",
      "Iteration 8, loss = 0.38435876\n",
      "Iteration 9, loss = 0.37449652\n",
      "Iteration 10, loss = 0.37033768\n",
      "Iteration 11, loss = 0.36294000\n",
      "Iteration 12, loss = 0.35515805\n",
      "Iteration 13, loss = 0.35310955\n",
      "Iteration 14, loss = 0.34789825\n",
      "Iteration 15, loss = 0.34197314\n",
      "Iteration 16, loss = 0.33366857\n",
      "Iteration 17, loss = 0.33106032\n",
      "Iteration 18, loss = 0.32758535\n",
      "Iteration 19, loss = 0.32349168\n",
      "Iteration 20, loss = 0.31987472\n",
      "Iteration 21, loss = 0.31719266\n",
      "Iteration 22, loss = 0.31243171\n",
      "Iteration 23, loss = 0.31484269\n",
      "Iteration 24, loss = 0.30692696\n",
      "Iteration 25, loss = 0.30452440\n",
      "Iteration 26, loss = 0.30091899\n",
      "Iteration 27, loss = 0.29796053\n",
      "Iteration 28, loss = 0.29376234\n",
      "Iteration 29, loss = 0.29680001\n",
      "Iteration 30, loss = 0.29136162\n",
      "Iteration 31, loss = 0.28924067\n",
      "Iteration 32, loss = 0.28332334\n",
      "Iteration 33, loss = 0.28249541\n",
      "Iteration 34, loss = 0.27911781\n",
      "Iteration 35, loss = 0.27948142\n",
      "Iteration 36, loss = 0.27284867\n",
      "Iteration 37, loss = 0.27135667\n",
      "Iteration 38, loss = 0.26877989\n",
      "Iteration 39, loss = 0.26573869\n",
      "Iteration 40, loss = 0.26428364\n",
      "Iteration 41, loss = 0.26633908\n",
      "Iteration 42, loss = 0.26219931\n",
      "Iteration 43, loss = 0.25490570\n",
      "Iteration 44, loss = 0.25776849\n",
      "Iteration 45, loss = 0.25528898\n",
      "Iteration 46, loss = 0.25537879\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08174123\n",
      "Iteration 2, loss = 0.55021492\n",
      "Iteration 3, loss = 0.47197672\n",
      "Iteration 4, loss = 0.43766926\n",
      "Iteration 5, loss = 0.41807636\n",
      "Iteration 6, loss = 0.40723613\n",
      "Iteration 7, loss = 0.39672651\n",
      "Iteration 8, loss = 0.38499201\n",
      "Iteration 9, loss = 0.37549767\n",
      "Iteration 10, loss = 0.37048228\n",
      "Iteration 11, loss = 0.36363040\n",
      "Iteration 12, loss = 0.35797749\n",
      "Iteration 13, loss = 0.35108717\n",
      "Iteration 14, loss = 0.34801468\n",
      "Iteration 15, loss = 0.34327600\n",
      "Iteration 16, loss = 0.33732047\n",
      "Iteration 17, loss = 0.33230204\n",
      "Iteration 18, loss = 0.32898674\n",
      "Iteration 19, loss = 0.32727301\n",
      "Iteration 20, loss = 0.32257494\n",
      "Iteration 21, loss = 0.31706801\n",
      "Iteration 22, loss = 0.31408662\n",
      "Iteration 23, loss = 0.31068707\n",
      "Iteration 24, loss = 0.31082140\n",
      "Iteration 25, loss = 0.30284436\n",
      "Iteration 26, loss = 0.30161445\n",
      "Iteration 27, loss = 0.29727910\n",
      "Iteration 28, loss = 0.29536835\n",
      "Iteration 29, loss = 0.29601324\n",
      "Iteration 30, loss = 0.28907447\n",
      "Iteration 31, loss = 0.28716950\n",
      "Iteration 32, loss = 0.28630830\n",
      "Iteration 33, loss = 0.28346973\n",
      "Iteration 34, loss = 0.28042175\n",
      "Iteration 35, loss = 0.27654498\n",
      "Iteration 36, loss = 0.27379825\n",
      "Iteration 37, loss = 0.27516432\n",
      "Iteration 38, loss = 0.27484764\n",
      "Iteration 39, loss = 0.26964827\n",
      "Iteration 40, loss = 0.26741269\n",
      "Iteration 41, loss = 0.26532853\n",
      "Iteration 42, loss = 0.26187171\n",
      "Iteration 43, loss = 0.26114077\n",
      "Iteration 44, loss = 0.25934741\n",
      "Iteration 45, loss = 0.25491492\n",
      "Iteration 46, loss = 0.25946310\n",
      "Iteration 47, loss = 0.24978014\n",
      "Iteration 48, loss = 0.25054645\n",
      "Iteration 49, loss = 0.24614246\n",
      "Iteration 50, loss = 0.25238261\n",
      "Iteration 51, loss = 0.24387611\n",
      "Iteration 52, loss = 0.24728534\n",
      "Iteration 53, loss = 0.24195822\n",
      "Iteration 54, loss = 0.24410491\n",
      "Iteration 55, loss = 0.24081134\n",
      "Iteration 56, loss = 0.24201361\n",
      "Iteration 57, loss = 0.23668961\n",
      "Iteration 58, loss = 0.23445625\n",
      "Iteration 59, loss = 0.23501894\n",
      "Iteration 60, loss = 0.23296760\n",
      "Iteration 61, loss = 0.23524944\n",
      "Iteration 62, loss = 0.22679858\n",
      "Iteration 63, loss = 0.22538503\n",
      "Iteration 64, loss = 0.22617968\n",
      "Iteration 65, loss = 0.22596125\n",
      "Iteration 66, loss = 0.22153960\n",
      "Iteration 67, loss = 0.22521096\n",
      "Iteration 68, loss = 0.21697716\n",
      "Iteration 69, loss = 0.21969741\n",
      "Iteration 70, loss = 0.21790231\n",
      "Iteration 71, loss = 0.21620540\n",
      "Iteration 72, loss = 0.21595728\n",
      "Iteration 73, loss = 0.21772864\n",
      "Iteration 74, loss = 0.21225403\n",
      "Iteration 75, loss = 0.21675550\n",
      "Iteration 76, loss = 0.21208060\n",
      "Iteration 77, loss = 0.21079633\n",
      "Iteration 78, loss = 0.21326485\n",
      "Iteration 79, loss = 0.21248843\n",
      "Iteration 80, loss = 0.20563206\n",
      "Iteration 81, loss = 0.20775308\n",
      "Iteration 82, loss = 0.20078170\n",
      "Iteration 83, loss = 0.20569975\n",
      "Iteration 84, loss = 0.20353119\n",
      "Iteration 85, loss = 0.19951500\n",
      "Iteration 86, loss = 0.19907778\n",
      "Iteration 87, loss = 0.20325427\n",
      "Iteration 88, loss = 0.19742660\n",
      "Iteration 89, loss = 0.19539684\n",
      "Iteration 90, loss = 0.19874558\n",
      "Iteration 91, loss = 0.20363819\n",
      "Iteration 92, loss = 0.19695737\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.83285896\n",
      "Iteration 2, loss = 0.53361349\n",
      "Iteration 3, loss = 0.48903008\n",
      "Iteration 4, loss = 0.46664300\n",
      "Iteration 5, loss = 0.44994280\n",
      "Iteration 6, loss = 0.43686333\n",
      "Iteration 7, loss = 0.43087069\n",
      "Iteration 8, loss = 0.42132683\n",
      "Iteration 9, loss = 0.41602904\n",
      "Iteration 10, loss = 0.41077104\n",
      "Iteration 11, loss = 0.40510277\n",
      "Iteration 12, loss = 0.40201339\n",
      "Iteration 13, loss = 0.40047129\n",
      "Iteration 14, loss = 0.39496989\n",
      "Iteration 15, loss = 0.38984097\n",
      "Iteration 16, loss = 0.38980369\n",
      "Iteration 17, loss = 0.38406801\n",
      "Iteration 18, loss = 0.38251369\n",
      "Iteration 19, loss = 0.38278100\n",
      "Iteration 20, loss = 0.38142933\n",
      "Iteration 21, loss = 0.37412285\n",
      "Iteration 22, loss = 0.37366049\n",
      "Iteration 23, loss = 0.37271040\n",
      "Iteration 24, loss = 0.37096165\n",
      "Iteration 25, loss = 0.36695194\n",
      "Iteration 26, loss = 0.36697060\n",
      "Iteration 27, loss = 0.36165012\n",
      "Iteration 28, loss = 0.36316232\n",
      "Iteration 29, loss = 0.36200461\n",
      "Iteration 30, loss = 0.35891245\n",
      "Iteration 31, loss = 0.35541398\n",
      "Iteration 32, loss = 0.35716523\n",
      "Iteration 33, loss = 0.35361878\n",
      "Iteration 34, loss = 0.35384204\n",
      "Iteration 35, loss = 0.35332557\n",
      "Iteration 36, loss = 0.35184441\n",
      "Iteration 37, loss = 0.34992491\n",
      "Iteration 38, loss = 0.34971873\n",
      "Iteration 39, loss = 0.34991366\n",
      "Iteration 40, loss = 0.34852769\n",
      "Iteration 41, loss = 0.34947343\n",
      "Iteration 42, loss = 0.34383605\n",
      "Iteration 43, loss = 0.34556596\n",
      "Iteration 44, loss = 0.34424800\n",
      "Iteration 45, loss = 0.34314843\n",
      "Iteration 46, loss = 0.34361378\n",
      "Iteration 47, loss = 0.34064069\n",
      "Iteration 48, loss = 0.34226485\n",
      "Iteration 49, loss = 0.34187714\n",
      "Iteration 50, loss = 0.33918274\n",
      "Iteration 51, loss = 0.33888173\n",
      "Iteration 52, loss = 0.33794632\n",
      "Iteration 53, loss = 0.33540012\n",
      "Iteration 54, loss = 0.33729744\n",
      "Iteration 55, loss = 0.33457837\n",
      "Iteration 56, loss = 0.33522948\n",
      "Iteration 57, loss = 0.33299159\n",
      "Iteration 58, loss = 0.33296374\n",
      "Iteration 59, loss = 0.33618371\n",
      "Iteration 60, loss = 0.33423104\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.83751845\n",
      "Iteration 2, loss = 0.53753816\n",
      "Iteration 3, loss = 0.49391077\n",
      "Iteration 4, loss = 0.46924975\n",
      "Iteration 5, loss = 0.45408620\n",
      "Iteration 6, loss = 0.44552574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.43623418\n",
      "Iteration 8, loss = 0.42724817\n",
      "Iteration 9, loss = 0.42275989\n",
      "Iteration 10, loss = 0.41640437\n",
      "Iteration 11, loss = 0.40937129\n",
      "Iteration 12, loss = 0.40634626\n",
      "Iteration 13, loss = 0.40153036\n",
      "Iteration 14, loss = 0.40233498\n",
      "Iteration 15, loss = 0.39415174\n",
      "Iteration 16, loss = 0.39239266\n",
      "Iteration 17, loss = 0.38965784\n",
      "Iteration 18, loss = 0.38703130\n",
      "Iteration 19, loss = 0.38398011\n",
      "Iteration 20, loss = 0.38189866\n",
      "Iteration 21, loss = 0.37685713\n",
      "Iteration 22, loss = 0.37407872\n",
      "Iteration 23, loss = 0.37436962\n",
      "Iteration 24, loss = 0.37062572\n",
      "Iteration 25, loss = 0.36786567\n",
      "Iteration 26, loss = 0.36808865\n",
      "Iteration 27, loss = 0.36741755\n",
      "Iteration 28, loss = 0.36382036\n",
      "Iteration 29, loss = 0.36534988\n",
      "Iteration 30, loss = 0.36015575\n",
      "Iteration 31, loss = 0.35822285\n",
      "Iteration 32, loss = 0.36127331\n",
      "Iteration 33, loss = 0.35650408\n",
      "Iteration 34, loss = 0.35330686\n",
      "Iteration 35, loss = 0.35655550\n",
      "Iteration 36, loss = 0.35563601\n",
      "Iteration 37, loss = 0.35224027\n",
      "Iteration 38, loss = 0.35074204\n",
      "Iteration 39, loss = 0.35050585\n",
      "Iteration 40, loss = 0.35310670\n",
      "Iteration 41, loss = 0.35072739\n",
      "Iteration 42, loss = 0.34619211\n",
      "Iteration 43, loss = 0.34891901\n",
      "Iteration 44, loss = 0.34800823\n",
      "Iteration 45, loss = 0.34797868\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.84685008\n",
      "Iteration 2, loss = 0.53443821\n",
      "Iteration 3, loss = 0.48950049\n",
      "Iteration 4, loss = 0.46700823\n",
      "Iteration 5, loss = 0.45459246\n",
      "Iteration 6, loss = 0.44475182\n",
      "Iteration 7, loss = 0.43567614\n",
      "Iteration 8, loss = 0.42640860\n",
      "Iteration 9, loss = 0.42193874\n",
      "Iteration 10, loss = 0.41915616\n",
      "Iteration 11, loss = 0.41005336\n",
      "Iteration 12, loss = 0.40837402\n",
      "Iteration 13, loss = 0.40340175\n",
      "Iteration 14, loss = 0.40037261\n",
      "Iteration 15, loss = 0.39669382\n",
      "Iteration 16, loss = 0.39297178\n",
      "Iteration 17, loss = 0.39202925\n",
      "Iteration 18, loss = 0.38660116\n",
      "Iteration 19, loss = 0.38026819\n",
      "Iteration 20, loss = 0.38029093\n",
      "Iteration 21, loss = 0.38035547\n",
      "Iteration 22, loss = 0.37562498\n",
      "Iteration 23, loss = 0.37740774\n",
      "Iteration 24, loss = 0.37248800\n",
      "Iteration 25, loss = 0.37011567\n",
      "Iteration 26, loss = 0.36895914\n",
      "Iteration 27, loss = 0.36471210\n",
      "Iteration 28, loss = 0.36729335\n",
      "Iteration 29, loss = 0.36223768\n",
      "Iteration 30, loss = 0.36244692\n",
      "Iteration 31, loss = 0.36082248\n",
      "Iteration 32, loss = 0.35848808\n",
      "Iteration 33, loss = 0.35925511\n",
      "Iteration 34, loss = 0.36062868\n",
      "Iteration 35, loss = 0.35494840\n",
      "Iteration 36, loss = 0.35563161\n",
      "Iteration 37, loss = 0.35151394\n",
      "Iteration 38, loss = 0.35281412\n",
      "Iteration 39, loss = 0.35130059\n",
      "Iteration 40, loss = 0.35155704\n",
      "Iteration 41, loss = 0.34822295\n",
      "Iteration 42, loss = 0.34999203\n",
      "Iteration 43, loss = 0.34693115\n",
      "Iteration 44, loss = 0.34606290\n",
      "Iteration 45, loss = 0.34519834\n",
      "Iteration 46, loss = 0.34507125\n",
      "Iteration 47, loss = 0.34289890\n",
      "Iteration 48, loss = 0.34360494\n",
      "Iteration 49, loss = 0.34010849\n",
      "Iteration 50, loss = 0.34217964\n",
      "Iteration 51, loss = 0.34225518\n",
      "Iteration 52, loss = 0.33921541\n",
      "Iteration 53, loss = 0.33984083\n",
      "Iteration 54, loss = 0.33966963\n",
      "Iteration 55, loss = 0.33639747\n",
      "Iteration 56, loss = 0.34094132\n",
      "Iteration 57, loss = 0.33820157\n",
      "Iteration 58, loss = 0.33871378\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.95641330\n",
      "Iteration 2, loss = 0.56807325\n",
      "Iteration 3, loss = 0.50136205\n",
      "Iteration 4, loss = 0.46981180\n",
      "Iteration 5, loss = 0.45131421\n",
      "Iteration 6, loss = 0.43811829\n",
      "Iteration 7, loss = 0.42844972\n",
      "Iteration 8, loss = 0.42103119\n",
      "Iteration 9, loss = 0.41325881\n",
      "Iteration 10, loss = 0.40732335\n",
      "Iteration 11, loss = 0.40252316\n",
      "Iteration 12, loss = 0.39827059\n",
      "Iteration 13, loss = 0.39409743\n",
      "Iteration 14, loss = 0.39053061\n",
      "Iteration 15, loss = 0.38588557\n",
      "Iteration 16, loss = 0.38267587\n",
      "Iteration 17, loss = 0.37888629\n",
      "Iteration 18, loss = 0.37827878\n",
      "Iteration 19, loss = 0.37728589\n",
      "Iteration 20, loss = 0.37191903\n",
      "Iteration 21, loss = 0.37041093\n",
      "Iteration 22, loss = 0.36937910\n",
      "Iteration 23, loss = 0.36410349\n",
      "Iteration 24, loss = 0.36532958\n",
      "Iteration 25, loss = 0.36326107\n",
      "Iteration 26, loss = 0.36123053\n",
      "Iteration 27, loss = 0.35823499\n",
      "Iteration 28, loss = 0.35873715\n",
      "Iteration 29, loss = 0.35581149\n",
      "Iteration 30, loss = 0.35436258\n",
      "Iteration 31, loss = 0.35305419\n",
      "Iteration 32, loss = 0.35167614\n",
      "Iteration 33, loss = 0.34792581\n",
      "Iteration 34, loss = 0.34870869\n",
      "Iteration 35, loss = 0.34680761\n",
      "Iteration 36, loss = 0.34530487\n",
      "Iteration 37, loss = 0.34436645\n",
      "Iteration 38, loss = 0.34291813\n",
      "Iteration 39, loss = 0.34096165\n",
      "Iteration 40, loss = 0.34140740\n",
      "Iteration 41, loss = 0.33961305\n",
      "Iteration 42, loss = 0.33829394\n",
      "Iteration 43, loss = 0.33756663\n",
      "Iteration 44, loss = 0.33813813\n",
      "Iteration 45, loss = 0.33737967\n",
      "Iteration 46, loss = 0.33441808\n",
      "Iteration 47, loss = 0.33457872\n",
      "Iteration 48, loss = 0.33443848\n",
      "Iteration 49, loss = 0.33416799\n",
      "Iteration 50, loss = 0.33438709\n",
      "Iteration 51, loss = 0.32998856\n",
      "Iteration 52, loss = 0.33290246\n",
      "Iteration 53, loss = 0.32885126\n",
      "Iteration 54, loss = 0.32838370\n",
      "Iteration 55, loss = 0.32930348\n",
      "Iteration 56, loss = 0.32984342\n",
      "Iteration 57, loss = 0.32535851\n",
      "Iteration 58, loss = 0.32565544\n",
      "Iteration 59, loss = 0.32753096\n",
      "Iteration 60, loss = 0.32681565\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.95575147\n",
      "Iteration 2, loss = 0.56860179\n",
      "Iteration 3, loss = 0.50330751\n",
      "Iteration 4, loss = 0.47308218\n",
      "Iteration 5, loss = 0.45539946\n",
      "Iteration 6, loss = 0.44227483\n",
      "Iteration 7, loss = 0.43160638\n",
      "Iteration 8, loss = 0.42321569\n",
      "Iteration 9, loss = 0.41779756\n",
      "Iteration 10, loss = 0.41104450\n",
      "Iteration 11, loss = 0.40620218\n",
      "Iteration 12, loss = 0.40131734\n",
      "Iteration 13, loss = 0.39789285\n",
      "Iteration 14, loss = 0.39332996\n",
      "Iteration 15, loss = 0.39020937\n",
      "Iteration 16, loss = 0.38504288\n",
      "Iteration 17, loss = 0.38393117\n",
      "Iteration 18, loss = 0.38030509\n",
      "Iteration 19, loss = 0.37802199\n",
      "Iteration 20, loss = 0.37722712\n",
      "Iteration 21, loss = 0.37255102\n",
      "Iteration 22, loss = 0.37296787\n",
      "Iteration 23, loss = 0.37170472\n",
      "Iteration 24, loss = 0.36657508\n",
      "Iteration 25, loss = 0.36371522\n",
      "Iteration 26, loss = 0.36317044\n",
      "Iteration 27, loss = 0.35934876\n",
      "Iteration 28, loss = 0.36021806\n",
      "Iteration 29, loss = 0.35816959\n",
      "Iteration 30, loss = 0.35723801\n",
      "Iteration 31, loss = 0.35639560\n",
      "Iteration 32, loss = 0.35382630\n",
      "Iteration 33, loss = 0.35237901\n",
      "Iteration 34, loss = 0.35141819\n",
      "Iteration 35, loss = 0.35016621\n",
      "Iteration 36, loss = 0.35061900\n",
      "Iteration 37, loss = 0.34691732\n",
      "Iteration 38, loss = 0.34430970\n",
      "Iteration 39, loss = 0.34400294\n",
      "Iteration 40, loss = 0.34360594\n",
      "Iteration 41, loss = 0.34185932\n",
      "Iteration 42, loss = 0.34103646\n",
      "Iteration 43, loss = 0.34172557\n",
      "Iteration 44, loss = 0.33918204\n",
      "Iteration 45, loss = 0.33941525\n",
      "Iteration 46, loss = 0.33885226\n",
      "Iteration 47, loss = 0.33633576\n",
      "Iteration 48, loss = 0.33551616\n",
      "Iteration 49, loss = 0.33517464\n",
      "Iteration 50, loss = 0.33647961\n",
      "Iteration 51, loss = 0.33453663\n",
      "Iteration 52, loss = 0.33532988\n",
      "Iteration 53, loss = 0.33287129\n",
      "Iteration 54, loss = 0.33353104\n",
      "Iteration 55, loss = 0.33048124\n",
      "Iteration 56, loss = 0.33242123\n",
      "Iteration 57, loss = 0.32932416\n",
      "Iteration 58, loss = 0.33012469\n",
      "Iteration 59, loss = 0.32907724\n",
      "Iteration 60, loss = 0.32708676\n",
      "Iteration 61, loss = 0.32684637\n",
      "Iteration 62, loss = 0.32799429\n",
      "Iteration 63, loss = 0.32698789\n",
      "Iteration 64, loss = 0.32388781\n",
      "Iteration 65, loss = 0.32521825\n",
      "Iteration 66, loss = 0.32674879\n",
      "Iteration 67, loss = 0.32437587\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.95758481\n",
      "Iteration 2, loss = 0.56719616\n",
      "Iteration 3, loss = 0.50079276\n",
      "Iteration 4, loss = 0.47293162\n",
      "Iteration 5, loss = 0.45465912\n",
      "Iteration 6, loss = 0.44288274\n",
      "Iteration 7, loss = 0.43295411\n",
      "Iteration 8, loss = 0.42619676\n",
      "Iteration 9, loss = 0.41908866\n",
      "Iteration 10, loss = 0.41159353\n",
      "Iteration 11, loss = 0.40771440\n",
      "Iteration 12, loss = 0.40396779\n",
      "Iteration 13, loss = 0.39731539\n",
      "Iteration 14, loss = 0.39451054\n",
      "Iteration 15, loss = 0.39147559\n",
      "Iteration 16, loss = 0.38751034\n",
      "Iteration 17, loss = 0.38594129\n",
      "Iteration 18, loss = 0.38059407\n",
      "Iteration 19, loss = 0.38038229\n",
      "Iteration 20, loss = 0.37804846\n",
      "Iteration 21, loss = 0.37493172\n",
      "Iteration 22, loss = 0.37257986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23, loss = 0.37256029\n",
      "Iteration 24, loss = 0.36667874\n",
      "Iteration 25, loss = 0.36527950\n",
      "Iteration 26, loss = 0.36295096\n",
      "Iteration 27, loss = 0.36242815\n",
      "Iteration 28, loss = 0.36088589\n",
      "Iteration 29, loss = 0.35794646\n",
      "Iteration 30, loss = 0.35678639\n",
      "Iteration 31, loss = 0.35566582\n",
      "Iteration 32, loss = 0.35262607\n",
      "Iteration 33, loss = 0.35259930\n",
      "Iteration 34, loss = 0.35115549\n",
      "Iteration 35, loss = 0.35110739\n",
      "Iteration 36, loss = 0.35154324\n",
      "Iteration 37, loss = 0.34819599\n",
      "Iteration 38, loss = 0.34490795\n",
      "Iteration 39, loss = 0.34481070\n",
      "Iteration 40, loss = 0.34540735\n",
      "Iteration 41, loss = 0.34160411\n",
      "Iteration 42, loss = 0.34105671\n",
      "Iteration 43, loss = 0.34266004\n",
      "Iteration 44, loss = 0.34108747\n",
      "Iteration 45, loss = 0.33762253\n",
      "Iteration 46, loss = 0.33748327\n",
      "Iteration 47, loss = 0.33617278\n",
      "Iteration 48, loss = 0.33895007\n",
      "Iteration 49, loss = 0.33704061\n",
      "Iteration 50, loss = 0.33306199\n",
      "Iteration 51, loss = 0.33283485\n",
      "Iteration 52, loss = 0.33542013\n",
      "Iteration 53, loss = 0.33186186\n",
      "Iteration 54, loss = 0.33202022\n",
      "Iteration 55, loss = 0.33208887\n",
      "Iteration 56, loss = 0.33519094\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.02714985\n",
      "Iteration 2, loss = 0.57331897\n",
      "Iteration 3, loss = 0.51091764\n",
      "Iteration 4, loss = 0.48401446\n",
      "Iteration 5, loss = 0.46355430\n",
      "Iteration 6, loss = 0.45403645\n",
      "Iteration 7, loss = 0.44639926\n",
      "Iteration 8, loss = 0.43431145\n",
      "Iteration 9, loss = 0.42825414\n",
      "Iteration 10, loss = 0.42492882\n",
      "Iteration 11, loss = 0.41970437\n",
      "Iteration 12, loss = 0.41943683\n",
      "Iteration 13, loss = 0.41195039\n",
      "Iteration 14, loss = 0.40791642\n",
      "Iteration 15, loss = 0.40822649\n",
      "Iteration 16, loss = 0.40560761\n",
      "Iteration 17, loss = 0.39931708\n",
      "Iteration 18, loss = 0.40067430\n",
      "Iteration 19, loss = 0.39803291\n",
      "Iteration 20, loss = 0.38862704\n",
      "Iteration 21, loss = 0.38992463\n",
      "Iteration 22, loss = 0.38617825\n",
      "Iteration 23, loss = 0.38542667\n",
      "Iteration 24, loss = 0.37956041\n",
      "Iteration 25, loss = 0.38032796\n",
      "Iteration 26, loss = 0.37648203\n",
      "Iteration 27, loss = 0.37417533\n",
      "Iteration 28, loss = 0.37655675\n",
      "Iteration 29, loss = 0.37387275\n",
      "Iteration 30, loss = 0.36950613\n",
      "Iteration 31, loss = 0.37082025\n",
      "Iteration 32, loss = 0.36723255\n",
      "Iteration 33, loss = 0.36658252\n",
      "Iteration 34, loss = 0.36266741\n",
      "Iteration 35, loss = 0.36543805\n",
      "Iteration 36, loss = 0.35836692\n",
      "Iteration 37, loss = 0.35735780\n",
      "Iteration 38, loss = 0.35696029\n",
      "Iteration 39, loss = 0.36294645\n",
      "Iteration 40, loss = 0.35740229\n",
      "Iteration 41, loss = 0.35266029\n",
      "Iteration 42, loss = 0.35342608\n",
      "Iteration 43, loss = 0.35256523\n",
      "Iteration 44, loss = 0.35208638\n",
      "Iteration 45, loss = 0.35186467\n",
      "Iteration 46, loss = 0.34339942\n",
      "Iteration 47, loss = 0.34771049\n",
      "Iteration 48, loss = 0.34910568\n",
      "Iteration 49, loss = 0.34990942\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.02618357\n",
      "Iteration 2, loss = 0.57312172\n",
      "Iteration 3, loss = 0.50946953\n",
      "Iteration 4, loss = 0.48640381\n",
      "Iteration 5, loss = 0.46835135\n",
      "Iteration 6, loss = 0.45765340\n",
      "Iteration 7, loss = 0.45492304\n",
      "Iteration 8, loss = 0.44197959\n",
      "Iteration 9, loss = 0.43612494\n",
      "Iteration 10, loss = 0.43199864\n",
      "Iteration 11, loss = 0.42350548\n",
      "Iteration 12, loss = 0.42169545\n",
      "Iteration 13, loss = 0.41780423\n",
      "Iteration 14, loss = 0.41685980\n",
      "Iteration 15, loss = 0.41225882\n",
      "Iteration 16, loss = 0.40997203\n",
      "Iteration 17, loss = 0.39869156\n",
      "Iteration 18, loss = 0.39911600\n",
      "Iteration 19, loss = 0.40243096\n",
      "Iteration 20, loss = 0.39450256\n",
      "Iteration 21, loss = 0.39489565\n",
      "Iteration 22, loss = 0.38954154\n",
      "Iteration 23, loss = 0.38897490\n",
      "Iteration 24, loss = 0.38455201\n",
      "Iteration 25, loss = 0.38470781\n",
      "Iteration 26, loss = 0.38262494\n",
      "Iteration 27, loss = 0.37945419\n",
      "Iteration 28, loss = 0.38442543\n",
      "Iteration 29, loss = 0.38007560\n",
      "Iteration 30, loss = 0.37345572\n",
      "Iteration 31, loss = 0.37166805\n",
      "Iteration 32, loss = 0.37265929\n",
      "Iteration 33, loss = 0.36965968\n",
      "Iteration 34, loss = 0.36474511\n",
      "Iteration 35, loss = 0.36530626\n",
      "Iteration 36, loss = 0.36409834\n",
      "Iteration 37, loss = 0.35935226\n",
      "Iteration 38, loss = 0.36536832\n",
      "Iteration 39, loss = 0.35937589\n",
      "Iteration 40, loss = 0.35518770\n",
      "Iteration 41, loss = 0.35862193\n",
      "Iteration 42, loss = 0.35538181\n",
      "Iteration 43, loss = 0.35362876\n",
      "Iteration 44, loss = 0.35299483\n",
      "Iteration 45, loss = 0.35370822\n",
      "Iteration 46, loss = 0.35023653\n",
      "Iteration 47, loss = 0.35183387\n",
      "Iteration 48, loss = 0.35119293\n",
      "Iteration 49, loss = 0.35119323\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.02565442\n",
      "Iteration 2, loss = 0.56931909\n",
      "Iteration 3, loss = 0.51051453\n",
      "Iteration 4, loss = 0.48430986\n",
      "Iteration 5, loss = 0.46915052\n",
      "Iteration 6, loss = 0.45614288\n",
      "Iteration 7, loss = 0.44908597\n",
      "Iteration 8, loss = 0.44688728\n",
      "Iteration 9, loss = 0.43753245\n",
      "Iteration 10, loss = 0.42881793\n",
      "Iteration 11, loss = 0.42612595\n",
      "Iteration 12, loss = 0.42143275\n",
      "Iteration 13, loss = 0.41622028\n",
      "Iteration 14, loss = 0.41627964\n",
      "Iteration 15, loss = 0.40759771\n",
      "Iteration 16, loss = 0.40668693\n",
      "Iteration 17, loss = 0.40318091\n",
      "Iteration 18, loss = 0.39594248\n",
      "Iteration 19, loss = 0.39756504\n",
      "Iteration 20, loss = 0.39163729\n",
      "Iteration 21, loss = 0.39438174\n",
      "Iteration 22, loss = 0.39014156\n",
      "Iteration 23, loss = 0.38897927\n",
      "Iteration 24, loss = 0.38349354\n",
      "Iteration 25, loss = 0.38216955\n",
      "Iteration 26, loss = 0.37727267\n",
      "Iteration 27, loss = 0.38071246\n",
      "Iteration 28, loss = 0.37620964\n",
      "Iteration 29, loss = 0.37390013\n",
      "Iteration 30, loss = 0.37219115\n",
      "Iteration 31, loss = 0.36973732\n",
      "Iteration 32, loss = 0.37105684\n",
      "Iteration 33, loss = 0.36418126\n",
      "Iteration 34, loss = 0.36685819\n",
      "Iteration 35, loss = 0.36839202\n",
      "Iteration 36, loss = 0.36195892\n",
      "Iteration 37, loss = 0.35919186\n",
      "Iteration 38, loss = 0.35909545\n",
      "Iteration 39, loss = 0.36113860\n",
      "Iteration 40, loss = 0.35679606\n",
      "Iteration 41, loss = 0.35693255\n",
      "Iteration 42, loss = 0.35701709\n",
      "Iteration 43, loss = 0.35316194\n",
      "Iteration 44, loss = 0.34984610\n",
      "Iteration 45, loss = 0.35184590\n",
      "Iteration 46, loss = 0.35298812\n",
      "Iteration 47, loss = 0.35297018\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.23573716\n",
      "Iteration 2, loss = 0.62747479\n",
      "Iteration 3, loss = 0.52880356\n",
      "Iteration 4, loss = 0.48964692\n",
      "Iteration 5, loss = 0.46496097\n",
      "Iteration 6, loss = 0.45196813\n",
      "Iteration 7, loss = 0.44270367\n",
      "Iteration 8, loss = 0.43467080\n",
      "Iteration 9, loss = 0.42361731\n",
      "Iteration 10, loss = 0.41674109\n",
      "Iteration 11, loss = 0.41263248\n",
      "Iteration 12, loss = 0.40704385\n",
      "Iteration 13, loss = 0.40315450\n",
      "Iteration 14, loss = 0.40100006\n",
      "Iteration 15, loss = 0.39371990\n",
      "Iteration 16, loss = 0.39331947\n",
      "Iteration 17, loss = 0.38838319\n",
      "Iteration 18, loss = 0.38712618\n",
      "Iteration 19, loss = 0.38609781\n",
      "Iteration 20, loss = 0.38266764\n",
      "Iteration 21, loss = 0.37847611\n",
      "Iteration 22, loss = 0.37474199\n",
      "Iteration 23, loss = 0.37649367\n",
      "Iteration 24, loss = 0.37209613\n",
      "Iteration 25, loss = 0.37055040\n",
      "Iteration 26, loss = 0.36793724\n",
      "Iteration 27, loss = 0.36216325\n",
      "Iteration 28, loss = 0.36394111\n",
      "Iteration 29, loss = 0.36015895\n",
      "Iteration 30, loss = 0.36241504\n",
      "Iteration 31, loss = 0.35550889\n",
      "Iteration 32, loss = 0.35754524\n",
      "Iteration 33, loss = 0.35723977\n",
      "Iteration 34, loss = 0.35333824\n",
      "Iteration 35, loss = 0.35201343\n",
      "Iteration 36, loss = 0.35000072\n",
      "Iteration 37, loss = 0.34892096\n",
      "Iteration 38, loss = 0.34675013\n",
      "Iteration 39, loss = 0.34984042\n",
      "Iteration 40, loss = 0.34544693\n",
      "Iteration 41, loss = 0.34408904\n",
      "Iteration 42, loss = 0.34137903\n",
      "Iteration 43, loss = 0.34095434\n",
      "Iteration 44, loss = 0.33904723\n",
      "Iteration 45, loss = 0.33871268\n",
      "Iteration 46, loss = 0.33760192\n",
      "Iteration 47, loss = 0.33663008\n",
      "Iteration 48, loss = 0.33696479\n",
      "Iteration 49, loss = 0.33351868\n",
      "Iteration 50, loss = 0.33647737\n",
      "Iteration 51, loss = 0.33387853\n",
      "Iteration 52, loss = 0.33282398\n",
      "Iteration 53, loss = 0.33048506\n",
      "Iteration 54, loss = 0.33205652\n",
      "Iteration 55, loss = 0.32979738\n",
      "Iteration 56, loss = 0.32823832\n",
      "Iteration 57, loss = 0.32835644\n",
      "Iteration 58, loss = 0.32445810\n",
      "Iteration 59, loss = 0.32670879\n",
      "Iteration 60, loss = 0.32532424\n",
      "Iteration 61, loss = 0.32495706\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.22249828\n",
      "Iteration 2, loss = 0.63020981\n",
      "Iteration 3, loss = 0.53315065\n",
      "Iteration 4, loss = 0.49254993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.47022590\n",
      "Iteration 6, loss = 0.45468099\n",
      "Iteration 7, loss = 0.44527762\n",
      "Iteration 8, loss = 0.43628301\n",
      "Iteration 9, loss = 0.42591081\n",
      "Iteration 10, loss = 0.41970408\n",
      "Iteration 11, loss = 0.41535977\n",
      "Iteration 12, loss = 0.40941029\n",
      "Iteration 13, loss = 0.40894335\n",
      "Iteration 14, loss = 0.40188096\n",
      "Iteration 15, loss = 0.39851801\n",
      "Iteration 16, loss = 0.39548885\n",
      "Iteration 17, loss = 0.39328652\n",
      "Iteration 18, loss = 0.38667721\n",
      "Iteration 19, loss = 0.38947791\n",
      "Iteration 20, loss = 0.38694636\n",
      "Iteration 21, loss = 0.38334767\n",
      "Iteration 22, loss = 0.37789277\n",
      "Iteration 23, loss = 0.37768073\n",
      "Iteration 24, loss = 0.37282137\n",
      "Iteration 25, loss = 0.37389943\n",
      "Iteration 26, loss = 0.36968926\n",
      "Iteration 27, loss = 0.36535627\n",
      "Iteration 28, loss = 0.36414949\n",
      "Iteration 29, loss = 0.36493605\n",
      "Iteration 30, loss = 0.36538575\n",
      "Iteration 31, loss = 0.36307093\n",
      "Iteration 32, loss = 0.35731460\n",
      "Iteration 33, loss = 0.35876806\n",
      "Iteration 34, loss = 0.35466542\n",
      "Iteration 35, loss = 0.35350203\n",
      "Iteration 36, loss = 0.35379387\n",
      "Iteration 37, loss = 0.34985945\n",
      "Iteration 38, loss = 0.34956167\n",
      "Iteration 39, loss = 0.34992076\n",
      "Iteration 40, loss = 0.34778808\n",
      "Iteration 41, loss = 0.34663242\n",
      "Iteration 42, loss = 0.34382700\n",
      "Iteration 43, loss = 0.34481948\n",
      "Iteration 44, loss = 0.34326930\n",
      "Iteration 45, loss = 0.34110138\n",
      "Iteration 46, loss = 0.34036042\n",
      "Iteration 47, loss = 0.34064471\n",
      "Iteration 48, loss = 0.33855590\n",
      "Iteration 49, loss = 0.33642748\n",
      "Iteration 50, loss = 0.33659485\n",
      "Iteration 51, loss = 0.33549294\n",
      "Iteration 52, loss = 0.33368112\n",
      "Iteration 53, loss = 0.33365056\n",
      "Iteration 54, loss = 0.33421997\n",
      "Iteration 55, loss = 0.33166169\n",
      "Iteration 56, loss = 0.33008023\n",
      "Iteration 57, loss = 0.33076268\n",
      "Iteration 58, loss = 0.33180820\n",
      "Iteration 59, loss = 0.33076600\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.22673558\n",
      "Iteration 2, loss = 0.63655610\n",
      "Iteration 3, loss = 0.53520035\n",
      "Iteration 4, loss = 0.49647888\n",
      "Iteration 5, loss = 0.46991762\n",
      "Iteration 6, loss = 0.45637964\n",
      "Iteration 7, loss = 0.44612584\n",
      "Iteration 8, loss = 0.43702581\n",
      "Iteration 9, loss = 0.42685375\n",
      "Iteration 10, loss = 0.42062005\n",
      "Iteration 11, loss = 0.41339069\n",
      "Iteration 12, loss = 0.41325227\n",
      "Iteration 13, loss = 0.41103654\n",
      "Iteration 14, loss = 0.40385660\n",
      "Iteration 15, loss = 0.40088667\n",
      "Iteration 16, loss = 0.39780388\n",
      "Iteration 17, loss = 0.39127599\n",
      "Iteration 18, loss = 0.38828080\n",
      "Iteration 19, loss = 0.38608475\n",
      "Iteration 20, loss = 0.38482965\n",
      "Iteration 21, loss = 0.38267858\n",
      "Iteration 22, loss = 0.38074049\n",
      "Iteration 23, loss = 0.37728535\n",
      "Iteration 24, loss = 0.37471819\n",
      "Iteration 25, loss = 0.37233035\n",
      "Iteration 26, loss = 0.37011159\n",
      "Iteration 27, loss = 0.36788441\n",
      "Iteration 28, loss = 0.36833258\n",
      "Iteration 29, loss = 0.36358620\n",
      "Iteration 30, loss = 0.36083920\n",
      "Iteration 31, loss = 0.36153195\n",
      "Iteration 32, loss = 0.35705574\n",
      "Iteration 33, loss = 0.35862552\n",
      "Iteration 34, loss = 0.35724083\n",
      "Iteration 35, loss = 0.35511302\n",
      "Iteration 36, loss = 0.35404660\n",
      "Iteration 37, loss = 0.35061126\n",
      "Iteration 38, loss = 0.34928489\n",
      "Iteration 39, loss = 0.34721894\n",
      "Iteration 40, loss = 0.34767530\n",
      "Iteration 41, loss = 0.34433568\n",
      "Iteration 42, loss = 0.34218430\n",
      "Iteration 43, loss = 0.34437873\n",
      "Iteration 44, loss = 0.34213157\n",
      "Iteration 45, loss = 0.34051833\n",
      "Iteration 46, loss = 0.34366297\n",
      "Iteration 47, loss = 0.33874272\n",
      "Iteration 48, loss = 0.34117706\n",
      "Iteration 49, loss = 0.33579608\n",
      "Iteration 50, loss = 0.33546846\n",
      "Iteration 51, loss = 0.33380972\n",
      "Iteration 52, loss = 0.33364116\n",
      "Iteration 53, loss = 0.33353725\n",
      "Iteration 54, loss = 0.33375339\n",
      "Iteration 55, loss = 0.33122359\n",
      "Iteration 56, loss = 0.33160820\n",
      "Iteration 57, loss = 0.32976992\n",
      "Iteration 58, loss = 0.33051730\n",
      "Iteration 59, loss = 0.32989583\n",
      "Iteration 60, loss = 0.33090992\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.10242008\n",
      "Iteration 2, loss = 0.58139060\n",
      "Iteration 3, loss = 0.51448738\n",
      "Iteration 4, loss = 0.48313400\n",
      "Iteration 5, loss = 0.46834895\n",
      "Iteration 6, loss = 0.45404973\n",
      "Iteration 7, loss = 0.44187185\n",
      "Iteration 8, loss = 0.43650430\n",
      "Iteration 9, loss = 0.42963463\n",
      "Iteration 10, loss = 0.42374986\n",
      "Iteration 11, loss = 0.41896885\n",
      "Iteration 12, loss = 0.41183584\n",
      "Iteration 13, loss = 0.40768768\n",
      "Iteration 14, loss = 0.40387840\n",
      "Iteration 15, loss = 0.40425434\n",
      "Iteration 16, loss = 0.40070378\n",
      "Iteration 17, loss = 0.39378444\n",
      "Iteration 18, loss = 0.39357089\n",
      "Iteration 19, loss = 0.39019311\n",
      "Iteration 20, loss = 0.38830553\n",
      "Iteration 21, loss = 0.38592177\n",
      "Iteration 22, loss = 0.38257190\n",
      "Iteration 23, loss = 0.38094619\n",
      "Iteration 24, loss = 0.37898844\n",
      "Iteration 25, loss = 0.37793773\n",
      "Iteration 26, loss = 0.37632429\n",
      "Iteration 27, loss = 0.37414390\n",
      "Iteration 28, loss = 0.37291282\n",
      "Iteration 29, loss = 0.36949764\n",
      "Iteration 30, loss = 0.37069779\n",
      "Iteration 31, loss = 0.37104968\n",
      "Iteration 32, loss = 0.36920930\n",
      "Iteration 33, loss = 0.35975919\n",
      "Iteration 34, loss = 0.36262509\n",
      "Iteration 35, loss = 0.36362934\n",
      "Iteration 36, loss = 0.35555916\n",
      "Iteration 37, loss = 0.35872735\n",
      "Iteration 38, loss = 0.35481909\n",
      "Iteration 39, loss = 0.35483559\n",
      "Iteration 40, loss = 0.35149405\n",
      "Iteration 41, loss = 0.35310895\n",
      "Iteration 42, loss = 0.35357734\n",
      "Iteration 43, loss = 0.34922145\n",
      "Iteration 44, loss = 0.34949995\n",
      "Iteration 45, loss = 0.34751433\n",
      "Iteration 46, loss = 0.34776494\n",
      "Iteration 47, loss = 0.34525012\n",
      "Iteration 48, loss = 0.35018077\n",
      "Iteration 49, loss = 0.34301665\n",
      "Iteration 50, loss = 0.34142371\n",
      "Iteration 51, loss = 0.34283723\n",
      "Iteration 52, loss = 0.34369874\n",
      "Iteration 53, loss = 0.34002721\n",
      "Iteration 54, loss = 0.34037727\n",
      "Iteration 55, loss = 0.33617717\n",
      "Iteration 56, loss = 0.33889267\n",
      "Iteration 57, loss = 0.33516791\n",
      "Iteration 58, loss = 0.34138822\n",
      "Iteration 59, loss = 0.33428569\n",
      "Iteration 60, loss = 0.33775175\n",
      "Iteration 61, loss = 0.33376012\n",
      "Iteration 62, loss = 0.33514034\n",
      "Iteration 63, loss = 0.33128920\n",
      "Iteration 64, loss = 0.33639769\n",
      "Iteration 65, loss = 0.33023355\n",
      "Iteration 66, loss = 0.32797742\n",
      "Iteration 67, loss = 0.33382441\n",
      "Iteration 68, loss = 0.32557751\n",
      "Iteration 69, loss = 0.32656383\n",
      "Iteration 70, loss = 0.32800526\n",
      "Iteration 71, loss = 0.32620579\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.11306947\n",
      "Iteration 2, loss = 0.59152861\n",
      "Iteration 3, loss = 0.52047863\n",
      "Iteration 4, loss = 0.48883942\n",
      "Iteration 5, loss = 0.47128176\n",
      "Iteration 6, loss = 0.46075679\n",
      "Iteration 7, loss = 0.44640751\n",
      "Iteration 8, loss = 0.44183227\n",
      "Iteration 9, loss = 0.43210691\n",
      "Iteration 10, loss = 0.43032164\n",
      "Iteration 11, loss = 0.42161019\n",
      "Iteration 12, loss = 0.41675029\n",
      "Iteration 13, loss = 0.41519101\n",
      "Iteration 14, loss = 0.41153673\n",
      "Iteration 15, loss = 0.40570668\n",
      "Iteration 16, loss = 0.39937214\n",
      "Iteration 17, loss = 0.39807386\n",
      "Iteration 18, loss = 0.39622158\n",
      "Iteration 19, loss = 0.39199277\n",
      "Iteration 20, loss = 0.38834640\n",
      "Iteration 21, loss = 0.38853667\n",
      "Iteration 22, loss = 0.38434693\n",
      "Iteration 23, loss = 0.38714770\n",
      "Iteration 24, loss = 0.38171689\n",
      "Iteration 25, loss = 0.38021877\n",
      "Iteration 26, loss = 0.37664213\n",
      "Iteration 27, loss = 0.37500680\n",
      "Iteration 28, loss = 0.37112092\n",
      "Iteration 29, loss = 0.37574011\n",
      "Iteration 30, loss = 0.37242557\n",
      "Iteration 31, loss = 0.37013857\n",
      "Iteration 32, loss = 0.36484987\n",
      "Iteration 33, loss = 0.36600234\n",
      "Iteration 34, loss = 0.36275250\n",
      "Iteration 35, loss = 0.36611166\n",
      "Iteration 36, loss = 0.36013042\n",
      "Iteration 37, loss = 0.35913106\n",
      "Iteration 38, loss = 0.35668450\n",
      "Iteration 39, loss = 0.35572480\n",
      "Iteration 40, loss = 0.35406357\n",
      "Iteration 41, loss = 0.35674788\n",
      "Iteration 42, loss = 0.35368964\n",
      "Iteration 43, loss = 0.34910985\n",
      "Iteration 44, loss = 0.35146809\n",
      "Iteration 45, loss = 0.35012709\n",
      "Iteration 46, loss = 0.35156796\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.10924691\n",
      "Iteration 2, loss = 0.59171663\n",
      "Iteration 3, loss = 0.51751256\n",
      "Iteration 4, loss = 0.48547552\n",
      "Iteration 5, loss = 0.46908351\n",
      "Iteration 6, loss = 0.46029207\n",
      "Iteration 7, loss = 0.45221412\n",
      "Iteration 8, loss = 0.43906589\n",
      "Iteration 9, loss = 0.43176662\n",
      "Iteration 10, loss = 0.42819261\n",
      "Iteration 11, loss = 0.42379109\n",
      "Iteration 12, loss = 0.41704740\n",
      "Iteration 13, loss = 0.41275175\n",
      "Iteration 14, loss = 0.41109437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.40787851\n",
      "Iteration 16, loss = 0.40421994\n",
      "Iteration 17, loss = 0.39764644\n",
      "Iteration 18, loss = 0.39589028\n",
      "Iteration 19, loss = 0.39644892\n",
      "Iteration 20, loss = 0.39032661\n",
      "Iteration 21, loss = 0.38740458\n",
      "Iteration 22, loss = 0.38588174\n",
      "Iteration 23, loss = 0.38557439\n",
      "Iteration 24, loss = 0.38553155\n",
      "Iteration 25, loss = 0.37739463\n",
      "Iteration 26, loss = 0.37590464\n",
      "Iteration 27, loss = 0.37545702\n",
      "Iteration 28, loss = 0.37360153\n",
      "Iteration 29, loss = 0.37468621\n",
      "Iteration 30, loss = 0.37004868\n",
      "Iteration 31, loss = 0.36936797\n",
      "Iteration 32, loss = 0.36989655\n",
      "Iteration 33, loss = 0.36761099\n",
      "Iteration 34, loss = 0.36465139\n",
      "Iteration 35, loss = 0.36200510\n",
      "Iteration 36, loss = 0.36167457\n",
      "Iteration 37, loss = 0.36192453\n",
      "Iteration 38, loss = 0.36419960\n",
      "Iteration 39, loss = 0.35778792\n",
      "Iteration 40, loss = 0.35676199\n",
      "Iteration 41, loss = 0.35713185\n",
      "Iteration 42, loss = 0.35494591\n",
      "Iteration 43, loss = 0.35284514\n",
      "Iteration 44, loss = 0.35415514\n",
      "Iteration 45, loss = 0.34890545\n",
      "Iteration 46, loss = 0.35212077\n",
      "Iteration 47, loss = 0.34727815\n",
      "Iteration 48, loss = 0.34767943\n",
      "Iteration 49, loss = 0.34425726\n",
      "Iteration 50, loss = 0.34931745\n",
      "Iteration 51, loss = 0.34398555\n",
      "Iteration 52, loss = 0.34471982\n",
      "Iteration 53, loss = 0.34248351\n",
      "Iteration 54, loss = 0.34426112\n",
      "Iteration 55, loss = 0.34316053\n",
      "Iteration 56, loss = 0.34092330\n",
      "Iteration 57, loss = 0.33961037\n",
      "Iteration 58, loss = 0.33654907\n",
      "Iteration 59, loss = 0.33772445\n",
      "Iteration 60, loss = 0.33563450\n",
      "Iteration 61, loss = 0.33864104\n",
      "Iteration 62, loss = 0.33285148\n",
      "Iteration 63, loss = 0.33166440\n",
      "Iteration 64, loss = 0.33155736\n",
      "Iteration 65, loss = 0.33304290\n",
      "Iteration 66, loss = 0.32877266\n",
      "Iteration 67, loss = 0.33155179\n",
      "Iteration 68, loss = 0.32899004\n",
      "Iteration 69, loss = 0.32908314\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.88007266\n",
      "Iteration 2, loss = 0.59286974\n",
      "Iteration 3, loss = 0.55123251\n",
      "Iteration 4, loss = 0.53038342\n",
      "Iteration 5, loss = 0.51523213\n",
      "Iteration 6, loss = 0.50353465\n",
      "Iteration 7, loss = 0.49790531\n",
      "Iteration 8, loss = 0.49008604\n",
      "Iteration 9, loss = 0.48783814\n",
      "Iteration 10, loss = 0.48376901\n",
      "Iteration 11, loss = 0.48149710\n",
      "Iteration 12, loss = 0.47814181\n",
      "Iteration 13, loss = 0.47702432\n",
      "Iteration 14, loss = 0.47543799\n",
      "Iteration 15, loss = 0.46944221\n",
      "Iteration 16, loss = 0.47118872\n",
      "Iteration 17, loss = 0.46692489\n",
      "Iteration 18, loss = 0.46690634\n",
      "Iteration 19, loss = 0.46630923\n",
      "Iteration 20, loss = 0.46824712\n",
      "Iteration 21, loss = 0.46337001\n",
      "Iteration 22, loss = 0.46441601\n",
      "Iteration 23, loss = 0.46378168\n",
      "Iteration 24, loss = 0.46251005\n",
      "Iteration 25, loss = 0.45916012\n",
      "Iteration 26, loss = 0.46007929\n",
      "Iteration 27, loss = 0.45577643\n",
      "Iteration 28, loss = 0.45857983\n",
      "Iteration 29, loss = 0.45891977\n",
      "Iteration 30, loss = 0.45569657\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.88452165\n",
      "Iteration 2, loss = 0.59671992\n",
      "Iteration 3, loss = 0.55447450\n",
      "Iteration 4, loss = 0.53181118\n",
      "Iteration 5, loss = 0.51692190\n",
      "Iteration 6, loss = 0.51202518\n",
      "Iteration 7, loss = 0.50616862\n",
      "Iteration 8, loss = 0.49701474\n",
      "Iteration 9, loss = 0.49402202\n",
      "Iteration 10, loss = 0.48957876\n",
      "Iteration 11, loss = 0.48546176\n",
      "Iteration 12, loss = 0.48478735\n",
      "Iteration 13, loss = 0.47935764\n",
      "Iteration 14, loss = 0.48084652\n",
      "Iteration 15, loss = 0.47487697\n",
      "Iteration 16, loss = 0.47466487\n",
      "Iteration 17, loss = 0.47348715\n",
      "Iteration 18, loss = 0.47171491\n",
      "Iteration 19, loss = 0.47008020\n",
      "Iteration 20, loss = 0.47148647\n",
      "Iteration 21, loss = 0.46693976\n",
      "Iteration 22, loss = 0.46517093\n",
      "Iteration 23, loss = 0.46577605\n",
      "Iteration 24, loss = 0.46237326\n",
      "Iteration 25, loss = 0.46187379\n",
      "Iteration 26, loss = 0.46351143\n",
      "Iteration 27, loss = 0.46334597\n",
      "Iteration 28, loss = 0.45967097\n",
      "Iteration 29, loss = 0.46315935\n",
      "Iteration 30, loss = 0.45752267\n",
      "Iteration 31, loss = 0.45853122\n",
      "Iteration 32, loss = 0.46123400\n",
      "Iteration 33, loss = 0.45677279\n",
      "Iteration 34, loss = 0.45498072\n",
      "Iteration 35, loss = 0.45965951\n",
      "Iteration 36, loss = 0.45982336\n",
      "Iteration 37, loss = 0.45617780\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.89351518\n",
      "Iteration 2, loss = 0.59339521\n",
      "Iteration 3, loss = 0.54911434\n",
      "Iteration 4, loss = 0.52919924\n",
      "Iteration 5, loss = 0.51698804\n",
      "Iteration 6, loss = 0.50897786\n",
      "Iteration 7, loss = 0.50309381\n",
      "Iteration 8, loss = 0.49637147\n",
      "Iteration 9, loss = 0.49150291\n",
      "Iteration 10, loss = 0.49045186\n",
      "Iteration 11, loss = 0.48293459\n",
      "Iteration 12, loss = 0.48584373\n",
      "Iteration 13, loss = 0.48005965\n",
      "Iteration 14, loss = 0.47999294\n",
      "Iteration 15, loss = 0.47503329\n",
      "Iteration 16, loss = 0.47305606\n",
      "Iteration 17, loss = 0.47548323\n",
      "Iteration 18, loss = 0.46928380\n",
      "Iteration 19, loss = 0.46657521\n",
      "Iteration 20, loss = 0.46666099\n",
      "Iteration 21, loss = 0.46967588\n",
      "Iteration 22, loss = 0.46592901\n",
      "Iteration 23, loss = 0.46978692\n",
      "Iteration 24, loss = 0.46382724\n",
      "Iteration 25, loss = 0.46269792\n",
      "Iteration 26, loss = 0.46327516\n",
      "Iteration 27, loss = 0.45976481\n",
      "Iteration 28, loss = 0.46299735\n",
      "Iteration 29, loss = 0.45851525\n",
      "Iteration 30, loss = 0.46011513\n",
      "Iteration 31, loss = 0.46130093\n",
      "Iteration 32, loss = 0.45770173\n",
      "Iteration 33, loss = 0.45767487\n",
      "Iteration 34, loss = 0.46210721\n",
      "Iteration 35, loss = 0.45645085\n",
      "Iteration 36, loss = 0.45773668\n",
      "Iteration 37, loss = 0.45485486\n",
      "Iteration 38, loss = 0.45688629\n",
      "Iteration 39, loss = 0.45744807\n",
      "Iteration 40, loss = 0.45486465\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.99043840\n",
      "Iteration 2, loss = 0.61975917\n",
      "Iteration 3, loss = 0.55836560\n",
      "Iteration 4, loss = 0.52904957\n",
      "Iteration 5, loss = 0.51316023\n",
      "Iteration 6, loss = 0.50138038\n",
      "Iteration 7, loss = 0.49415827\n",
      "Iteration 8, loss = 0.48806509\n",
      "Iteration 9, loss = 0.48165884\n",
      "Iteration 10, loss = 0.47757862\n",
      "Iteration 11, loss = 0.47554217\n",
      "Iteration 12, loss = 0.47180280\n",
      "Iteration 13, loss = 0.46934064\n",
      "Iteration 14, loss = 0.46783654\n",
      "Iteration 15, loss = 0.46430824\n",
      "Iteration 16, loss = 0.46214123\n",
      "Iteration 17, loss = 0.46014764\n",
      "Iteration 18, loss = 0.46073652\n",
      "Iteration 19, loss = 0.46132451\n",
      "Iteration 20, loss = 0.45706029\n",
      "Iteration 21, loss = 0.45719657\n",
      "Iteration 22, loss = 0.45754965\n",
      "Iteration 23, loss = 0.45305660\n",
      "Iteration 24, loss = 0.45592592\n",
      "Iteration 25, loss = 0.45433781\n",
      "Iteration 26, loss = 0.45409547\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.98967354\n",
      "Iteration 2, loss = 0.61959498\n",
      "Iteration 3, loss = 0.55941457\n",
      "Iteration 4, loss = 0.53225600\n",
      "Iteration 5, loss = 0.51717994\n",
      "Iteration 6, loss = 0.50556937\n",
      "Iteration 7, loss = 0.49592481\n",
      "Iteration 8, loss = 0.48942039\n",
      "Iteration 9, loss = 0.48652778\n",
      "Iteration 10, loss = 0.48228703\n",
      "Iteration 11, loss = 0.47791633\n",
      "Iteration 12, loss = 0.47473690\n",
      "Iteration 13, loss = 0.47290099\n",
      "Iteration 14, loss = 0.47010330\n",
      "Iteration 15, loss = 0.46888554\n",
      "Iteration 16, loss = 0.46446307\n",
      "Iteration 17, loss = 0.46451174\n",
      "Iteration 18, loss = 0.46200826\n",
      "Iteration 19, loss = 0.46236616\n",
      "Iteration 20, loss = 0.46361757\n",
      "Iteration 21, loss = 0.45850624\n",
      "Iteration 22, loss = 0.46091704\n",
      "Iteration 23, loss = 0.45988693\n",
      "Iteration 24, loss = 0.45684796\n",
      "Iteration 25, loss = 0.45493052\n",
      "Iteration 26, loss = 0.45680909\n",
      "Iteration 27, loss = 0.45352109\n",
      "Iteration 28, loss = 0.45459547\n",
      "Iteration 29, loss = 0.45472550\n",
      "Iteration 30, loss = 0.45424897\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.99146233\n",
      "Iteration 2, loss = 0.61817526\n",
      "Iteration 3, loss = 0.55686074\n",
      "Iteration 4, loss = 0.53151536\n",
      "Iteration 5, loss = 0.51467148\n",
      "Iteration 6, loss = 0.50472256\n",
      "Iteration 7, loss = 0.49575859\n",
      "Iteration 8, loss = 0.49149778\n",
      "Iteration 9, loss = 0.48729659\n",
      "Iteration 10, loss = 0.48083902\n",
      "Iteration 11, loss = 0.47917043\n",
      "Iteration 12, loss = 0.47599327\n",
      "Iteration 13, loss = 0.47130676\n",
      "Iteration 14, loss = 0.47095869\n",
      "Iteration 15, loss = 0.46855720\n",
      "Iteration 16, loss = 0.46693857\n",
      "Iteration 17, loss = 0.46506375\n",
      "Iteration 18, loss = 0.46133976\n",
      "Iteration 19, loss = 0.46432984\n",
      "Iteration 20, loss = 0.46182488\n",
      "Iteration 21, loss = 0.45992501\n",
      "Iteration 22, loss = 0.45895721\n",
      "Iteration 23, loss = 0.46072790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, loss = 0.45517468\n",
      "Iteration 25, loss = 0.45477038\n",
      "Iteration 26, loss = 0.45413431\n",
      "Iteration 27, loss = 0.45455309\n",
      "Iteration 28, loss = 0.45434957\n",
      "Iteration 29, loss = 0.45181235\n",
      "Iteration 30, loss = 0.45328707\n",
      "Iteration 31, loss = 0.45222913\n",
      "Iteration 32, loss = 0.44935110\n",
      "Iteration 33, loss = 0.45074476\n",
      "Iteration 34, loss = 0.44896998\n",
      "Iteration 35, loss = 0.45150654\n",
      "Iteration 36, loss = 0.45248944\n",
      "Iteration 37, loss = 0.44926688\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.10511183\n",
      "Iteration 2, loss = 0.66783579\n",
      "Iteration 3, loss = 0.60677008\n",
      "Iteration 4, loss = 0.58322144\n",
      "Iteration 5, loss = 0.56287325\n",
      "Iteration 6, loss = 0.55452122\n",
      "Iteration 7, loss = 0.54693891\n",
      "Iteration 8, loss = 0.53709095\n",
      "Iteration 9, loss = 0.53113550\n",
      "Iteration 10, loss = 0.52987665\n",
      "Iteration 11, loss = 0.52490696\n",
      "Iteration 12, loss = 0.52854862\n",
      "Iteration 13, loss = 0.51968222\n",
      "Iteration 14, loss = 0.51558664\n",
      "Iteration 15, loss = 0.51766535\n",
      "Iteration 16, loss = 0.51544233\n",
      "Iteration 17, loss = 0.51037463\n",
      "Iteration 18, loss = 0.51450462\n",
      "Iteration 19, loss = 0.50906916\n",
      "Iteration 20, loss = 0.50308745\n",
      "Iteration 21, loss = 0.50549018\n",
      "Iteration 22, loss = 0.50293269\n",
      "Iteration 23, loss = 0.50225343\n",
      "Iteration 24, loss = 0.49871673\n",
      "Iteration 25, loss = 0.50240844\n",
      "Iteration 26, loss = 0.49584621\n",
      "Iteration 27, loss = 0.49454393\n",
      "Iteration 28, loss = 0.49751279\n",
      "Iteration 29, loss = 0.49653504\n",
      "Iteration 30, loss = 0.49190773\n",
      "Iteration 31, loss = 0.49400107\n",
      "Iteration 32, loss = 0.49313049\n",
      "Iteration 33, loss = 0.49222777\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.10378576\n",
      "Iteration 2, loss = 0.66836522\n",
      "Iteration 3, loss = 0.60818774\n",
      "Iteration 4, loss = 0.58505475\n",
      "Iteration 5, loss = 0.56837618\n",
      "Iteration 6, loss = 0.55935873\n",
      "Iteration 7, loss = 0.55284710\n",
      "Iteration 8, loss = 0.54793357\n",
      "Iteration 9, loss = 0.53946845\n",
      "Iteration 10, loss = 0.53886891\n",
      "Iteration 11, loss = 0.53227225\n",
      "Iteration 12, loss = 0.53039050\n",
      "Iteration 13, loss = 0.52452475\n",
      "Iteration 14, loss = 0.52848904\n",
      "Iteration 15, loss = 0.52541857\n",
      "Iteration 16, loss = 0.52202586\n",
      "Iteration 17, loss = 0.51312237\n",
      "Iteration 18, loss = 0.51454306\n",
      "Iteration 19, loss = 0.51830505\n",
      "Iteration 20, loss = 0.51005560\n",
      "Iteration 21, loss = 0.51250503\n",
      "Iteration 22, loss = 0.50882560\n",
      "Iteration 23, loss = 0.50963070\n",
      "Iteration 24, loss = 0.50409589\n",
      "Iteration 25, loss = 0.50849620\n",
      "Iteration 26, loss = 0.50569490\n",
      "Iteration 27, loss = 0.50055535\n",
      "Iteration 28, loss = 0.50457066\n",
      "Iteration 29, loss = 0.50560251\n",
      "Iteration 30, loss = 0.49993840\n",
      "Iteration 31, loss = 0.49819997\n",
      "Iteration 32, loss = 0.49929809\n",
      "Iteration 33, loss = 0.49726940\n",
      "Iteration 34, loss = 0.49354900\n",
      "Iteration 35, loss = 0.49507222\n",
      "Iteration 36, loss = 0.49324382\n",
      "Iteration 37, loss = 0.49120206\n",
      "Iteration 38, loss = 0.49546149\n",
      "Iteration 39, loss = 0.49384466\n",
      "Iteration 40, loss = 0.48836606\n",
      "Iteration 41, loss = 0.48937932\n",
      "Iteration 42, loss = 0.48909124\n",
      "Iteration 43, loss = 0.48718201\n",
      "Iteration 44, loss = 0.48812913\n",
      "Iteration 45, loss = 0.48941277\n",
      "Iteration 46, loss = 0.48632457\n",
      "Iteration 47, loss = 0.48739339\n",
      "Iteration 48, loss = 0.48766644\n",
      "Iteration 49, loss = 0.48819269\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.10319474\n",
      "Iteration 2, loss = 0.66437895\n",
      "Iteration 3, loss = 0.60779202\n",
      "Iteration 4, loss = 0.58326184\n",
      "Iteration 5, loss = 0.56853039\n",
      "Iteration 6, loss = 0.55464099\n",
      "Iteration 7, loss = 0.54591670\n",
      "Iteration 8, loss = 0.54748245\n",
      "Iteration 9, loss = 0.54167464\n",
      "Iteration 10, loss = 0.53331994\n",
      "Iteration 11, loss = 0.52956149\n",
      "Iteration 12, loss = 0.52668478\n",
      "Iteration 13, loss = 0.52461037\n",
      "Iteration 14, loss = 0.52596626\n",
      "Iteration 15, loss = 0.51730977\n",
      "Iteration 16, loss = 0.51892382\n",
      "Iteration 17, loss = 0.51620810\n",
      "Iteration 18, loss = 0.50911027\n",
      "Iteration 19, loss = 0.51239750\n",
      "Iteration 20, loss = 0.50748530\n",
      "Iteration 21, loss = 0.51120937\n",
      "Iteration 22, loss = 0.50732583\n",
      "Iteration 23, loss = 0.50978913\n",
      "Iteration 24, loss = 0.50273836\n",
      "Iteration 25, loss = 0.50447763\n",
      "Iteration 26, loss = 0.49971654\n",
      "Iteration 27, loss = 0.49986684\n",
      "Iteration 28, loss = 0.49801113\n",
      "Iteration 29, loss = 0.49874478\n",
      "Iteration 30, loss = 0.49767053\n",
      "Iteration 31, loss = 0.49616086\n",
      "Iteration 32, loss = 0.49746177\n",
      "Iteration 33, loss = 0.49268872\n",
      "Iteration 34, loss = 0.49488093\n",
      "Iteration 35, loss = 0.49647310\n",
      "Iteration 36, loss = 0.49123899\n",
      "Iteration 37, loss = 0.48879076\n",
      "Iteration 38, loss = 0.49022875\n",
      "Iteration 39, loss = 0.49224421\n",
      "Iteration 40, loss = 0.48915353\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.29399675\n",
      "Iteration 2, loss = 0.71165031\n",
      "Iteration 3, loss = 0.62278406\n",
      "Iteration 4, loss = 0.58699858\n",
      "Iteration 5, loss = 0.56254573\n",
      "Iteration 6, loss = 0.55141216\n",
      "Iteration 7, loss = 0.54285674\n",
      "Iteration 8, loss = 0.53653382\n",
      "Iteration 9, loss = 0.52751644\n",
      "Iteration 10, loss = 0.52177265\n",
      "Iteration 11, loss = 0.51767754\n",
      "Iteration 12, loss = 0.51395428\n",
      "Iteration 13, loss = 0.51118845\n",
      "Iteration 14, loss = 0.51084900\n",
      "Iteration 15, loss = 0.50202209\n",
      "Iteration 16, loss = 0.50474796\n",
      "Iteration 17, loss = 0.49993849\n",
      "Iteration 18, loss = 0.50001859\n",
      "Iteration 19, loss = 0.50172829\n",
      "Iteration 20, loss = 0.49842039\n",
      "Iteration 21, loss = 0.49638779\n",
      "Iteration 22, loss = 0.49440175\n",
      "Iteration 23, loss = 0.49544719\n",
      "Iteration 24, loss = 0.49406161\n",
      "Iteration 25, loss = 0.49215727\n",
      "Iteration 26, loss = 0.49103191\n",
      "Iteration 27, loss = 0.48658372\n",
      "Iteration 28, loss = 0.49007533\n",
      "Iteration 29, loss = 0.48643905\n",
      "Iteration 30, loss = 0.49093112\n",
      "Iteration 31, loss = 0.48454344\n",
      "Iteration 32, loss = 0.48662522\n",
      "Iteration 33, loss = 0.48750441\n",
      "Iteration 34, loss = 0.48638867\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.28057579\n",
      "Iteration 2, loss = 0.71275752\n",
      "Iteration 3, loss = 0.62553631\n",
      "Iteration 4, loss = 0.59016124\n",
      "Iteration 5, loss = 0.56889799\n",
      "Iteration 6, loss = 0.55434777\n",
      "Iteration 7, loss = 0.54636326\n",
      "Iteration 8, loss = 0.54025665\n",
      "Iteration 9, loss = 0.52922599\n",
      "Iteration 10, loss = 0.52442270\n",
      "Iteration 11, loss = 0.52278818\n",
      "Iteration 12, loss = 0.51748702\n",
      "Iteration 13, loss = 0.51915067\n",
      "Iteration 14, loss = 0.51149482\n",
      "Iteration 15, loss = 0.51014201\n",
      "Iteration 16, loss = 0.50975461\n",
      "Iteration 17, loss = 0.50694825\n",
      "Iteration 18, loss = 0.50230656\n",
      "Iteration 19, loss = 0.50551057\n",
      "Iteration 20, loss = 0.50494495\n",
      "Iteration 21, loss = 0.50316290\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.28523554\n",
      "Iteration 2, loss = 0.71916756\n",
      "Iteration 3, loss = 0.62746944\n",
      "Iteration 4, loss = 0.59315831\n",
      "Iteration 5, loss = 0.56779241\n",
      "Iteration 6, loss = 0.55597690\n",
      "Iteration 7, loss = 0.54559361\n",
      "Iteration 8, loss = 0.53823263\n",
      "Iteration 9, loss = 0.52981006\n",
      "Iteration 10, loss = 0.52494483\n",
      "Iteration 11, loss = 0.51759733\n",
      "Iteration 12, loss = 0.51910124\n",
      "Iteration 13, loss = 0.51744198\n",
      "Iteration 14, loss = 0.51191546\n",
      "Iteration 15, loss = 0.51086466\n",
      "Iteration 16, loss = 0.50920110\n",
      "Iteration 17, loss = 0.50319690\n",
      "Iteration 18, loss = 0.50243391\n",
      "Iteration 19, loss = 0.50038252\n",
      "Iteration 20, loss = 0.50111299\n",
      "Iteration 21, loss = 0.50062130\n",
      "Iteration 22, loss = 0.49990199\n",
      "Iteration 23, loss = 0.49732625\n",
      "Iteration 24, loss = 0.49520183\n",
      "Iteration 25, loss = 0.49468569\n",
      "Iteration 26, loss = 0.49342372\n",
      "Iteration 27, loss = 0.49327429\n",
      "Iteration 28, loss = 0.49324821\n",
      "Iteration 29, loss = 0.49131857\n",
      "Iteration 30, loss = 0.49032278\n",
      "Iteration 31, loss = 0.48992596\n",
      "Iteration 32, loss = 0.48823355\n",
      "Iteration 33, loss = 0.49137017\n",
      "Iteration 34, loss = 0.48852440\n",
      "Iteration 35, loss = 0.48767851\n",
      "Iteration 36, loss = 0.48784426\n",
      "Iteration 37, loss = 0.48461752\n",
      "Iteration 38, loss = 0.48655298\n",
      "Iteration 39, loss = 0.48518927\n",
      "Iteration 40, loss = 0.48498817\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.17574156\n",
      "Iteration 2, loss = 0.67323616\n",
      "Iteration 3, loss = 0.61081001\n",
      "Iteration 4, loss = 0.58149857\n",
      "Iteration 5, loss = 0.57013730\n",
      "Iteration 6, loss = 0.55790342\n",
      "Iteration 7, loss = 0.54555427\n",
      "Iteration 8, loss = 0.54167424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.53368461\n",
      "Iteration 10, loss = 0.53068978\n",
      "Iteration 11, loss = 0.52616643\n",
      "Iteration 12, loss = 0.52349373\n",
      "Iteration 13, loss = 0.51862715\n",
      "Iteration 14, loss = 0.51624371\n",
      "Iteration 15, loss = 0.51596114\n",
      "Iteration 16, loss = 0.51214570\n",
      "Iteration 17, loss = 0.51133821\n",
      "Iteration 18, loss = 0.50987029\n",
      "Iteration 19, loss = 0.50808919\n",
      "Iteration 20, loss = 0.50561254\n",
      "Iteration 21, loss = 0.50570428\n",
      "Iteration 22, loss = 0.50245073\n",
      "Iteration 23, loss = 0.50298157\n",
      "Iteration 24, loss = 0.50165596\n",
      "Iteration 25, loss = 0.50058139\n",
      "Iteration 26, loss = 0.50260801\n",
      "Iteration 27, loss = 0.49841613\n",
      "Iteration 28, loss = 0.49938195\n",
      "Iteration 29, loss = 0.49370760\n",
      "Iteration 30, loss = 0.49654245\n",
      "Iteration 31, loss = 0.49841676\n",
      "Iteration 32, loss = 0.49417843\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.18599780\n",
      "Iteration 2, loss = 0.68332636\n",
      "Iteration 3, loss = 0.61960541\n",
      "Iteration 4, loss = 0.58756743\n",
      "Iteration 5, loss = 0.57202901\n",
      "Iteration 6, loss = 0.56522975\n",
      "Iteration 7, loss = 0.55044566\n",
      "Iteration 8, loss = 0.54981805\n",
      "Iteration 9, loss = 0.54070920\n",
      "Iteration 10, loss = 0.53873279\n",
      "Iteration 11, loss = 0.53125738\n",
      "Iteration 12, loss = 0.52906761\n",
      "Iteration 13, loss = 0.52623750\n",
      "Iteration 14, loss = 0.52351314\n",
      "Iteration 15, loss = 0.51980313\n",
      "Iteration 16, loss = 0.51606620\n",
      "Iteration 17, loss = 0.51500603\n",
      "Iteration 18, loss = 0.51420346\n",
      "Iteration 19, loss = 0.51112908\n",
      "Iteration 20, loss = 0.50823984\n",
      "Iteration 21, loss = 0.50842406\n",
      "Iteration 22, loss = 0.50612997\n",
      "Iteration 23, loss = 0.50753613\n",
      "Iteration 24, loss = 0.50490716\n",
      "Iteration 25, loss = 0.50571619\n",
      "Iteration 26, loss = 0.50300771\n",
      "Iteration 27, loss = 0.50284267\n",
      "Iteration 28, loss = 0.49775304\n",
      "Iteration 29, loss = 0.50226153\n",
      "Iteration 30, loss = 0.49997055\n",
      "Iteration 31, loss = 0.49980021\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.18266742\n",
      "Iteration 2, loss = 0.68620708\n",
      "Iteration 3, loss = 0.61490603\n",
      "Iteration 4, loss = 0.58351341\n",
      "Iteration 5, loss = 0.56980978\n",
      "Iteration 6, loss = 0.56085637\n",
      "Iteration 7, loss = 0.55254419\n",
      "Iteration 8, loss = 0.54222190\n",
      "Iteration 9, loss = 0.53626501\n",
      "Iteration 10, loss = 0.53503935\n",
      "Iteration 11, loss = 0.53095648\n",
      "Iteration 12, loss = 0.52405058\n",
      "Iteration 13, loss = 0.52140075\n",
      "Iteration 14, loss = 0.52323031\n",
      "Iteration 15, loss = 0.51986776\n",
      "Iteration 16, loss = 0.51774020\n",
      "Iteration 17, loss = 0.51256218\n",
      "Iteration 18, loss = 0.51208408\n",
      "Iteration 19, loss = 0.51482653\n",
      "Iteration 20, loss = 0.50741040\n",
      "Iteration 21, loss = 0.50545924\n",
      "Iteration 22, loss = 0.50542247\n",
      "Iteration 23, loss = 0.50722032\n",
      "Iteration 24, loss = 0.50596248\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.97400831\n",
      "Iteration 2, loss = 0.68541091\n",
      "Iteration 3, loss = 0.64353076\n",
      "Iteration 4, loss = 0.62768769\n",
      "Iteration 5, loss = 0.61430168\n",
      "Iteration 6, loss = 0.60689267\n",
      "Iteration 7, loss = 0.60323489\n",
      "Iteration 8, loss = 0.59819409\n",
      "Iteration 9, loss = 0.59753092\n",
      "Iteration 10, loss = 0.59655907\n",
      "Iteration 11, loss = 0.59461528\n",
      "Iteration 12, loss = 0.59275039\n",
      "Iteration 13, loss = 0.59304839\n",
      "Iteration 14, loss = 0.59261258\n",
      "Iteration 15, loss = 0.58643542\n",
      "Iteration 16, loss = 0.58761972\n",
      "Iteration 17, loss = 0.58464083\n",
      "Iteration 18, loss = 0.58723147\n",
      "Iteration 19, loss = 0.58622164\n",
      "Iteration 20, loss = 0.58935288\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.97797925\n",
      "Iteration 2, loss = 0.68880960\n",
      "Iteration 3, loss = 0.64676573\n",
      "Iteration 4, loss = 0.62711452\n",
      "Iteration 5, loss = 0.61504988\n",
      "Iteration 6, loss = 0.61571756\n",
      "Iteration 7, loss = 0.61098975\n",
      "Iteration 8, loss = 0.60566163\n",
      "Iteration 9, loss = 0.60306596\n",
      "Iteration 10, loss = 0.60106662\n",
      "Iteration 11, loss = 0.59825305\n",
      "Iteration 12, loss = 0.59927947\n",
      "Iteration 13, loss = 0.59490883\n",
      "Iteration 14, loss = 0.59596530\n",
      "Iteration 15, loss = 0.59285426\n",
      "Iteration 16, loss = 0.59145517\n",
      "Iteration 17, loss = 0.59187203\n",
      "Iteration 18, loss = 0.59174450\n",
      "Iteration 19, loss = 0.58952771\n",
      "Iteration 20, loss = 0.59297808\n",
      "Iteration 21, loss = 0.59108277\n",
      "Iteration 22, loss = 0.58970500\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.98711264\n",
      "Iteration 2, loss = 0.68539864\n",
      "Iteration 3, loss = 0.64163610\n",
      "Iteration 4, loss = 0.62430974\n",
      "Iteration 5, loss = 0.61527184\n",
      "Iteration 6, loss = 0.60989555\n",
      "Iteration 7, loss = 0.60593262\n",
      "Iteration 8, loss = 0.60300085\n",
      "Iteration 9, loss = 0.59856173\n",
      "Iteration 10, loss = 0.59843751\n",
      "Iteration 11, loss = 0.59366974\n",
      "Iteration 12, loss = 0.59913512\n",
      "Iteration 13, loss = 0.59299458\n",
      "Iteration 14, loss = 0.59348427\n",
      "Iteration 15, loss = 0.58993142\n",
      "Iteration 16, loss = 0.58985581\n",
      "Iteration 17, loss = 0.59073696\n",
      "Iteration 18, loss = 0.58681402\n",
      "Iteration 19, loss = 0.58708917\n",
      "Iteration 20, loss = 0.58529938\n",
      "Iteration 21, loss = 0.59055224\n",
      "Iteration 22, loss = 0.58811333\n",
      "Iteration 23, loss = 0.59038812\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.06524848\n",
      "Iteration 2, loss = 0.71021469\n",
      "Iteration 3, loss = 0.65283724\n",
      "Iteration 4, loss = 0.62623109\n",
      "Iteration 5, loss = 0.61346880\n",
      "Iteration 6, loss = 0.60458953\n",
      "Iteration 7, loss = 0.59975869\n",
      "Iteration 8, loss = 0.59618858\n",
      "Iteration 9, loss = 0.59130498\n",
      "Iteration 10, loss = 0.58859173\n",
      "Iteration 11, loss = 0.59056460\n",
      "Iteration 12, loss = 0.58681578\n",
      "Iteration 13, loss = 0.58668562\n",
      "Iteration 14, loss = 0.58531114\n",
      "Iteration 15, loss = 0.58390767\n",
      "Iteration 16, loss = 0.58283765\n",
      "Iteration 17, loss = 0.58167166\n",
      "Iteration 18, loss = 0.58249833\n",
      "Iteration 19, loss = 0.58312003\n",
      "Iteration 20, loss = 0.58017690\n",
      "Iteration 21, loss = 0.58094266\n",
      "Iteration 22, loss = 0.58246986\n",
      "Iteration 23, loss = 0.57808310\n",
      "Iteration 24, loss = 0.58107006\n",
      "Iteration 25, loss = 0.57959010\n",
      "Iteration 26, loss = 0.58030953\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.06414796\n",
      "Iteration 2, loss = 0.70893202\n",
      "Iteration 3, loss = 0.65200189\n",
      "Iteration 4, loss = 0.62887029\n",
      "Iteration 5, loss = 0.61693742\n",
      "Iteration 6, loss = 0.60838317\n",
      "Iteration 7, loss = 0.60125719\n",
      "Iteration 8, loss = 0.59699781\n",
      "Iteration 9, loss = 0.59724217\n",
      "Iteration 10, loss = 0.59586758\n",
      "Iteration 11, loss = 0.59192880\n",
      "Iteration 12, loss = 0.59057803\n",
      "Iteration 13, loss = 0.58991895\n",
      "Iteration 14, loss = 0.58839476\n",
      "Iteration 15, loss = 0.58792375\n",
      "Iteration 16, loss = 0.58482630\n",
      "Iteration 17, loss = 0.58556100\n",
      "Iteration 18, loss = 0.58380389\n",
      "Iteration 19, loss = 0.58509182\n",
      "Iteration 20, loss = 0.58679184\n",
      "Iteration 21, loss = 0.58228945\n",
      "Iteration 22, loss = 0.58486909\n",
      "Iteration 23, loss = 0.58536495\n",
      "Iteration 24, loss = 0.58218199\n",
      "Iteration 25, loss = 0.58048756\n",
      "Iteration 26, loss = 0.58396817\n",
      "Iteration 27, loss = 0.58065717\n",
      "Iteration 28, loss = 0.58195851\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.06596576\n",
      "Iteration 2, loss = 0.70746653\n",
      "Iteration 3, loss = 0.64924452\n",
      "Iteration 4, loss = 0.62667674\n",
      "Iteration 5, loss = 0.61234673\n",
      "Iteration 6, loss = 0.60640794\n",
      "Iteration 7, loss = 0.59885787\n",
      "Iteration 8, loss = 0.59653593\n",
      "Iteration 9, loss = 0.59478429\n",
      "Iteration 10, loss = 0.59129079\n",
      "Iteration 11, loss = 0.59201355\n",
      "Iteration 12, loss = 0.58908314\n",
      "Iteration 13, loss = 0.58735669\n",
      "Iteration 14, loss = 0.58769846\n",
      "Iteration 15, loss = 0.58507028\n",
      "Iteration 16, loss = 0.58563540\n",
      "Iteration 17, loss = 0.58330912\n",
      "Iteration 18, loss = 0.58072413\n",
      "Iteration 19, loss = 0.58496070\n",
      "Iteration 20, loss = 0.58309300\n",
      "Iteration 21, loss = 0.58125731\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.25735417\n",
      "Iteration 2, loss = 0.82612834\n",
      "Iteration 3, loss = 0.76885981\n",
      "Iteration 4, loss = 0.74308050\n",
      "Iteration 5, loss = 0.72435797\n",
      "Iteration 6, loss = 0.71797033\n",
      "Iteration 7, loss = 0.71114912\n",
      "Iteration 8, loss = 0.70353528\n",
      "Iteration 9, loss = 0.69889139\n",
      "Iteration 10, loss = 0.69731407\n",
      "Iteration 11, loss = 0.69493976\n",
      "Iteration 12, loss = 0.69727840\n",
      "Iteration 13, loss = 0.69315586\n",
      "Iteration 14, loss = 0.68973460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.68747114\n",
      "Iteration 16, loss = 0.68728884\n",
      "Iteration 17, loss = 0.68601748\n",
      "Iteration 18, loss = 0.69033828\n",
      "Iteration 19, loss = 0.68523707\n",
      "Iteration 20, loss = 0.68118030\n",
      "Iteration 21, loss = 0.68326490\n",
      "Iteration 22, loss = 0.68007684\n",
      "Iteration 23, loss = 0.68011813\n",
      "Iteration 24, loss = 0.67990356\n",
      "Iteration 25, loss = 0.68104055\n",
      "Iteration 26, loss = 0.67717866\n",
      "Iteration 27, loss = 0.67645986\n",
      "Iteration 28, loss = 0.68100597\n",
      "Iteration 29, loss = 0.67888347\n",
      "Iteration 30, loss = 0.67470230\n",
      "Iteration 31, loss = 0.67644405\n",
      "Iteration 32, loss = 0.67621080\n",
      "Iteration 33, loss = 0.68064169\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.25581498\n",
      "Iteration 2, loss = 0.83010951\n",
      "Iteration 3, loss = 0.77159247\n",
      "Iteration 4, loss = 0.74852024\n",
      "Iteration 5, loss = 0.73024827\n",
      "Iteration 6, loss = 0.72445088\n",
      "Iteration 7, loss = 0.71484100\n",
      "Iteration 8, loss = 0.71304797\n",
      "Iteration 9, loss = 0.70353651\n",
      "Iteration 10, loss = 0.70338845\n",
      "Iteration 11, loss = 0.70266617\n",
      "Iteration 12, loss = 0.70171709\n",
      "Iteration 13, loss = 0.69550187\n",
      "Iteration 14, loss = 0.70116801\n",
      "Iteration 15, loss = 0.69825681\n",
      "Iteration 16, loss = 0.69626552\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.25550522\n",
      "Iteration 2, loss = 0.82778316\n",
      "Iteration 3, loss = 0.76945941\n",
      "Iteration 4, loss = 0.74574420\n",
      "Iteration 5, loss = 0.73031956\n",
      "Iteration 6, loss = 0.71497688\n",
      "Iteration 7, loss = 0.70804414\n",
      "Iteration 8, loss = 0.70918970\n",
      "Iteration 9, loss = 0.70322061\n",
      "Iteration 10, loss = 0.70095834\n",
      "Iteration 11, loss = 0.69374394\n",
      "Iteration 12, loss = 0.69746706\n",
      "Iteration 13, loss = 0.69357875\n",
      "Iteration 14, loss = 0.69405566\n",
      "Iteration 15, loss = 0.68747105\n",
      "Iteration 16, loss = 0.68912124\n",
      "Iteration 17, loss = 0.68645006\n",
      "Iteration 18, loss = 0.68291768\n",
      "Iteration 19, loss = 0.68662583\n",
      "Iteration 20, loss = 0.68341459\n",
      "Iteration 21, loss = 0.68443931\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.41673421\n",
      "Iteration 2, loss = 0.87299526\n",
      "Iteration 3, loss = 0.78994665\n",
      "Iteration 4, loss = 0.75409867\n",
      "Iteration 5, loss = 0.73260455\n",
      "Iteration 6, loss = 0.72200222\n",
      "Iteration 7, loss = 0.71511802\n",
      "Iteration 8, loss = 0.71062482\n",
      "Iteration 9, loss = 0.70409136\n",
      "Iteration 10, loss = 0.69968186\n",
      "Iteration 11, loss = 0.69625980\n",
      "Iteration 12, loss = 0.69451211\n",
      "Iteration 13, loss = 0.69301192\n",
      "Iteration 14, loss = 0.69274793\n",
      "Iteration 15, loss = 0.68492284\n",
      "Iteration 16, loss = 0.68790557\n",
      "Iteration 17, loss = 0.68402804\n",
      "Iteration 18, loss = 0.68398469\n",
      "Iteration 19, loss = 0.68614620\n",
      "Iteration 20, loss = 0.68421759\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.40345987\n",
      "Iteration 2, loss = 0.87093302\n",
      "Iteration 3, loss = 0.79039721\n",
      "Iteration 4, loss = 0.75692375\n",
      "Iteration 5, loss = 0.73746102\n",
      "Iteration 6, loss = 0.72393155\n",
      "Iteration 7, loss = 0.71759743\n",
      "Iteration 8, loss = 0.71447771\n",
      "Iteration 9, loss = 0.70399131\n",
      "Iteration 10, loss = 0.70062849\n",
      "Iteration 11, loss = 0.70112834\n",
      "Iteration 12, loss = 0.69773851\n",
      "Iteration 13, loss = 0.69981848\n",
      "Iteration 14, loss = 0.69327307\n",
      "Iteration 15, loss = 0.69274608\n",
      "Iteration 16, loss = 0.69323036\n",
      "Iteration 17, loss = 0.69004730\n",
      "Iteration 18, loss = 0.68809010\n",
      "Iteration 19, loss = 0.68987486\n",
      "Iteration 20, loss = 0.69176369\n",
      "Iteration 21, loss = 0.68932945\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.40882328\n",
      "Iteration 2, loss = 0.87647589\n",
      "Iteration 3, loss = 0.79193947\n",
      "Iteration 4, loss = 0.75878994\n",
      "Iteration 5, loss = 0.73618343\n",
      "Iteration 6, loss = 0.72488585\n",
      "Iteration 7, loss = 0.71568562\n",
      "Iteration 8, loss = 0.71028634\n",
      "Iteration 9, loss = 0.70448770\n",
      "Iteration 10, loss = 0.70221565\n",
      "Iteration 11, loss = 0.69514303\n",
      "Iteration 12, loss = 0.69606622\n",
      "Iteration 13, loss = 0.69580902\n",
      "Iteration 14, loss = 0.69171420\n",
      "Iteration 15, loss = 0.68938248\n",
      "Iteration 16, loss = 0.68936632\n",
      "Iteration 17, loss = 0.68504714\n",
      "Iteration 18, loss = 0.68663621\n",
      "Iteration 19, loss = 0.68388516\n",
      "Iteration 20, loss = 0.68567374\n",
      "Iteration 21, loss = 0.68463285\n",
      "Iteration 22, loss = 0.68608491\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.32092571\n",
      "Iteration 2, loss = 0.83985034\n",
      "Iteration 3, loss = 0.77443736\n",
      "Iteration 4, loss = 0.74542620\n",
      "Iteration 5, loss = 0.73180731\n",
      "Iteration 6, loss = 0.72003358\n",
      "Iteration 7, loss = 0.71269061\n",
      "Iteration 8, loss = 0.70841145\n",
      "Iteration 9, loss = 0.70087618\n",
      "Iteration 10, loss = 0.69900876\n",
      "Iteration 11, loss = 0.69718204\n",
      "Iteration 12, loss = 0.69539288\n",
      "Iteration 13, loss = 0.69287120\n",
      "Iteration 14, loss = 0.69086331\n",
      "Iteration 15, loss = 0.69135627\n",
      "Iteration 16, loss = 0.68753535\n",
      "Iteration 17, loss = 0.68905117\n",
      "Iteration 18, loss = 0.68753294\n",
      "Iteration 19, loss = 0.68997869\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.32969401\n",
      "Iteration 2, loss = 0.84566256\n",
      "Iteration 3, loss = 0.78137121\n",
      "Iteration 4, loss = 0.74852250\n",
      "Iteration 5, loss = 0.73444861\n",
      "Iteration 6, loss = 0.72776138\n",
      "Iteration 7, loss = 0.71615185\n",
      "Iteration 8, loss = 0.71462992\n",
      "Iteration 9, loss = 0.71039608\n",
      "Iteration 10, loss = 0.70624316\n",
      "Iteration 11, loss = 0.70294019\n",
      "Iteration 12, loss = 0.70337593\n",
      "Iteration 13, loss = 0.69915255\n",
      "Iteration 14, loss = 0.69586327\n",
      "Iteration 15, loss = 0.69524154\n",
      "Iteration 16, loss = 0.69292714\n",
      "Iteration 17, loss = 0.69384165\n",
      "Iteration 18, loss = 0.69323870\n",
      "Iteration 19, loss = 0.69021634\n",
      "Iteration 20, loss = 0.69037860\n",
      "Iteration 21, loss = 0.68788022\n",
      "Iteration 22, loss = 0.68836510\n",
      "Iteration 23, loss = 0.69025030\n",
      "Iteration 24, loss = 0.68769899\n",
      "Iteration 25, loss = 0.68942982\n",
      "Iteration 26, loss = 0.68697375\n",
      "Iteration 27, loss = 0.68757352\n",
      "Iteration 28, loss = 0.68265771\n",
      "Iteration 29, loss = 0.68835350\n",
      "Iteration 30, loss = 0.68559561\n",
      "Iteration 31, loss = 0.68380969\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.32810526\n",
      "Iteration 2, loss = 0.85000153\n",
      "Iteration 3, loss = 0.77833566\n",
      "Iteration 4, loss = 0.74767239\n",
      "Iteration 5, loss = 0.73109735\n",
      "Iteration 6, loss = 0.72388027\n",
      "Iteration 7, loss = 0.71588521\n",
      "Iteration 8, loss = 0.70839135\n",
      "Iteration 9, loss = 0.70386879\n",
      "Iteration 10, loss = 0.70230763\n",
      "Iteration 11, loss = 0.69942557\n",
      "Iteration 12, loss = 0.69375081\n",
      "Iteration 13, loss = 0.69258812\n",
      "Iteration 14, loss = 0.69206142\n",
      "Iteration 15, loss = 0.68866517\n",
      "Iteration 16, loss = 0.68994502\n",
      "Iteration 17, loss = 0.68777690\n",
      "Iteration 18, loss = 0.68755696\n",
      "Iteration 19, loss = 0.69126834\n",
      "Iteration 20, loss = 0.68567376\n",
      "Iteration 21, loss = 0.68319249\n",
      "Iteration 22, loss = 0.68427368\n",
      "Iteration 23, loss = 0.68573023\n",
      "Iteration 24, loss = 0.68442999\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.06792996\n",
      "Iteration 2, loss = 0.77357548\n",
      "Iteration 3, loss = 0.73864021\n",
      "Iteration 4, loss = 0.72516405\n",
      "Iteration 5, loss = 0.71604434\n",
      "Iteration 6, loss = 0.71007221\n",
      "Iteration 7, loss = 0.70848125\n",
      "Iteration 8, loss = 0.70544030\n",
      "Iteration 9, loss = 0.70471838\n",
      "Iteration 10, loss = 0.70527299\n",
      "Iteration 11, loss = 0.70319505\n",
      "Iteration 12, loss = 0.70190005\n",
      "Iteration 13, loss = 0.70269492\n",
      "Iteration 14, loss = 0.70242916\n",
      "Iteration 15, loss = 0.69750006\n",
      "Iteration 16, loss = 0.69764863\n",
      "Iteration 17, loss = 0.69537470\n",
      "Iteration 18, loss = 0.69912946\n",
      "Iteration 19, loss = 0.69755153\n",
      "Iteration 20, loss = 0.70100853\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.07122488\n",
      "Iteration 2, loss = 0.77764938\n",
      "Iteration 3, loss = 0.74087071\n",
      "Iteration 4, loss = 0.72484985\n",
      "Iteration 5, loss = 0.71566095\n",
      "Iteration 6, loss = 0.71913504\n",
      "Iteration 7, loss = 0.71513219\n",
      "Iteration 8, loss = 0.71194647\n",
      "Iteration 9, loss = 0.70989133\n",
      "Iteration 10, loss = 0.70827410\n",
      "Iteration 11, loss = 0.70600894\n",
      "Iteration 12, loss = 0.70756746\n",
      "Iteration 13, loss = 0.70319663\n",
      "Iteration 14, loss = 0.70462800\n",
      "Iteration 15, loss = 0.70296466\n",
      "Iteration 16, loss = 0.70103542\n",
      "Iteration 17, loss = 0.70259408\n",
      "Iteration 18, loss = 0.70277761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, loss = 0.70057219\n",
      "Iteration 20, loss = 0.70434275\n",
      "Iteration 21, loss = 0.70320601\n",
      "Iteration 22, loss = 0.70202893\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08099931\n",
      "Iteration 2, loss = 0.77356380\n",
      "Iteration 3, loss = 0.73547749\n",
      "Iteration 4, loss = 0.72190955\n",
      "Iteration 5, loss = 0.71554818\n",
      "Iteration 6, loss = 0.71265010\n",
      "Iteration 7, loss = 0.70993576\n",
      "Iteration 8, loss = 0.70842334\n",
      "Iteration 9, loss = 0.70415949\n",
      "Iteration 10, loss = 0.70490850\n",
      "Iteration 11, loss = 0.70079648\n",
      "Iteration 12, loss = 0.70636542\n",
      "Iteration 13, loss = 0.70101178\n",
      "Iteration 14, loss = 0.70166099\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.14694086\n",
      "Iteration 2, loss = 0.79815702\n",
      "Iteration 3, loss = 0.74831401\n",
      "Iteration 4, loss = 0.72657427\n",
      "Iteration 5, loss = 0.71667061\n",
      "Iteration 6, loss = 0.71095892\n",
      "Iteration 7, loss = 0.70769760\n",
      "Iteration 8, loss = 0.70557442\n",
      "Iteration 9, loss = 0.70215060\n",
      "Iteration 10, loss = 0.69971549\n",
      "Iteration 11, loss = 0.70208952\n",
      "Iteration 12, loss = 0.69926983\n",
      "Iteration 13, loss = 0.70039375\n",
      "Iteration 14, loss = 0.69857650\n",
      "Iteration 15, loss = 0.69772282\n",
      "Iteration 16, loss = 0.69740140\n",
      "Iteration 17, loss = 0.69630679\n",
      "Iteration 18, loss = 0.69707369\n",
      "Iteration 19, loss = 0.69727995\n",
      "Iteration 20, loss = 0.69496998\n",
      "Iteration 21, loss = 0.69644158\n",
      "Iteration 22, loss = 0.69884357\n",
      "Iteration 23, loss = 0.69362499\n",
      "Iteration 24, loss = 0.69607480\n",
      "Iteration 25, loss = 0.69455373\n",
      "Iteration 26, loss = 0.69681082\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.14541924\n",
      "Iteration 2, loss = 0.79655007\n",
      "Iteration 3, loss = 0.74704211\n",
      "Iteration 4, loss = 0.72923894\n",
      "Iteration 5, loss = 0.71979977\n",
      "Iteration 6, loss = 0.71452661\n",
      "Iteration 7, loss = 0.70925537\n",
      "Iteration 8, loss = 0.70627624\n",
      "Iteration 9, loss = 0.70854057\n",
      "Iteration 10, loss = 0.70788488\n",
      "Iteration 11, loss = 0.70368800\n",
      "Iteration 12, loss = 0.70276829\n",
      "Iteration 13, loss = 0.70318012\n",
      "Iteration 14, loss = 0.70132577\n",
      "Iteration 15, loss = 0.70102759\n",
      "Iteration 16, loss = 0.69901299\n",
      "Iteration 17, loss = 0.70002321\n",
      "Iteration 18, loss = 0.69843034\n",
      "Iteration 19, loss = 0.69917185\n",
      "Iteration 20, loss = 0.70081249\n",
      "Iteration 21, loss = 0.69723544\n",
      "Iteration 22, loss = 0.69982330\n",
      "Iteration 23, loss = 0.70080546\n",
      "Iteration 24, loss = 0.69755566\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.14732791\n",
      "Iteration 2, loss = 0.79492808\n",
      "Iteration 3, loss = 0.74385161\n",
      "Iteration 4, loss = 0.72639061\n",
      "Iteration 5, loss = 0.71504787\n",
      "Iteration 6, loss = 0.71218492\n",
      "Iteration 7, loss = 0.70680283\n",
      "Iteration 8, loss = 0.70449593\n",
      "Iteration 9, loss = 0.70428094\n",
      "Iteration 10, loss = 0.70201212\n",
      "Iteration 11, loss = 0.70253356\n",
      "Iteration 12, loss = 0.70032030\n",
      "Iteration 13, loss = 0.69971433\n",
      "Iteration 14, loss = 0.70000873\n",
      "Iteration 15, loss = 0.69717763\n",
      "Iteration 16, loss = 0.69838333\n",
      "Iteration 17, loss = 0.69619834\n",
      "Iteration 18, loss = 0.69391552\n",
      "Iteration 19, loss = 0.69818507\n",
      "Iteration 20, loss = 0.69677856\n",
      "Iteration 21, loss = 0.69522185\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.42080513\n",
      "Iteration 2, loss = 0.98224518\n",
      "Iteration 3, loss = 0.92638486\n",
      "Iteration 4, loss = 0.90027216\n",
      "Iteration 5, loss = 0.88824136\n",
      "Iteration 6, loss = 0.88547752\n",
      "Iteration 7, loss = 0.87497326\n",
      "Iteration 8, loss = 0.87231563\n",
      "Iteration 9, loss = 0.86829131\n",
      "Iteration 10, loss = 0.87048772\n",
      "Iteration 11, loss = 0.86798421\n",
      "Iteration 12, loss = 0.86610353\n",
      "Iteration 13, loss = 0.86440255\n",
      "Iteration 14, loss = 0.86558784\n",
      "Iteration 15, loss = 0.85987635\n",
      "Iteration 16, loss = 0.86165406\n",
      "Iteration 17, loss = 0.86284085\n",
      "Iteration 18, loss = 0.86387672\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.41886816\n",
      "Iteration 2, loss = 0.98667813\n",
      "Iteration 3, loss = 0.92826292\n",
      "Iteration 4, loss = 0.91002275\n",
      "Iteration 5, loss = 0.89204326\n",
      "Iteration 6, loss = 0.88896396\n",
      "Iteration 7, loss = 0.87942842\n",
      "Iteration 8, loss = 0.88172314\n",
      "Iteration 9, loss = 0.87302486\n",
      "Iteration 10, loss = 0.87167298\n",
      "Iteration 11, loss = 0.87367493\n",
      "Iteration 12, loss = 0.87251771\n",
      "Iteration 13, loss = 0.87042168\n",
      "Iteration 14, loss = 0.87071087\n",
      "Iteration 15, loss = 0.86853093\n",
      "Iteration 16, loss = 0.87088334\n",
      "Iteration 17, loss = 0.86696984\n",
      "Iteration 18, loss = 0.86556101\n",
      "Iteration 19, loss = 0.86445916\n",
      "Iteration 20, loss = 0.86044549\n",
      "Iteration 21, loss = 0.86392794\n",
      "Iteration 22, loss = 0.86371690\n",
      "Iteration 23, loss = 0.86239765\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.41937143\n",
      "Iteration 2, loss = 0.98475832\n",
      "Iteration 3, loss = 0.92560403\n",
      "Iteration 4, loss = 0.90634455\n",
      "Iteration 5, loss = 0.89137809\n",
      "Iteration 6, loss = 0.87946976\n",
      "Iteration 7, loss = 0.87637495\n",
      "Iteration 8, loss = 0.87825556\n",
      "Iteration 9, loss = 0.86980039\n",
      "Iteration 10, loss = 0.87019847\n",
      "Iteration 11, loss = 0.86766773\n",
      "Iteration 12, loss = 0.87298479\n",
      "Iteration 13, loss = 0.86857155\n",
      "Iteration 14, loss = 0.86612714\n",
      "Iteration 15, loss = 0.86257155\n",
      "Iteration 16, loss = 0.86256454\n",
      "Iteration 17, loss = 0.86176210\n",
      "Iteration 18, loss = 0.85975433\n",
      "Iteration 19, loss = 0.86133340\n",
      "Iteration 20, loss = 0.85858987\n",
      "Iteration 21, loss = 0.86103194\n",
      "Iteration 22, loss = 0.85865215\n",
      "Iteration 23, loss = 0.86198007\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.55605678\n",
      "Iteration 2, loss = 1.03917522\n",
      "Iteration 3, loss = 0.95238894\n",
      "Iteration 4, loss = 0.91976536\n",
      "Iteration 5, loss = 0.90205772\n",
      "Iteration 6, loss = 0.89220990\n",
      "Iteration 7, loss = 0.88893003\n",
      "Iteration 8, loss = 0.88321982\n",
      "Iteration 9, loss = 0.87840555\n",
      "Iteration 10, loss = 0.87593184\n",
      "Iteration 11, loss = 0.87407858\n",
      "Iteration 12, loss = 0.87207558\n",
      "Iteration 13, loss = 0.87165025\n",
      "Iteration 14, loss = 0.87017066\n",
      "Iteration 15, loss = 0.86508179\n",
      "Iteration 16, loss = 0.86707637\n",
      "Iteration 17, loss = 0.86441069\n",
      "Iteration 18, loss = 0.86293666\n",
      "Iteration 19, loss = 0.86275426\n",
      "Iteration 20, loss = 0.86266886\n",
      "Iteration 21, loss = 0.86223421\n",
      "Iteration 22, loss = 0.86182208\n",
      "Iteration 23, loss = 0.85946445\n",
      "Iteration 24, loss = 0.86178952\n",
      "Iteration 25, loss = 0.86011592\n",
      "Iteration 26, loss = 0.85871279\n",
      "Iteration 27, loss = 0.85682990\n",
      "Iteration 28, loss = 0.85938151\n",
      "Iteration 29, loss = 0.85795361\n",
      "Iteration 30, loss = 0.85889040\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.54336317\n",
      "Iteration 2, loss = 1.03535532\n",
      "Iteration 3, loss = 0.95272298\n",
      "Iteration 4, loss = 0.92137068\n",
      "Iteration 5, loss = 0.90452990\n",
      "Iteration 6, loss = 0.89377168\n",
      "Iteration 7, loss = 0.88849913\n",
      "Iteration 8, loss = 0.88858695\n",
      "Iteration 9, loss = 0.87917215\n",
      "Iteration 10, loss = 0.87635053\n",
      "Iteration 11, loss = 0.87436134\n",
      "Iteration 12, loss = 0.87571147\n",
      "Iteration 13, loss = 0.87769398\n",
      "Iteration 14, loss = 0.87290545\n",
      "Iteration 15, loss = 0.87054420\n",
      "Iteration 16, loss = 0.87050091\n",
      "Iteration 17, loss = 0.86674275\n",
      "Iteration 18, loss = 0.86560969\n",
      "Iteration 19, loss = 0.86720297\n",
      "Iteration 20, loss = 0.86829657\n",
      "Iteration 21, loss = 0.86623118\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.54995855\n",
      "Iteration 2, loss = 1.04039314\n",
      "Iteration 3, loss = 0.95326623\n",
      "Iteration 4, loss = 0.92205152\n",
      "Iteration 5, loss = 0.90405393\n",
      "Iteration 6, loss = 0.89313571\n",
      "Iteration 7, loss = 0.88721930\n",
      "Iteration 8, loss = 0.88609576\n",
      "Iteration 9, loss = 0.87984037\n",
      "Iteration 10, loss = 0.87980833\n",
      "Iteration 11, loss = 0.87390995\n",
      "Iteration 12, loss = 0.87277650\n",
      "Iteration 13, loss = 0.87535432\n",
      "Iteration 14, loss = 0.86999645\n",
      "Iteration 15, loss = 0.86763462\n",
      "Iteration 16, loss = 0.86693853\n",
      "Iteration 17, loss = 0.86553455\n",
      "Iteration 18, loss = 0.86518968\n",
      "Iteration 19, loss = 0.86345242\n",
      "Iteration 20, loss = 0.86375791\n",
      "Iteration 21, loss = 0.86386632\n",
      "Iteration 22, loss = 0.86658019\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48122449\n",
      "Iteration 2, loss = 1.00497638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.93608164\n",
      "Iteration 4, loss = 0.90899642\n",
      "Iteration 5, loss = 0.89768490\n",
      "Iteration 6, loss = 0.88799054\n",
      "Iteration 7, loss = 0.88081189\n",
      "Iteration 8, loss = 0.87930182\n",
      "Iteration 9, loss = 0.87269636\n",
      "Iteration 10, loss = 0.86980057\n",
      "Iteration 11, loss = 0.87020761\n",
      "Iteration 12, loss = 0.86926344\n",
      "Iteration 13, loss = 0.86663256\n",
      "Iteration 14, loss = 0.86566689\n",
      "Iteration 15, loss = 0.86576598\n",
      "Iteration 16, loss = 0.86430167\n",
      "Iteration 17, loss = 0.86348909\n",
      "Iteration 18, loss = 0.86421291\n",
      "Iteration 19, loss = 0.86761761\n",
      "Iteration 20, loss = 0.85913930\n",
      "Iteration 21, loss = 0.85956782\n",
      "Iteration 22, loss = 0.85923148\n",
      "Iteration 23, loss = 0.85968039\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48954795\n",
      "Iteration 2, loss = 1.00813732\n",
      "Iteration 3, loss = 0.94024416\n",
      "Iteration 4, loss = 0.90982929\n",
      "Iteration 5, loss = 0.90057520\n",
      "Iteration 6, loss = 0.89228765\n",
      "Iteration 7, loss = 0.88457221\n",
      "Iteration 8, loss = 0.88243709\n",
      "Iteration 9, loss = 0.88056521\n",
      "Iteration 10, loss = 0.87632299\n",
      "Iteration 11, loss = 0.87428195\n",
      "Iteration 12, loss = 0.87384016\n",
      "Iteration 13, loss = 0.87256257\n",
      "Iteration 14, loss = 0.86894463\n",
      "Iteration 15, loss = 0.86863481\n",
      "Iteration 16, loss = 0.86580263\n",
      "Iteration 17, loss = 0.86670476\n",
      "Iteration 18, loss = 0.86559316\n",
      "Iteration 19, loss = 0.86487709\n",
      "Iteration 20, loss = 0.86535539\n",
      "Iteration 21, loss = 0.86320696\n",
      "Iteration 22, loss = 0.86369853\n",
      "Iteration 23, loss = 0.86595775\n",
      "Iteration 24, loss = 0.86307682\n",
      "Iteration 25, loss = 0.86327903\n",
      "Iteration 26, loss = 0.86267514\n",
      "Iteration 27, loss = 0.86144319\n",
      "Iteration 28, loss = 0.85923499\n",
      "Iteration 29, loss = 0.86396183\n",
      "Iteration 30, loss = 0.86331126\n",
      "Iteration 31, loss = 0.86080840\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48632859\n",
      "Iteration 2, loss = 1.01085432\n",
      "Iteration 3, loss = 0.94006181\n",
      "Iteration 4, loss = 0.91180817\n",
      "Iteration 5, loss = 0.89547010\n",
      "Iteration 6, loss = 0.88976854\n",
      "Iteration 7, loss = 0.88375498\n",
      "Iteration 8, loss = 0.87943327\n",
      "Iteration 9, loss = 0.87617580\n",
      "Iteration 10, loss = 0.87642492\n",
      "Iteration 11, loss = 0.87271049\n",
      "Iteration 12, loss = 0.86696764\n",
      "Iteration 13, loss = 0.86668836\n",
      "Iteration 14, loss = 0.86526127\n",
      "Iteration 15, loss = 0.86241356\n",
      "Iteration 16, loss = 0.86379066\n",
      "Iteration 17, loss = 0.86430381\n",
      "Iteration 18, loss = 0.86334362\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.15980918\n",
      "Iteration 2, loss = 0.86599495\n",
      "Iteration 3, loss = 0.83825462\n",
      "Iteration 4, loss = 0.82670159\n",
      "Iteration 5, loss = 0.82078322\n",
      "Iteration 6, loss = 0.81498646\n",
      "Iteration 7, loss = 0.81466703\n",
      "Iteration 8, loss = 0.81288272\n",
      "Iteration 9, loss = 0.81160883\n",
      "Iteration 10, loss = 0.81242325\n",
      "Iteration 11, loss = 0.81025979\n",
      "Iteration 12, loss = 0.80880756\n",
      "Iteration 13, loss = 0.80988015\n",
      "Iteration 14, loss = 0.80978645\n",
      "Iteration 15, loss = 0.80678647\n",
      "Iteration 16, loss = 0.80530802\n",
      "Iteration 17, loss = 0.80399706\n",
      "Iteration 18, loss = 0.80767689\n",
      "Iteration 19, loss = 0.80619929\n",
      "Iteration 20, loss = 0.80892802\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.16239205\n",
      "Iteration 2, loss = 0.87054290\n",
      "Iteration 3, loss = 0.83940083\n",
      "Iteration 4, loss = 0.82616247\n",
      "Iteration 5, loss = 0.81909132\n",
      "Iteration 6, loss = 0.82412205\n",
      "Iteration 7, loss = 0.81946103\n",
      "Iteration 8, loss = 0.81808243\n",
      "Iteration 9, loss = 0.81659263\n",
      "Iteration 10, loss = 0.81490096\n",
      "Iteration 11, loss = 0.81262326\n",
      "Iteration 12, loss = 0.81398908\n",
      "Iteration 13, loss = 0.80949447\n",
      "Iteration 14, loss = 0.81107671\n",
      "Iteration 15, loss = 0.81080632\n",
      "Iteration 16, loss = 0.80866992\n",
      "Iteration 17, loss = 0.81007032\n",
      "Iteration 18, loss = 0.81070210\n",
      "Iteration 19, loss = 0.80872640\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.17258918\n",
      "Iteration 2, loss = 0.86539828\n",
      "Iteration 3, loss = 0.83443070\n",
      "Iteration 4, loss = 0.82379747\n",
      "Iteration 5, loss = 0.81898463\n",
      "Iteration 6, loss = 0.81752015\n",
      "Iteration 7, loss = 0.81562529\n",
      "Iteration 8, loss = 0.81354610\n",
      "Iteration 9, loss = 0.81017252\n",
      "Iteration 10, loss = 0.81154934\n",
      "Iteration 11, loss = 0.80764155\n",
      "Iteration 12, loss = 0.81288727\n",
      "Iteration 13, loss = 0.80740427\n",
      "Iteration 14, loss = 0.80851856\n",
      "Iteration 15, loss = 0.80722717\n",
      "Iteration 16, loss = 0.80850973\n",
      "Iteration 17, loss = 0.80826739\n",
      "Iteration 18, loss = 0.80545382\n",
      "Iteration 19, loss = 0.80487763\n",
      "Iteration 20, loss = 0.80321335\n",
      "Iteration 21, loss = 0.80942845\n",
      "Iteration 22, loss = 0.80768884\n",
      "Iteration 23, loss = 0.80751085\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.22975492\n",
      "Iteration 2, loss = 0.88913150\n",
      "Iteration 3, loss = 0.84825855\n",
      "Iteration 4, loss = 0.83139766\n",
      "Iteration 5, loss = 0.82357442\n",
      "Iteration 6, loss = 0.81941395\n",
      "Iteration 7, loss = 0.81716707\n",
      "Iteration 8, loss = 0.81549607\n",
      "Iteration 9, loss = 0.81259015\n",
      "Iteration 10, loss = 0.81057586\n",
      "Iteration 11, loss = 0.81213134\n",
      "Iteration 12, loss = 0.81010709\n",
      "Iteration 13, loss = 0.81192474\n",
      "Iteration 14, loss = 0.80972689\n",
      "Iteration 15, loss = 0.80879195\n",
      "Iteration 16, loss = 0.80911716\n",
      "Iteration 17, loss = 0.80758055\n",
      "Iteration 18, loss = 0.80830220\n",
      "Iteration 19, loss = 0.80847713\n",
      "Iteration 20, loss = 0.80674570\n",
      "Iteration 21, loss = 0.80849485\n",
      "Iteration 22, loss = 0.81149091\n",
      "Iteration 23, loss = 0.80550890\n",
      "Iteration 24, loss = 0.80756168\n",
      "Iteration 25, loss = 0.80565152\n",
      "Iteration 26, loss = 0.80901011\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.22794063\n",
      "Iteration 2, loss = 0.88738972\n",
      "Iteration 3, loss = 0.84701466\n",
      "Iteration 4, loss = 0.83395118\n",
      "Iteration 5, loss = 0.82609632\n",
      "Iteration 6, loss = 0.82305474\n",
      "Iteration 7, loss = 0.81864588\n",
      "Iteration 8, loss = 0.81572400\n",
      "Iteration 9, loss = 0.81850956\n",
      "Iteration 10, loss = 0.81808520\n",
      "Iteration 11, loss = 0.81395433\n",
      "Iteration 12, loss = 0.81331654\n",
      "Iteration 13, loss = 0.81432155\n",
      "Iteration 14, loss = 0.81164247\n",
      "Iteration 15, loss = 0.81131510\n",
      "Iteration 16, loss = 0.81029381\n",
      "Iteration 17, loss = 0.81116726\n",
      "Iteration 18, loss = 0.80967658\n",
      "Iteration 19, loss = 0.80995304\n",
      "Iteration 20, loss = 0.81088706\n",
      "Iteration 21, loss = 0.80836351\n",
      "Iteration 22, loss = 0.81094621\n",
      "Iteration 23, loss = 0.81228897\n",
      "Iteration 24, loss = 0.80914711\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.22988183\n",
      "Iteration 2, loss = 0.88597375\n",
      "Iteration 3, loss = 0.84366889\n",
      "Iteration 4, loss = 0.83108124\n",
      "Iteration 5, loss = 0.82187089\n",
      "Iteration 6, loss = 0.82024420\n",
      "Iteration 7, loss = 0.81666068\n",
      "Iteration 8, loss = 0.81404041\n",
      "Iteration 9, loss = 0.81426852\n",
      "Iteration 10, loss = 0.81254491\n",
      "Iteration 11, loss = 0.81166622\n",
      "Iteration 12, loss = 0.81050815\n",
      "Iteration 13, loss = 0.81021513\n",
      "Iteration 14, loss = 0.81064868\n",
      "Iteration 15, loss = 0.80754154\n",
      "Iteration 16, loss = 0.80888176\n",
      "Iteration 17, loss = 0.80696138\n",
      "Iteration 18, loss = 0.80495183\n",
      "Iteration 19, loss = 0.80908477\n",
      "Iteration 20, loss = 0.80793376\n",
      "Iteration 21, loss = 0.80646525\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.58803768\n",
      "Iteration 2, loss = 1.14645666\n",
      "Iteration 3, loss = 1.08512935\n",
      "Iteration 4, loss = 1.06010508\n",
      "Iteration 5, loss = 1.04845350\n",
      "Iteration 6, loss = 1.04820536\n",
      "Iteration 7, loss = 1.03692154\n",
      "Iteration 8, loss = 1.03594132\n",
      "Iteration 9, loss = 1.03193383\n",
      "Iteration 10, loss = 1.03417054\n",
      "Iteration 11, loss = 1.03125278\n",
      "Iteration 12, loss = 1.02888210\n",
      "Iteration 13, loss = 1.02924346\n",
      "Iteration 14, loss = 1.03104573\n",
      "Iteration 15, loss = 1.02605290\n",
      "Iteration 16, loss = 1.02567938\n",
      "Iteration 17, loss = 1.02990813\n",
      "Iteration 18, loss = 1.03057196\n",
      "Iteration 19, loss = 1.02548158\n",
      "Iteration 20, loss = 1.02621898\n",
      "Iteration 21, loss = 1.02635692\n",
      "Iteration 22, loss = 1.02257981\n",
      "Iteration 23, loss = 1.02159102\n",
      "Iteration 24, loss = 1.02374353\n",
      "Iteration 25, loss = 1.02741181\n",
      "Iteration 26, loss = 1.02017315\n",
      "Iteration 27, loss = 1.02380096\n",
      "Iteration 28, loss = 1.02679376\n",
      "Iteration 29, loss = 1.01853741\n",
      "Iteration 30, loss = 1.02098095\n",
      "Iteration 31, loss = 1.01834978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32, loss = 1.02044292\n",
      "Iteration 33, loss = 1.02460849\n",
      "Iteration 34, loss = 1.02050663\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.58639679\n",
      "Iteration 2, loss = 1.14868315\n",
      "Iteration 3, loss = 1.08748652\n",
      "Iteration 4, loss = 1.06902449\n",
      "Iteration 5, loss = 1.05031967\n",
      "Iteration 6, loss = 1.04977731\n",
      "Iteration 7, loss = 1.04037383\n",
      "Iteration 8, loss = 1.04249414\n",
      "Iteration 9, loss = 1.03602248\n",
      "Iteration 10, loss = 1.03396434\n",
      "Iteration 11, loss = 1.03613307\n",
      "Iteration 12, loss = 1.03535752\n",
      "Iteration 13, loss = 1.03395552\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.58746694\n",
      "Iteration 2, loss = 1.14740761\n",
      "Iteration 3, loss = 1.08392302\n",
      "Iteration 4, loss = 1.06362661\n",
      "Iteration 5, loss = 1.05144522\n",
      "Iteration 6, loss = 1.04075091\n",
      "Iteration 7, loss = 1.03846847\n",
      "Iteration 8, loss = 1.03957250\n",
      "Iteration 9, loss = 1.03342525\n",
      "Iteration 10, loss = 1.03340278\n",
      "Iteration 11, loss = 1.03148129\n",
      "Iteration 12, loss = 1.03602919\n",
      "Iteration 13, loss = 1.03306563\n",
      "Iteration 14, loss = 1.03049222\n",
      "Iteration 15, loss = 1.02633481\n",
      "Iteration 16, loss = 1.02905083\n",
      "Iteration 17, loss = 1.02973517\n",
      "Iteration 18, loss = 1.02653081\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.69832010\n",
      "Iteration 2, loss = 1.20776456\n",
      "Iteration 3, loss = 1.11580843\n",
      "Iteration 4, loss = 1.08228130\n",
      "Iteration 5, loss = 1.06603197\n",
      "Iteration 6, loss = 1.05533801\n",
      "Iteration 7, loss = 1.05297522\n",
      "Iteration 8, loss = 1.04738037\n",
      "Iteration 9, loss = 1.04288226\n",
      "Iteration 10, loss = 1.04108841\n",
      "Iteration 11, loss = 1.04072833\n",
      "Iteration 12, loss = 1.03831121\n",
      "Iteration 13, loss = 1.03901188\n",
      "Iteration 14, loss = 1.03677200\n",
      "Iteration 15, loss = 1.03374362\n",
      "Iteration 16, loss = 1.03687485\n",
      "Iteration 17, loss = 1.03331536\n",
      "Iteration 18, loss = 1.03258706\n",
      "Iteration 19, loss = 1.03107701\n",
      "Iteration 20, loss = 1.03348699\n",
      "Iteration 21, loss = 1.03107781\n",
      "Iteration 22, loss = 1.03142936\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.68812460\n",
      "Iteration 2, loss = 1.20603026\n",
      "Iteration 3, loss = 1.11744522\n",
      "Iteration 4, loss = 1.08407981\n",
      "Iteration 5, loss = 1.06667137\n",
      "Iteration 6, loss = 1.05645508\n",
      "Iteration 7, loss = 1.05137255\n",
      "Iteration 8, loss = 1.05142246\n",
      "Iteration 9, loss = 1.04391634\n",
      "Iteration 10, loss = 1.04007009\n",
      "Iteration 11, loss = 1.03778673\n",
      "Iteration 12, loss = 1.03957630\n",
      "Iteration 13, loss = 1.04284989\n",
      "Iteration 14, loss = 1.03842526\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.69514095\n",
      "Iteration 2, loss = 1.21158489\n",
      "Iteration 3, loss = 1.11803581\n",
      "Iteration 4, loss = 1.08437246\n",
      "Iteration 5, loss = 1.06621589\n",
      "Iteration 6, loss = 1.05684522\n",
      "Iteration 7, loss = 1.05065185\n",
      "Iteration 8, loss = 1.04949210\n",
      "Iteration 9, loss = 1.04398592\n",
      "Iteration 10, loss = 1.04397647\n",
      "Iteration 11, loss = 1.04025894\n",
      "Iteration 12, loss = 1.03666060\n",
      "Iteration 13, loss = 1.04011902\n",
      "Iteration 14, loss = 1.03666875\n",
      "Iteration 15, loss = 1.03518138\n",
      "Iteration 16, loss = 1.03333377\n",
      "Iteration 17, loss = 1.03262893\n",
      "Iteration 18, loss = 1.03282952\n",
      "Iteration 19, loss = 1.03129303\n",
      "Iteration 20, loss = 1.03171258\n",
      "Iteration 21, loss = 1.03271374\n",
      "Iteration 22, loss = 1.03564189\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.64450574\n",
      "Iteration 2, loss = 1.17155638\n",
      "Iteration 3, loss = 1.09903488\n",
      "Iteration 4, loss = 1.07129230\n",
      "Iteration 5, loss = 1.06096694\n",
      "Iteration 6, loss = 1.05331586\n",
      "Iteration 7, loss = 1.04639866\n",
      "Iteration 8, loss = 1.04483229\n",
      "Iteration 9, loss = 1.03980546\n",
      "Iteration 10, loss = 1.03714300\n",
      "Iteration 11, loss = 1.03894393\n",
      "Iteration 12, loss = 1.03842536\n",
      "Iteration 13, loss = 1.03487365\n",
      "Iteration 14, loss = 1.03514525\n",
      "Iteration 15, loss = 1.03507414\n",
      "Iteration 16, loss = 1.03341984\n",
      "Iteration 17, loss = 1.03384930\n",
      "Iteration 18, loss = 1.03444372\n",
      "Iteration 19, loss = 1.03673952\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.65189832\n",
      "Iteration 2, loss = 1.17161108\n",
      "Iteration 3, loss = 1.10089059\n",
      "Iteration 4, loss = 1.07177721\n",
      "Iteration 5, loss = 1.06285421\n",
      "Iteration 6, loss = 1.05440067\n",
      "Iteration 7, loss = 1.04896367\n",
      "Iteration 8, loss = 1.04829832\n",
      "Iteration 9, loss = 1.04675721\n",
      "Iteration 10, loss = 1.04238525\n",
      "Iteration 11, loss = 1.03968035\n",
      "Iteration 12, loss = 1.04050228\n",
      "Iteration 13, loss = 1.03862422\n",
      "Iteration 14, loss = 1.03667108\n",
      "Iteration 15, loss = 1.03688175\n",
      "Iteration 16, loss = 1.03706125\n",
      "Iteration 17, loss = 1.03527738\n",
      "Iteration 18, loss = 1.03533729\n",
      "Iteration 19, loss = 1.03523421\n",
      "Iteration 20, loss = 1.03459282\n",
      "Iteration 21, loss = 1.03397775\n",
      "Iteration 22, loss = 1.03382064\n",
      "Iteration 23, loss = 1.03301559\n",
      "Iteration 24, loss = 1.03389862\n",
      "Iteration 25, loss = 1.03187754\n",
      "Iteration 26, loss = 1.03550724\n",
      "Iteration 27, loss = 1.03179497\n",
      "Iteration 28, loss = 1.03007230\n",
      "Iteration 29, loss = 1.03207005\n",
      "Iteration 30, loss = 1.03304279\n",
      "Iteration 31, loss = 1.02816571\n",
      "Iteration 32, loss = 1.03224562\n",
      "Iteration 33, loss = 1.03172163\n",
      "Iteration 34, loss = 1.02845978\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.64772877\n",
      "Iteration 2, loss = 1.17440875\n",
      "Iteration 3, loss = 1.10144589\n",
      "Iteration 4, loss = 1.07271398\n",
      "Iteration 5, loss = 1.05903483\n",
      "Iteration 6, loss = 1.05431152\n",
      "Iteration 7, loss = 1.04771964\n",
      "Iteration 8, loss = 1.04488658\n",
      "Iteration 9, loss = 1.04357057\n",
      "Iteration 10, loss = 1.04453499\n",
      "Iteration 11, loss = 1.04100655\n",
      "Iteration 12, loss = 1.03750943\n",
      "Iteration 13, loss = 1.03537253\n",
      "Iteration 14, loss = 1.03623669\n",
      "Iteration 15, loss = 1.03340353\n",
      "Iteration 16, loss = 1.03457720\n",
      "Iteration 17, loss = 1.03393407\n",
      "Iteration 18, loss = 1.03454079\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.46583758\n",
      "Iteration 2, loss = 1.18392046\n",
      "Iteration 3, loss = 1.16732832\n",
      "Iteration 4, loss = 1.15996988\n",
      "Iteration 5, loss = 1.15651581\n",
      "Iteration 6, loss = 1.15406234\n",
      "Iteration 7, loss = 1.15383504\n",
      "Iteration 8, loss = 1.15156255\n",
      "Iteration 9, loss = 1.14938748\n",
      "Iteration 10, loss = 1.15060728\n",
      "Iteration 11, loss = 1.14873345\n",
      "Iteration 12, loss = 1.14800875\n",
      "Iteration 13, loss = 1.14852969\n",
      "Iteration 14, loss = 1.14818161\n",
      "Iteration 15, loss = 1.14928812\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.46708911\n",
      "Iteration 2, loss = 1.18554152\n",
      "Iteration 3, loss = 1.16919133\n",
      "Iteration 4, loss = 1.15854535\n",
      "Iteration 5, loss = 1.15546869\n",
      "Iteration 6, loss = 1.15668752\n",
      "Iteration 7, loss = 1.15344365\n",
      "Iteration 8, loss = 1.15437700\n",
      "Iteration 9, loss = 1.15412595\n",
      "Iteration 10, loss = 1.15319789\n",
      "Iteration 11, loss = 1.15215209\n",
      "Iteration 12, loss = 1.15126330\n",
      "Iteration 13, loss = 1.14675597\n",
      "Iteration 14, loss = 1.14854536\n",
      "Iteration 15, loss = 1.14771300\n",
      "Iteration 16, loss = 1.14631930\n",
      "Iteration 17, loss = 1.14766024\n",
      "Iteration 18, loss = 1.14812971\n",
      "Iteration 19, loss = 1.14736919\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.47651616\n",
      "Iteration 2, loss = 1.18091345\n",
      "Iteration 3, loss = 1.16628805\n",
      "Iteration 4, loss = 1.15875802\n",
      "Iteration 5, loss = 1.15494950\n",
      "Iteration 6, loss = 1.15411549\n",
      "Iteration 7, loss = 1.15363786\n",
      "Iteration 8, loss = 1.15128853\n",
      "Iteration 9, loss = 1.15015039\n",
      "Iteration 10, loss = 1.14973661\n",
      "Iteration 11, loss = 1.14853804\n",
      "Iteration 12, loss = 1.15001285\n",
      "Iteration 13, loss = 1.14720259\n",
      "Iteration 14, loss = 1.14716140\n",
      "Iteration 15, loss = 1.14817841\n",
      "Iteration 16, loss = 1.14770636\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.50516911\n",
      "Iteration 2, loss = 1.20349417\n",
      "Iteration 3, loss = 1.18165458\n",
      "Iteration 4, loss = 1.17376905\n",
      "Iteration 5, loss = 1.16820818\n",
      "Iteration 6, loss = 1.16514869\n",
      "Iteration 7, loss = 1.16341672\n",
      "Iteration 8, loss = 1.16137297\n",
      "Iteration 9, loss = 1.15942795\n",
      "Iteration 10, loss = 1.15720987\n",
      "Iteration 11, loss = 1.15770222\n",
      "Iteration 12, loss = 1.15747629\n",
      "Iteration 13, loss = 1.15918834\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.50318671\n",
      "Iteration 2, loss = 1.20182206\n",
      "Iteration 3, loss = 1.18094378\n",
      "Iteration 4, loss = 1.17550587\n",
      "Iteration 5, loss = 1.16933527\n",
      "Iteration 6, loss = 1.16751490\n",
      "Iteration 7, loss = 1.16444050\n",
      "Iteration 8, loss = 1.16086723\n",
      "Iteration 9, loss = 1.16187861\n",
      "Iteration 10, loss = 1.16103477\n",
      "Iteration 11, loss = 1.15805836\n",
      "Iteration 12, loss = 1.15883032\n",
      "Iteration 13, loss = 1.15799438\n",
      "Iteration 14, loss = 1.15641473\n",
      "Iteration 15, loss = 1.15511126\n",
      "Iteration 16, loss = 1.15505545\n",
      "Iteration 17, loss = 1.15566785\n",
      "Iteration 18, loss = 1.15596609\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.50510441\n",
      "Iteration 2, loss = 1.20166823\n",
      "Iteration 3, loss = 1.17888415\n",
      "Iteration 4, loss = 1.17276896\n",
      "Iteration 5, loss = 1.16650594\n",
      "Iteration 6, loss = 1.16545590\n",
      "Iteration 7, loss = 1.16263941\n",
      "Iteration 8, loss = 1.16058219\n",
      "Iteration 9, loss = 1.15958734\n",
      "Iteration 10, loss = 1.15887940\n",
      "Iteration 11, loss = 1.15749890\n",
      "Iteration 12, loss = 1.15774844\n",
      "Iteration 13, loss = 1.15636634\n",
      "Iteration 14, loss = 1.15625831\n",
      "Iteration 15, loss = 1.15521396\n",
      "Iteration 16, loss = 1.15581784\n",
      "Iteration 17, loss = 1.15356549\n",
      "Iteration 18, loss = 1.15317338\n",
      "Iteration 19, loss = 1.15463166\n",
      "Iteration 20, loss = 1.15554902\n",
      "Iteration 21, loss = 1.15358741\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.07304685\n",
      "Iteration 2, loss = 1.65390257\n",
      "Iteration 3, loss = 1.61877231\n",
      "Iteration 4, loss = 1.60656937\n",
      "Iteration 5, loss = 1.60153616\n",
      "Iteration 6, loss = 1.60176053\n",
      "Iteration 7, loss = 1.59713201\n",
      "Iteration 8, loss = 1.59194066\n",
      "Iteration 9, loss = 1.59538180\n",
      "Iteration 10, loss = 1.59568607\n",
      "Iteration 11, loss = 1.59320362\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.06928743\n",
      "Iteration 2, loss = 1.65412069\n",
      "Iteration 3, loss = 1.61650774\n",
      "Iteration 4, loss = 1.60535876\n",
      "Iteration 5, loss = 1.60002054\n",
      "Iteration 6, loss = 1.59815217\n",
      "Iteration 7, loss = 1.59458241\n",
      "Iteration 8, loss = 1.59322180\n",
      "Iteration 9, loss = 1.59436638\n",
      "Iteration 10, loss = 1.59227998\n",
      "Iteration 11, loss = 1.59188195\n",
      "Iteration 12, loss = 1.59252599\n",
      "Iteration 13, loss = 1.59012560\n",
      "Iteration 14, loss = 1.59174077\n",
      "Iteration 15, loss = 1.59111394\n",
      "Iteration 16, loss = 1.59016003\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.07162820\n",
      "Iteration 2, loss = 1.65539642\n",
      "Iteration 3, loss = 1.61901440\n",
      "Iteration 4, loss = 1.60550430\n",
      "Iteration 5, loss = 1.60205506\n",
      "Iteration 6, loss = 1.59706295\n",
      "Iteration 7, loss = 1.59573305\n",
      "Iteration 8, loss = 1.59304856\n",
      "Iteration 9, loss = 1.59080418\n",
      "Iteration 10, loss = 1.59264790\n",
      "Iteration 11, loss = 1.59303653\n",
      "Iteration 12, loss = 1.59237973\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12277695\n",
      "Iteration 2, loss = 1.70068982\n",
      "Iteration 3, loss = 1.64939265\n",
      "Iteration 4, loss = 1.63347649\n",
      "Iteration 5, loss = 1.62849218\n",
      "Iteration 6, loss = 1.62370925\n",
      "Iteration 7, loss = 1.62186009\n",
      "Iteration 8, loss = 1.62031417\n",
      "Iteration 9, loss = 1.61847398\n",
      "Iteration 10, loss = 1.61861689\n",
      "Iteration 11, loss = 1.61728853\n",
      "Iteration 12, loss = 1.61743878\n",
      "Iteration 13, loss = 1.61851705\n",
      "Iteration 14, loss = 1.61415009\n",
      "Iteration 15, loss = 1.61056744\n",
      "Iteration 16, loss = 1.60730613\n",
      "Iteration 17, loss = 1.59851445\n",
      "Iteration 18, loss = 1.59584018\n",
      "Iteration 19, loss = 1.59149656\n",
      "Iteration 20, loss = 1.58863124\n",
      "Iteration 21, loss = 1.58631997\n",
      "Iteration 22, loss = 1.58510370\n",
      "Iteration 23, loss = 1.58612360\n",
      "Iteration 24, loss = 1.58416292\n",
      "Iteration 25, loss = 1.58228547\n",
      "Iteration 26, loss = 1.58244716\n",
      "Iteration 27, loss = 1.57897318\n",
      "Iteration 28, loss = 1.58193682\n",
      "Iteration 29, loss = 1.58079141\n",
      "Iteration 30, loss = 1.57877708\n",
      "Iteration 31, loss = 1.57841800\n",
      "Iteration 32, loss = 1.58079320\n",
      "Iteration 33, loss = 1.58072853\n",
      "Iteration 34, loss = 1.57937892\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.11735215\n",
      "Iteration 2, loss = 1.69974146\n",
      "Iteration 3, loss = 1.64792651\n",
      "Iteration 4, loss = 1.63208112\n",
      "Iteration 5, loss = 1.62662500\n",
      "Iteration 6, loss = 1.62127586\n",
      "Iteration 7, loss = 1.62066659\n",
      "Iteration 8, loss = 1.61888760\n",
      "Iteration 9, loss = 1.61784000\n",
      "Iteration 10, loss = 1.61685224\n",
      "Iteration 11, loss = 1.61622551\n",
      "Iteration 12, loss = 1.61611324\n",
      "Iteration 13, loss = 1.61543484\n",
      "Iteration 14, loss = 1.61498025\n",
      "Iteration 15, loss = 1.61279564\n",
      "Iteration 16, loss = 1.60819320\n",
      "Iteration 17, loss = 1.60275171\n",
      "Iteration 18, loss = 1.59709606\n",
      "Iteration 19, loss = 1.59591565\n",
      "Iteration 20, loss = 1.59133277\n",
      "Iteration 21, loss = 1.58783052\n",
      "Iteration 22, loss = 1.58647755\n",
      "Iteration 23, loss = 1.58729371\n",
      "Iteration 24, loss = 1.58555156\n",
      "Iteration 25, loss = 1.58354929\n",
      "Iteration 26, loss = 1.58262012\n",
      "Iteration 27, loss = 1.57925138\n",
      "Iteration 28, loss = 1.58180447\n",
      "Iteration 29, loss = 1.58030805\n",
      "Iteration 30, loss = 1.57983084\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12232866\n",
      "Iteration 2, loss = 1.70226629\n",
      "Iteration 3, loss = 1.64775895\n",
      "Iteration 4, loss = 1.63317816\n",
      "Iteration 5, loss = 1.62514689\n",
      "Iteration 6, loss = 1.62181145\n",
      "Iteration 7, loss = 1.62099391\n",
      "Iteration 8, loss = 1.61861912\n",
      "Iteration 9, loss = 1.61780701\n",
      "Iteration 10, loss = 1.61829551\n",
      "Iteration 11, loss = 1.61802678\n",
      "Iteration 12, loss = 1.61571415\n",
      "Iteration 13, loss = 1.61581996\n",
      "Iteration 14, loss = 1.61502138\n",
      "Iteration 15, loss = 1.61143583\n",
      "Iteration 16, loss = 1.60768327\n",
      "Iteration 17, loss = 1.60095568\n",
      "Iteration 18, loss = 1.59528909\n",
      "Iteration 19, loss = 1.59062512\n",
      "Iteration 20, loss = 1.58965040\n",
      "Iteration 21, loss = 1.58783827\n",
      "Iteration 22, loss = 1.58821043\n",
      "Iteration 23, loss = 1.58475673\n",
      "Iteration 24, loss = 1.58307199\n",
      "Iteration 25, loss = 1.58147986\n",
      "Iteration 26, loss = 1.57929420\n",
      "Iteration 27, loss = 1.57833884\n",
      "Iteration 28, loss = 1.57886859\n",
      "Iteration 29, loss = 1.57808587\n",
      "Iteration 30, loss = 1.57881341\n",
      "Iteration 31, loss = 1.57998103\n",
      "Iteration 32, loss = 1.57755900\n",
      "Iteration 33, loss = 1.57982558\n",
      "Iteration 34, loss = 1.57963094\n",
      "Iteration 35, loss = 1.57772205\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.11787432\n",
      "Iteration 2, loss = 1.67731367\n",
      "Iteration 3, loss = 1.63783941\n",
      "Iteration 4, loss = 1.62130248\n",
      "Iteration 5, loss = 1.61551682\n",
      "Iteration 6, loss = 1.61238553\n",
      "Iteration 7, loss = 1.60728530\n",
      "Iteration 8, loss = 1.60668971\n",
      "Iteration 9, loss = 1.60541122\n",
      "Iteration 10, loss = 1.60238826\n",
      "Iteration 11, loss = 1.60482788\n",
      "Iteration 12, loss = 1.60354449\n",
      "Iteration 13, loss = 1.60304825\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12259988\n",
      "Iteration 2, loss = 1.67921550\n",
      "Iteration 3, loss = 1.63499040\n",
      "Iteration 4, loss = 1.62050971\n",
      "Iteration 5, loss = 1.61474030\n",
      "Iteration 6, loss = 1.60891502\n",
      "Iteration 7, loss = 1.61056635\n",
      "Iteration 8, loss = 1.60311684\n",
      "Iteration 9, loss = 1.60543673\n",
      "Iteration 10, loss = 1.60077869\n",
      "Iteration 11, loss = 1.60308535\n",
      "Iteration 12, loss = 1.60463369\n",
      "Iteration 13, loss = 1.60077014\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12190292\n",
      "Iteration 2, loss = 1.67956156\n",
      "Iteration 3, loss = 1.63795850\n",
      "Iteration 4, loss = 1.62152486\n",
      "Iteration 5, loss = 1.61610244\n",
      "Iteration 6, loss = 1.61338696\n",
      "Iteration 7, loss = 1.60606651\n",
      "Iteration 8, loss = 1.60423558\n",
      "Iteration 9, loss = 1.60402553\n",
      "Iteration 10, loss = 1.60637932\n",
      "Iteration 11, loss = 1.60192063\n",
      "Iteration 12, loss = 1.60310138\n",
      "Iteration 13, loss = 1.59978708\n",
      "Iteration 14, loss = 1.60078871\n",
      "Iteration 15, loss = 1.60126303\n",
      "Iteration 16, loss = 1.60147590\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.09000746\n",
      "Iteration 2, loss = 1.73997314\n",
      "Iteration 3, loss = 1.72713606\n",
      "Iteration 4, loss = 1.72144450\n",
      "Iteration 5, loss = 1.71834419\n",
      "Iteration 6, loss = 1.71617169\n",
      "Iteration 7, loss = 1.71599099\n",
      "Iteration 8, loss = 1.71518639\n",
      "Iteration 9, loss = 1.71225914\n",
      "Iteration 10, loss = 1.71302554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 1.71185319\n",
      "Iteration 12, loss = 1.71322410\n",
      "Iteration 13, loss = 1.71205795\n",
      "Iteration 14, loss = 1.71127171\n",
      "Iteration 15, loss = 1.71356403\n",
      "Iteration 16, loss = 1.70979132\n",
      "Iteration 17, loss = 1.70908819\n",
      "Iteration 18, loss = 1.71358294\n",
      "Iteration 19, loss = 1.71161998\n",
      "Iteration 20, loss = 1.71179712\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.08786292\n",
      "Iteration 2, loss = 1.73855651\n",
      "Iteration 3, loss = 1.72660752\n",
      "Iteration 4, loss = 1.72116924\n",
      "Iteration 5, loss = 1.71890604\n",
      "Iteration 6, loss = 1.71618146\n",
      "Iteration 7, loss = 1.71594725\n",
      "Iteration 8, loss = 1.71497826\n",
      "Iteration 9, loss = 1.71508343\n",
      "Iteration 10, loss = 1.71572617\n",
      "Iteration 11, loss = 1.71220290\n",
      "Iteration 12, loss = 1.71277230\n",
      "Iteration 13, loss = 1.70997612\n",
      "Iteration 14, loss = 1.71294763\n",
      "Iteration 15, loss = 1.71224810\n",
      "Iteration 16, loss = 1.70995694\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.09238523\n",
      "Iteration 2, loss = 1.73790385\n",
      "Iteration 3, loss = 1.72795008\n",
      "Iteration 4, loss = 1.72101357\n",
      "Iteration 5, loss = 1.71773181\n",
      "Iteration 6, loss = 1.71586432\n",
      "Iteration 7, loss = 1.71620841\n",
      "Iteration 8, loss = 1.71365431\n",
      "Iteration 9, loss = 1.71608979\n",
      "Iteration 10, loss = 1.71262889\n",
      "Iteration 11, loss = 1.71232591\n",
      "Iteration 12, loss = 1.71248236\n",
      "Iteration 13, loss = 1.71162981\n",
      "Iteration 14, loss = 1.71196190\n",
      "Iteration 15, loss = 1.71123707\n",
      "Iteration 16, loss = 1.71313678\n",
      "Iteration 17, loss = 1.71279861\n",
      "Iteration 18, loss = 1.71039340\n",
      "Iteration 19, loss = 1.71278407\n",
      "Iteration 20, loss = 1.71011586\n",
      "Iteration 21, loss = 1.70938746\n",
      "Iteration 22, loss = 1.71171182\n",
      "Iteration 23, loss = 1.71043722\n",
      "Iteration 24, loss = 1.71229920\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.04647215\n",
      "Iteration 2, loss = 1.75714217\n",
      "Iteration 3, loss = 1.74231596\n",
      "Iteration 4, loss = 1.73706285\n",
      "Iteration 5, loss = 1.73084169\n",
      "Iteration 6, loss = 1.72903801\n",
      "Iteration 7, loss = 1.72747487\n",
      "Iteration 8, loss = 1.72575405\n",
      "Iteration 9, loss = 1.72594837\n",
      "Iteration 10, loss = 1.72491252\n",
      "Iteration 11, loss = 1.72279787\n",
      "Iteration 12, loss = 1.72353984\n",
      "Iteration 13, loss = 1.72382865\n",
      "Iteration 14, loss = 1.72271226\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.04556145\n",
      "Iteration 2, loss = 1.75547457\n",
      "Iteration 3, loss = 1.74260863\n",
      "Iteration 4, loss = 1.73711531\n",
      "Iteration 5, loss = 1.73231334\n",
      "Iteration 6, loss = 1.72946795\n",
      "Iteration 7, loss = 1.72885762\n",
      "Iteration 8, loss = 1.72583914\n",
      "Iteration 9, loss = 1.72570670\n",
      "Iteration 10, loss = 1.72502540\n",
      "Iteration 11, loss = 1.72344799\n",
      "Iteration 12, loss = 1.72354964\n",
      "Iteration 13, loss = 1.72219841\n",
      "Iteration 14, loss = 1.72250009\n",
      "Iteration 15, loss = 1.72178438\n",
      "Iteration 16, loss = 1.72240805\n",
      "Iteration 17, loss = 1.72130713\n",
      "Iteration 18, loss = 1.72388196\n",
      "Iteration 19, loss = 1.72117975\n",
      "Iteration 20, loss = 1.72060083\n",
      "Iteration 21, loss = 1.72052031\n",
      "Iteration 22, loss = 1.72109852\n",
      "Iteration 23, loss = 1.72176982\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.04532967\n",
      "Iteration 2, loss = 1.75620313\n",
      "Iteration 3, loss = 1.74124902\n",
      "Iteration 4, loss = 1.73573179\n",
      "Iteration 5, loss = 1.73176115\n",
      "Iteration 6, loss = 1.72948897\n",
      "Iteration 7, loss = 1.72621003\n",
      "Iteration 8, loss = 1.72489094\n",
      "Iteration 9, loss = 1.72428016\n",
      "Iteration 10, loss = 1.72457460\n",
      "Iteration 11, loss = 1.72299834\n",
      "Iteration 12, loss = 1.72288070\n",
      "Iteration 13, loss = 1.72193609\n",
      "Iteration 14, loss = 1.72227253\n",
      "Iteration 15, loss = 1.72213585\n",
      "Iteration 16, loss = 1.72248955\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.80227701\n",
      "Iteration 2, loss = 2.32477308\n",
      "Iteration 3, loss = 2.31199229\n",
      "Iteration 4, loss = 2.30910420\n",
      "Iteration 5, loss = 2.30896850\n",
      "Iteration 6, loss = 2.30871994\n",
      "Iteration 7, loss = 2.30954870\n",
      "Iteration 8, loss = 2.30804244\n",
      "Iteration 9, loss = 2.30826469\n",
      "Iteration 10, loss = 2.30833332\n",
      "Iteration 11, loss = 2.30801545\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.80055998\n",
      "Iteration 2, loss = 2.32294917\n",
      "Iteration 3, loss = 2.31132277\n",
      "Iteration 4, loss = 2.30942294\n",
      "Iteration 5, loss = 2.30919046\n",
      "Iteration 6, loss = 2.30941070\n",
      "Iteration 7, loss = 2.30881120\n",
      "Iteration 8, loss = 2.30869660\n",
      "Iteration 9, loss = 2.30834183\n",
      "Iteration 10, loss = 2.30819042\n",
      "Iteration 11, loss = 2.31015748\n",
      "Iteration 12, loss = 2.30807451\n",
      "Iteration 13, loss = 2.30785549\n",
      "Iteration 14, loss = 2.30827003\n",
      "Iteration 15, loss = 2.30815164\n",
      "Iteration 16, loss = 2.30871935\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.80030287\n",
      "Iteration 2, loss = 2.32333363\n",
      "Iteration 3, loss = 2.31225956\n",
      "Iteration 4, loss = 2.30871237\n",
      "Iteration 5, loss = 2.31092727\n",
      "Iteration 6, loss = 2.30943205\n",
      "Iteration 7, loss = 2.30899671\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.66246167\n",
      "Iteration 2, loss = 2.31998208\n",
      "Iteration 3, loss = 2.30789879\n",
      "Iteration 4, loss = 2.30650916\n",
      "Iteration 5, loss = 2.30579414\n",
      "Iteration 6, loss = 2.30654887\n",
      "Iteration 7, loss = 2.30609535\n",
      "Iteration 8, loss = 2.30626334\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.65923284\n",
      "Iteration 2, loss = 2.32034255\n",
      "Iteration 3, loss = 2.30857692\n",
      "Iteration 4, loss = 2.30645634\n",
      "Iteration 5, loss = 2.30555078\n",
      "Iteration 6, loss = 2.30658547\n",
      "Iteration 7, loss = 2.30579100\n",
      "Iteration 8, loss = 2.30628424\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.65860478\n",
      "Iteration 2, loss = 2.32040387\n",
      "Iteration 3, loss = 2.30857714\n",
      "Iteration 4, loss = 2.30688301\n",
      "Iteration 5, loss = 2.30577497\n",
      "Iteration 6, loss = 2.30627028\n",
      "Iteration 7, loss = 2.30617570\n",
      "Iteration 8, loss = 2.30609903\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.75618242\n",
      "Iteration 2, loss = 2.32008572\n",
      "Iteration 3, loss = 2.30807336\n",
      "Iteration 4, loss = 2.30622114\n",
      "Iteration 5, loss = 2.30686361\n",
      "Iteration 6, loss = 2.30662031\n",
      "Iteration 7, loss = 2.30554387\n",
      "Iteration 8, loss = 2.30606656\n",
      "Iteration 9, loss = 2.30615694\n",
      "Iteration 10, loss = 2.30565107\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.75949679\n",
      "Iteration 2, loss = 2.32119928\n",
      "Iteration 3, loss = 2.30845178\n",
      "Iteration 4, loss = 2.30652309\n",
      "Iteration 5, loss = 2.30690939\n",
      "Iteration 6, loss = 2.30691063\n",
      "Iteration 7, loss = 2.30631237\n",
      "Iteration 8, loss = 2.30602771\n",
      "Iteration 9, loss = 2.30576914\n",
      "Iteration 10, loss = 2.30568924\n",
      "Iteration 11, loss = 2.30535909\n",
      "Iteration 12, loss = 2.30594903\n",
      "Iteration 13, loss = 2.30577166\n",
      "Iteration 14, loss = 2.30603108\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.75795845\n",
      "Iteration 2, loss = 2.32137269\n",
      "Iteration 3, loss = 2.30914417\n",
      "Iteration 4, loss = 2.30656223\n",
      "Iteration 5, loss = 2.30630940\n",
      "Iteration 6, loss = 2.30688801\n",
      "Iteration 7, loss = 2.30527704\n",
      "Iteration 8, loss = 2.30575840\n",
      "Iteration 9, loss = 2.30604657\n",
      "Iteration 10, loss = 2.30687044\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59630866\n",
      "Iteration 2, loss = 0.43735191\n",
      "Iteration 3, loss = 0.40037042\n",
      "Iteration 4, loss = 0.38071149\n",
      "Iteration 5, loss = 0.35981763\n",
      "Iteration 6, loss = 0.34589289\n",
      "Iteration 7, loss = 0.33351417\n",
      "Iteration 8, loss = 0.32109419\n",
      "Iteration 9, loss = 0.31573839\n",
      "Iteration 10, loss = 0.30544790\n",
      "Iteration 11, loss = 0.29932931\n",
      "Iteration 12, loss = 0.29289592\n",
      "Iteration 13, loss = 0.28517418\n",
      "Iteration 14, loss = 0.27942183\n",
      "Iteration 15, loss = 0.27140133\n",
      "Iteration 16, loss = 0.26907416\n",
      "Iteration 17, loss = 0.25879937\n",
      "Iteration 18, loss = 0.25519517\n",
      "Iteration 19, loss = 0.25316851\n",
      "Iteration 20, loss = 0.25331771\n",
      "Iteration 21, loss = 0.24430171\n",
      "Iteration 22, loss = 0.23881084\n",
      "Iteration 23, loss = 0.23516568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, loss = 0.23136517\n",
      "Iteration 25, loss = 0.22719991\n",
      "Iteration 26, loss = 0.22334866\n",
      "Iteration 27, loss = 0.21862671\n",
      "Iteration 28, loss = 0.21898717\n",
      "Iteration 29, loss = 0.21423964\n",
      "Iteration 30, loss = 0.20979889\n",
      "Iteration 31, loss = 0.20647322\n",
      "Iteration 32, loss = 0.20676731\n",
      "Iteration 33, loss = 0.20063664\n",
      "Iteration 34, loss = 0.19694175\n",
      "Iteration 35, loss = 0.19444910\n",
      "Iteration 36, loss = 0.19295556\n",
      "Iteration 37, loss = 0.18978821\n",
      "Iteration 38, loss = 0.19126256\n",
      "Iteration 39, loss = 0.18619480\n",
      "Iteration 40, loss = 0.18284746\n",
      "Iteration 41, loss = 0.18397905\n",
      "Iteration 42, loss = 0.17732797\n",
      "Iteration 43, loss = 0.17720507\n",
      "Iteration 44, loss = 0.17477418\n",
      "Iteration 45, loss = 0.17347402\n",
      "Iteration 46, loss = 0.17441499\n",
      "Iteration 47, loss = 0.16827888\n",
      "Iteration 48, loss = 0.16834669\n",
      "Iteration 49, loss = 0.16508423\n",
      "Iteration 50, loss = 0.16139415\n",
      "Iteration 51, loss = 0.16115231\n",
      "Iteration 52, loss = 0.16040921\n",
      "Iteration 53, loss = 0.15631188\n",
      "Iteration 54, loss = 0.15466758\n",
      "Iteration 55, loss = 0.15280922\n",
      "Iteration 56, loss = 0.14962618\n",
      "Iteration 57, loss = 0.15088010\n",
      "Iteration 58, loss = 0.15115710\n",
      "Iteration 59, loss = 0.15393728\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59279798\n",
      "Iteration 2, loss = 0.43714859\n",
      "Iteration 3, loss = 0.40539653\n",
      "Iteration 4, loss = 0.38131753\n",
      "Iteration 5, loss = 0.36130621\n",
      "Iteration 6, loss = 0.35157081\n",
      "Iteration 7, loss = 0.34327040\n",
      "Iteration 8, loss = 0.32907033\n",
      "Iteration 9, loss = 0.31932179\n",
      "Iteration 10, loss = 0.31258095\n",
      "Iteration 11, loss = 0.30256899\n",
      "Iteration 12, loss = 0.29635195\n",
      "Iteration 13, loss = 0.28890608\n",
      "Iteration 14, loss = 0.28630250\n",
      "Iteration 15, loss = 0.27579311\n",
      "Iteration 16, loss = 0.27114542\n",
      "Iteration 17, loss = 0.26471986\n",
      "Iteration 18, loss = 0.25871605\n",
      "Iteration 19, loss = 0.25485704\n",
      "Iteration 20, loss = 0.25316492\n",
      "Iteration 21, loss = 0.24392380\n",
      "Iteration 22, loss = 0.23926641\n",
      "Iteration 23, loss = 0.23483890\n",
      "Iteration 24, loss = 0.22836633\n",
      "Iteration 25, loss = 0.22673118\n",
      "Iteration 26, loss = 0.22674119\n",
      "Iteration 27, loss = 0.22393528\n",
      "Iteration 28, loss = 0.21670894\n",
      "Iteration 29, loss = 0.21700846\n",
      "Iteration 30, loss = 0.21332877\n",
      "Iteration 31, loss = 0.20871664\n",
      "Iteration 32, loss = 0.20817863\n",
      "Iteration 33, loss = 0.20283739\n",
      "Iteration 34, loss = 0.19685265\n",
      "Iteration 35, loss = 0.19796685\n",
      "Iteration 36, loss = 0.19305078\n",
      "Iteration 37, loss = 0.18914749\n",
      "Iteration 38, loss = 0.18852445\n",
      "Iteration 39, loss = 0.18622336\n",
      "Iteration 40, loss = 0.18375053\n",
      "Iteration 41, loss = 0.18150811\n",
      "Iteration 42, loss = 0.17703727\n",
      "Iteration 43, loss = 0.18230720\n",
      "Iteration 44, loss = 0.17882110\n",
      "Iteration 45, loss = 0.17454582\n",
      "Iteration 46, loss = 0.16691184\n",
      "Iteration 47, loss = 0.16891063\n",
      "Iteration 48, loss = 0.16860312\n",
      "Iteration 49, loss = 0.16598001\n",
      "Iteration 50, loss = 0.16437238\n",
      "Iteration 51, loss = 0.16173622\n",
      "Iteration 52, loss = 0.16100906\n",
      "Iteration 53, loss = 0.15957990\n",
      "Iteration 54, loss = 0.15865240\n",
      "Iteration 55, loss = 0.15329338\n",
      "Iteration 56, loss = 0.14814016\n",
      "Iteration 57, loss = 0.14758252\n",
      "Iteration 58, loss = 0.15185554\n",
      "Iteration 59, loss = 0.14887363\n",
      "Iteration 60, loss = 0.14401947\n",
      "Iteration 61, loss = 0.14807397\n",
      "Iteration 62, loss = 0.14035887\n",
      "Iteration 63, loss = 0.14863243\n",
      "Iteration 64, loss = 0.14246511\n",
      "Iteration 65, loss = 0.13685365\n",
      "Iteration 66, loss = 0.13984708\n",
      "Iteration 67, loss = 0.13832961\n",
      "Iteration 68, loss = 0.13867247\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60070659\n",
      "Iteration 2, loss = 0.44562267\n",
      "Iteration 3, loss = 0.40487186\n",
      "Iteration 4, loss = 0.38429101\n",
      "Iteration 5, loss = 0.36566976\n",
      "Iteration 6, loss = 0.35601410\n",
      "Iteration 7, loss = 0.34168485\n",
      "Iteration 8, loss = 0.32831636\n",
      "Iteration 9, loss = 0.31745522\n",
      "Iteration 10, loss = 0.30999669\n",
      "Iteration 11, loss = 0.30076525\n",
      "Iteration 12, loss = 0.29798360\n",
      "Iteration 13, loss = 0.29028786\n",
      "Iteration 14, loss = 0.28558806\n",
      "Iteration 15, loss = 0.27601693\n",
      "Iteration 16, loss = 0.27117370\n",
      "Iteration 17, loss = 0.26545417\n",
      "Iteration 18, loss = 0.25696720\n",
      "Iteration 19, loss = 0.25342024\n",
      "Iteration 20, loss = 0.24963290\n",
      "Iteration 21, loss = 0.24679052\n",
      "Iteration 22, loss = 0.23897819\n",
      "Iteration 23, loss = 0.23887504\n",
      "Iteration 24, loss = 0.23114004\n",
      "Iteration 25, loss = 0.22997515\n",
      "Iteration 26, loss = 0.22735673\n",
      "Iteration 27, loss = 0.21999733\n",
      "Iteration 28, loss = 0.22023268\n",
      "Iteration 29, loss = 0.21255562\n",
      "Iteration 30, loss = 0.21017211\n",
      "Iteration 31, loss = 0.21193995\n",
      "Iteration 32, loss = 0.20726224\n",
      "Iteration 33, loss = 0.20243042\n",
      "Iteration 34, loss = 0.19730290\n",
      "Iteration 35, loss = 0.19646402\n",
      "Iteration 36, loss = 0.19467519\n",
      "Iteration 37, loss = 0.19094214\n",
      "Iteration 38, loss = 0.18680238\n",
      "Iteration 39, loss = 0.18773700\n",
      "Iteration 40, loss = 0.18212832\n",
      "Iteration 41, loss = 0.17919048\n",
      "Iteration 42, loss = 0.18214642\n",
      "Iteration 43, loss = 0.17815679\n",
      "Iteration 44, loss = 0.17695361\n",
      "Iteration 45, loss = 0.17246895\n",
      "Iteration 46, loss = 0.17212456\n",
      "Iteration 47, loss = 0.16532966\n",
      "Iteration 48, loss = 0.17048617\n",
      "Iteration 49, loss = 0.16606770\n",
      "Iteration 50, loss = 0.16210487\n",
      "Iteration 51, loss = 0.16312187\n",
      "Iteration 52, loss = 0.16165629\n",
      "Iteration 53, loss = 0.15697905\n",
      "Iteration 54, loss = 0.15585564\n",
      "Iteration 55, loss = 0.15221156\n",
      "Iteration 56, loss = 0.15487222\n",
      "Iteration 57, loss = 0.15053141\n",
      "Iteration 58, loss = 0.15509342\n",
      "Iteration 59, loss = 0.14795916\n",
      "Iteration 60, loss = 0.14372831\n",
      "Iteration 61, loss = 0.14295153\n",
      "Iteration 62, loss = 0.14578384\n",
      "Iteration 63, loss = 0.14538094\n",
      "Iteration 64, loss = 0.13989954\n",
      "Iteration 65, loss = 0.13743463\n",
      "Iteration 66, loss = 0.13670077\n",
      "Iteration 67, loss = 0.13862671\n",
      "Iteration 68, loss = 0.13436829\n",
      "Iteration 69, loss = 0.13820665\n",
      "Iteration 70, loss = 0.13208422\n",
      "Iteration 71, loss = 0.13094310\n",
      "Iteration 72, loss = 0.13559773\n",
      "Iteration 73, loss = 0.13131150\n",
      "Iteration 74, loss = 0.12680086\n",
      "Iteration 75, loss = 0.12649933\n",
      "Iteration 76, loss = 0.12655644\n",
      "Iteration 77, loss = 0.12835049\n",
      "Iteration 78, loss = 0.12501671\n",
      "Iteration 79, loss = 0.12149225\n",
      "Iteration 80, loss = 0.13149620\n",
      "Iteration 81, loss = 0.12283861\n",
      "Iteration 82, loss = 0.12388916\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62091168\n",
      "Iteration 2, loss = 0.44402909\n",
      "Iteration 3, loss = 0.40271186\n",
      "Iteration 4, loss = 0.37776270\n",
      "Iteration 5, loss = 0.35905423\n",
      "Iteration 6, loss = 0.34590951\n",
      "Iteration 7, loss = 0.33691795\n",
      "Iteration 8, loss = 0.32323829\n",
      "Iteration 9, loss = 0.31322389\n",
      "Iteration 10, loss = 0.30619791\n",
      "Iteration 11, loss = 0.30109115\n",
      "Iteration 12, loss = 0.29447062\n",
      "Iteration 13, loss = 0.28410463\n",
      "Iteration 14, loss = 0.28112136\n",
      "Iteration 15, loss = 0.27225535\n",
      "Iteration 16, loss = 0.26741702\n",
      "Iteration 17, loss = 0.26219200\n",
      "Iteration 18, loss = 0.25803814\n",
      "Iteration 19, loss = 0.25515284\n",
      "Iteration 20, loss = 0.24741591\n",
      "Iteration 21, loss = 0.24531860\n",
      "Iteration 22, loss = 0.24239986\n",
      "Iteration 23, loss = 0.23384360\n",
      "Iteration 24, loss = 0.23433309\n",
      "Iteration 25, loss = 0.23109005\n",
      "Iteration 26, loss = 0.22570522\n",
      "Iteration 27, loss = 0.22279865\n",
      "Iteration 28, loss = 0.22239335\n",
      "Iteration 29, loss = 0.21566534\n",
      "Iteration 30, loss = 0.21623369\n",
      "Iteration 31, loss = 0.21154170\n",
      "Iteration 32, loss = 0.20938794\n",
      "Iteration 33, loss = 0.20300763\n",
      "Iteration 34, loss = 0.20176846\n",
      "Iteration 35, loss = 0.19916492\n",
      "Iteration 36, loss = 0.19926680\n",
      "Iteration 37, loss = 0.19350613\n",
      "Iteration 38, loss = 0.18834262\n",
      "Iteration 39, loss = 0.18944239\n",
      "Iteration 40, loss = 0.18619576\n",
      "Iteration 41, loss = 0.18605962\n",
      "Iteration 42, loss = 0.18461594\n",
      "Iteration 43, loss = 0.18005340\n",
      "Iteration 44, loss = 0.18020509\n",
      "Iteration 45, loss = 0.17653343\n",
      "Iteration 46, loss = 0.17343483\n",
      "Iteration 47, loss = 0.17407886\n",
      "Iteration 48, loss = 0.17333236\n",
      "Iteration 49, loss = 0.16797935\n",
      "Iteration 50, loss = 0.16983426\n",
      "Iteration 51, loss = 0.16494286\n",
      "Iteration 52, loss = 0.16260161\n",
      "Iteration 53, loss = 0.15889387\n",
      "Iteration 54, loss = 0.15861378\n",
      "Iteration 55, loss = 0.16231976\n",
      "Iteration 56, loss = 0.16393890\n",
      "Iteration 57, loss = 0.15149195\n",
      "Iteration 58, loss = 0.15377529\n",
      "Iteration 59, loss = 0.15420988\n",
      "Iteration 60, loss = 0.15207294\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.61906524\n",
      "Iteration 2, loss = 0.44449328\n",
      "Iteration 3, loss = 0.40516705\n",
      "Iteration 4, loss = 0.38156673\n",
      "Iteration 5, loss = 0.36871750\n",
      "Iteration 6, loss = 0.35069125\n",
      "Iteration 7, loss = 0.33908793\n",
      "Iteration 8, loss = 0.32869585\n",
      "Iteration 9, loss = 0.31972336\n",
      "Iteration 10, loss = 0.31291702\n",
      "Iteration 11, loss = 0.30425928\n",
      "Iteration 12, loss = 0.29661484\n",
      "Iteration 13, loss = 0.29076195\n",
      "Iteration 14, loss = 0.28228102\n",
      "Iteration 15, loss = 0.27741399\n",
      "Iteration 16, loss = 0.27004910\n",
      "Iteration 17, loss = 0.26512229\n",
      "Iteration 18, loss = 0.25851850\n",
      "Iteration 19, loss = 0.25486373\n",
      "Iteration 20, loss = 0.25228460\n",
      "Iteration 21, loss = 0.24543326\n",
      "Iteration 22, loss = 0.24348908\n",
      "Iteration 23, loss = 0.23903754\n",
      "Iteration 24, loss = 0.23423626\n",
      "Iteration 25, loss = 0.22900846\n",
      "Iteration 26, loss = 0.22749274\n",
      "Iteration 27, loss = 0.22037884\n",
      "Iteration 28, loss = 0.22008704\n",
      "Iteration 29, loss = 0.21871069\n",
      "Iteration 30, loss = 0.21244749\n",
      "Iteration 31, loss = 0.21264676\n",
      "Iteration 32, loss = 0.20817230\n",
      "Iteration 33, loss = 0.20263659\n",
      "Iteration 34, loss = 0.20289673\n",
      "Iteration 35, loss = 0.19667086\n",
      "Iteration 36, loss = 0.19663043\n",
      "Iteration 37, loss = 0.19241051\n",
      "Iteration 38, loss = 0.19018332\n",
      "Iteration 39, loss = 0.18716292\n",
      "Iteration 40, loss = 0.18703589\n",
      "Iteration 41, loss = 0.18419199\n",
      "Iteration 42, loss = 0.18269359\n",
      "Iteration 43, loss = 0.18176994\n",
      "Iteration 44, loss = 0.17523245\n",
      "Iteration 45, loss = 0.17447653\n",
      "Iteration 46, loss = 0.17292721\n",
      "Iteration 47, loss = 0.17200369\n",
      "Iteration 48, loss = 0.16916455\n",
      "Iteration 49, loss = 0.16654820\n",
      "Iteration 50, loss = 0.16834892\n",
      "Iteration 51, loss = 0.16356556\n",
      "Iteration 52, loss = 0.16131007\n",
      "Iteration 53, loss = 0.16169997\n",
      "Iteration 54, loss = 0.15838007\n",
      "Iteration 55, loss = 0.15521693\n",
      "Iteration 56, loss = 0.15616925\n",
      "Iteration 57, loss = 0.15441868\n",
      "Iteration 58, loss = 0.15190919\n",
      "Iteration 59, loss = 0.14824701\n",
      "Iteration 60, loss = 0.14923508\n",
      "Iteration 61, loss = 0.14609620\n",
      "Iteration 62, loss = 0.14394480\n",
      "Iteration 63, loss = 0.14586018\n",
      "Iteration 64, loss = 0.14440171\n",
      "Iteration 65, loss = 0.14174670\n",
      "Iteration 66, loss = 0.14412181\n",
      "Iteration 67, loss = 0.14064950\n",
      "Iteration 68, loss = 0.13757616\n",
      "Iteration 69, loss = 0.13388378\n",
      "Iteration 70, loss = 0.13719655\n",
      "Iteration 71, loss = 0.13682900\n",
      "Iteration 72, loss = 0.13184549\n",
      "Iteration 73, loss = 0.13427153\n",
      "Iteration 74, loss = 0.12877953\n",
      "Iteration 75, loss = 0.12850219\n",
      "Iteration 76, loss = 0.13071192\n",
      "Iteration 77, loss = 0.12927835\n",
      "Iteration 78, loss = 0.12950642\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61405814\n",
      "Iteration 2, loss = 0.44242217\n",
      "Iteration 3, loss = 0.40584049\n",
      "Iteration 4, loss = 0.38514815\n",
      "Iteration 5, loss = 0.36520083\n",
      "Iteration 6, loss = 0.35305976\n",
      "Iteration 7, loss = 0.33810305\n",
      "Iteration 8, loss = 0.32949284\n",
      "Iteration 9, loss = 0.32068573\n",
      "Iteration 10, loss = 0.31246565\n",
      "Iteration 11, loss = 0.30325333\n",
      "Iteration 12, loss = 0.29788449\n",
      "Iteration 13, loss = 0.28836574\n",
      "Iteration 14, loss = 0.28404580\n",
      "Iteration 15, loss = 0.27719282\n",
      "Iteration 16, loss = 0.27086136\n",
      "Iteration 17, loss = 0.26681271\n",
      "Iteration 18, loss = 0.25781042\n",
      "Iteration 19, loss = 0.25562461\n",
      "Iteration 20, loss = 0.25269658\n",
      "Iteration 21, loss = 0.24939641\n",
      "Iteration 22, loss = 0.24277216\n",
      "Iteration 23, loss = 0.24004659\n",
      "Iteration 24, loss = 0.23407556\n",
      "Iteration 25, loss = 0.23251510\n",
      "Iteration 26, loss = 0.22842076\n",
      "Iteration 27, loss = 0.22330812\n",
      "Iteration 28, loss = 0.21982321\n",
      "Iteration 29, loss = 0.21804058\n",
      "Iteration 30, loss = 0.21479649\n",
      "Iteration 31, loss = 0.21301438\n",
      "Iteration 32, loss = 0.20722202\n",
      "Iteration 33, loss = 0.20658670\n",
      "Iteration 34, loss = 0.20304414\n",
      "Iteration 35, loss = 0.20117280\n",
      "Iteration 36, loss = 0.19716654\n",
      "Iteration 37, loss = 0.19206864\n",
      "Iteration 38, loss = 0.19112234\n",
      "Iteration 39, loss = 0.18744763\n",
      "Iteration 40, loss = 0.19090908\n",
      "Iteration 41, loss = 0.18434210\n",
      "Iteration 42, loss = 0.18055298\n",
      "Iteration 43, loss = 0.17947427\n",
      "Iteration 44, loss = 0.17913709\n",
      "Iteration 45, loss = 0.17649918\n",
      "Iteration 46, loss = 0.17342044\n",
      "Iteration 47, loss = 0.17290732\n",
      "Iteration 48, loss = 0.16911551\n",
      "Iteration 49, loss = 0.16749616\n",
      "Iteration 50, loss = 0.16387159\n",
      "Iteration 51, loss = 0.16446775\n",
      "Iteration 52, loss = 0.16418350\n",
      "Iteration 53, loss = 0.16096465\n",
      "Iteration 54, loss = 0.15726880\n",
      "Iteration 55, loss = 0.15707995\n",
      "Iteration 56, loss = 0.15635812\n",
      "Iteration 57, loss = 0.15536114\n",
      "Iteration 58, loss = 0.15029808\n",
      "Iteration 59, loss = 0.14653642\n",
      "Iteration 60, loss = 0.14655745\n",
      "Iteration 61, loss = 0.14770314\n",
      "Iteration 62, loss = 0.14500902\n",
      "Iteration 63, loss = 0.14471973\n",
      "Iteration 64, loss = 0.14762163\n",
      "Iteration 65, loss = 0.14264211\n",
      "Iteration 66, loss = 0.14154649\n",
      "Iteration 67, loss = 0.13784338\n",
      "Iteration 68, loss = 0.13726903\n",
      "Iteration 69, loss = 0.13798990\n",
      "Iteration 70, loss = 0.13252481\n",
      "Iteration 71, loss = 0.13271230\n",
      "Iteration 72, loss = 0.13680346\n",
      "Iteration 73, loss = 0.13149903\n",
      "Iteration 74, loss = 0.13133344\n",
      "Iteration 75, loss = 0.13259344\n",
      "Iteration 76, loss = 0.12970015\n",
      "Iteration 77, loss = 0.12889924\n",
      "Iteration 78, loss = 0.12783640\n",
      "Iteration 79, loss = 0.12449842\n",
      "Iteration 80, loss = 0.12450163\n",
      "Iteration 81, loss = 0.12388593\n",
      "Iteration 82, loss = 0.12455650\n",
      "Iteration 83, loss = 0.12246628\n",
      "Iteration 84, loss = 0.11984837\n",
      "Iteration 85, loss = 0.12057076\n",
      "Iteration 86, loss = 0.12260372\n",
      "Iteration 87, loss = 0.11748843\n",
      "Iteration 88, loss = 0.12045602\n",
      "Iteration 89, loss = 0.11446706\n",
      "Iteration 90, loss = 0.11812739\n",
      "Iteration 91, loss = 0.11885248\n",
      "Iteration 92, loss = 0.11575092\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56953665\n",
      "Iteration 2, loss = 0.42333478\n",
      "Iteration 3, loss = 0.38546131\n",
      "Iteration 4, loss = 0.36161255\n",
      "Iteration 5, loss = 0.34540455\n",
      "Iteration 6, loss = 0.32961303\n",
      "Iteration 7, loss = 0.31646631\n",
      "Iteration 8, loss = 0.30426079\n",
      "Iteration 9, loss = 0.29161125\n",
      "Iteration 10, loss = 0.28343545\n",
      "Iteration 11, loss = 0.27645723\n",
      "Iteration 12, loss = 0.26803243\n",
      "Iteration 13, loss = 0.26018741\n",
      "Iteration 14, loss = 0.25388792\n",
      "Iteration 15, loss = 0.24891669\n",
      "Iteration 16, loss = 0.24289515\n",
      "Iteration 17, loss = 0.23818926\n",
      "Iteration 18, loss = 0.22810325\n",
      "Iteration 19, loss = 0.22700432\n",
      "Iteration 20, loss = 0.22110966\n",
      "Iteration 21, loss = 0.21371453\n",
      "Iteration 22, loss = 0.20832185\n",
      "Iteration 23, loss = 0.20272217\n",
      "Iteration 24, loss = 0.19728528\n",
      "Iteration 25, loss = 0.19484121\n",
      "Iteration 26, loss = 0.19240048\n",
      "Iteration 27, loss = 0.18331355\n",
      "Iteration 28, loss = 0.18718972\n",
      "Iteration 29, loss = 0.17942664\n",
      "Iteration 30, loss = 0.17461566\n",
      "Iteration 31, loss = 0.16552865\n",
      "Iteration 32, loss = 0.17087693\n",
      "Iteration 33, loss = 0.16429193\n",
      "Iteration 34, loss = 0.16037613\n",
      "Iteration 35, loss = 0.15925732\n",
      "Iteration 36, loss = 0.15552071\n",
      "Iteration 37, loss = 0.14983864\n",
      "Iteration 38, loss = 0.14483977\n",
      "Iteration 39, loss = 0.14890235\n",
      "Iteration 40, loss = 0.14370569\n",
      "Iteration 41, loss = 0.13772706\n",
      "Iteration 42, loss = 0.14219154\n",
      "Iteration 43, loss = 0.13624510\n",
      "Iteration 44, loss = 0.12924259\n",
      "Iteration 45, loss = 0.13150397\n",
      "Iteration 46, loss = 0.12097499\n",
      "Iteration 47, loss = 0.13075223\n",
      "Iteration 48, loss = 0.12476351\n",
      "Iteration 49, loss = 0.13109074\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57743351\n",
      "Iteration 2, loss = 0.43138269\n",
      "Iteration 3, loss = 0.38890003\n",
      "Iteration 4, loss = 0.36815815\n",
      "Iteration 5, loss = 0.34761707\n",
      "Iteration 6, loss = 0.33170166\n",
      "Iteration 7, loss = 0.32072743\n",
      "Iteration 8, loss = 0.30947487\n",
      "Iteration 9, loss = 0.30135631\n",
      "Iteration 10, loss = 0.28895153\n",
      "Iteration 11, loss = 0.27613193\n",
      "Iteration 12, loss = 0.26948509\n",
      "Iteration 13, loss = 0.26592567\n",
      "Iteration 14, loss = 0.25947961\n",
      "Iteration 15, loss = 0.24943409\n",
      "Iteration 16, loss = 0.24508362\n",
      "Iteration 17, loss = 0.23439718\n",
      "Iteration 18, loss = 0.22790828\n",
      "Iteration 19, loss = 0.23050359\n",
      "Iteration 20, loss = 0.21540354\n",
      "Iteration 21, loss = 0.21436907\n",
      "Iteration 22, loss = 0.20982297\n",
      "Iteration 23, loss = 0.20258722\n",
      "Iteration 24, loss = 0.20099649\n",
      "Iteration 25, loss = 0.19696539\n",
      "Iteration 26, loss = 0.19046547\n",
      "Iteration 27, loss = 0.18747426\n",
      "Iteration 28, loss = 0.19166559\n",
      "Iteration 29, loss = 0.18305134\n",
      "Iteration 30, loss = 0.17828192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31, loss = 0.16975640\n",
      "Iteration 32, loss = 0.17244117\n",
      "Iteration 33, loss = 0.17184218\n",
      "Iteration 34, loss = 0.15946765\n",
      "Iteration 35, loss = 0.15944541\n",
      "Iteration 36, loss = 0.15505628\n",
      "Iteration 37, loss = 0.15007656\n",
      "Iteration 38, loss = 0.15600993\n",
      "Iteration 39, loss = 0.14826514\n",
      "Iteration 40, loss = 0.14434512\n",
      "Iteration 41, loss = 0.14153136\n",
      "Iteration 42, loss = 0.14612022\n",
      "Iteration 43, loss = 0.13671133\n",
      "Iteration 44, loss = 0.13398876\n",
      "Iteration 45, loss = 0.13551212\n",
      "Iteration 46, loss = 0.12748087\n",
      "Iteration 47, loss = 0.13291101\n",
      "Iteration 48, loss = 0.12821511\n",
      "Iteration 49, loss = 0.12626447\n",
      "Iteration 50, loss = 0.12702085\n",
      "Iteration 51, loss = 0.12493124\n",
      "Iteration 52, loss = 0.12173661\n",
      "Iteration 53, loss = 0.12120961\n",
      "Iteration 54, loss = 0.11676695\n",
      "Iteration 55, loss = 0.11973629\n",
      "Iteration 56, loss = 0.11413019\n",
      "Iteration 57, loss = 0.12032105\n",
      "Iteration 58, loss = 0.11124928\n",
      "Iteration 59, loss = 0.10892573\n",
      "Iteration 60, loss = 0.11673357\n",
      "Iteration 61, loss = 0.10704240\n",
      "Iteration 62, loss = 0.10914207\n",
      "Iteration 63, loss = 0.11920842\n",
      "Iteration 64, loss = 0.09791804\n",
      "Iteration 65, loss = 0.10398077\n",
      "Iteration 66, loss = 0.11426398\n",
      "Iteration 67, loss = 0.10538870\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56520065\n",
      "Iteration 2, loss = 0.43579170\n",
      "Iteration 3, loss = 0.38792905\n",
      "Iteration 4, loss = 0.36949345\n",
      "Iteration 5, loss = 0.34863848\n",
      "Iteration 6, loss = 0.33185359\n",
      "Iteration 7, loss = 0.31850943\n",
      "Iteration 8, loss = 0.31018744\n",
      "Iteration 9, loss = 0.29660871\n",
      "Iteration 10, loss = 0.28416027\n",
      "Iteration 11, loss = 0.27818835\n",
      "Iteration 12, loss = 0.26940368\n",
      "Iteration 13, loss = 0.26703916\n",
      "Iteration 14, loss = 0.25882761\n",
      "Iteration 15, loss = 0.24683217\n",
      "Iteration 16, loss = 0.24220322\n",
      "Iteration 17, loss = 0.23338068\n",
      "Iteration 18, loss = 0.22712556\n",
      "Iteration 19, loss = 0.22565217\n",
      "Iteration 20, loss = 0.21451533\n",
      "Iteration 21, loss = 0.21787270\n",
      "Iteration 22, loss = 0.21292154\n",
      "Iteration 23, loss = 0.20407294\n",
      "Iteration 24, loss = 0.19526401\n",
      "Iteration 25, loss = 0.19692581\n",
      "Iteration 26, loss = 0.18709594\n",
      "Iteration 27, loss = 0.18595154\n",
      "Iteration 28, loss = 0.17842383\n",
      "Iteration 29, loss = 0.18352514\n",
      "Iteration 30, loss = 0.17528027\n",
      "Iteration 31, loss = 0.17131791\n",
      "Iteration 32, loss = 0.17465708\n",
      "Iteration 33, loss = 0.16065639\n",
      "Iteration 34, loss = 0.15876317\n",
      "Iteration 35, loss = 0.15498953\n",
      "Iteration 36, loss = 0.14845597\n",
      "Iteration 37, loss = 0.14854572\n",
      "Iteration 38, loss = 0.15209072\n",
      "Iteration 39, loss = 0.14862537\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60013125\n",
      "Iteration 2, loss = 0.41845703\n",
      "Iteration 3, loss = 0.38421271\n",
      "Iteration 4, loss = 0.36207716\n",
      "Iteration 5, loss = 0.34501304\n",
      "Iteration 6, loss = 0.33224730\n",
      "Iteration 7, loss = 0.31341906\n",
      "Iteration 8, loss = 0.30599451\n",
      "Iteration 9, loss = 0.29279813\n",
      "Iteration 10, loss = 0.28718452\n",
      "Iteration 11, loss = 0.27999200\n",
      "Iteration 12, loss = 0.27151778\n",
      "Iteration 13, loss = 0.26303902\n",
      "Iteration 14, loss = 0.25968453\n",
      "Iteration 15, loss = 0.24709958\n",
      "Iteration 16, loss = 0.24622352\n",
      "Iteration 17, loss = 0.23569800\n",
      "Iteration 18, loss = 0.23322105\n",
      "Iteration 19, loss = 0.23245092\n",
      "Iteration 20, loss = 0.22728815\n",
      "Iteration 21, loss = 0.21780113\n",
      "Iteration 22, loss = 0.21141908\n",
      "Iteration 23, loss = 0.20891765\n",
      "Iteration 24, loss = 0.20596833\n",
      "Iteration 25, loss = 0.19931963\n",
      "Iteration 26, loss = 0.19653748\n",
      "Iteration 27, loss = 0.19272114\n",
      "Iteration 28, loss = 0.18862804\n",
      "Iteration 29, loss = 0.18175513\n",
      "Iteration 30, loss = 0.18305018\n",
      "Iteration 31, loss = 0.17404093\n",
      "Iteration 32, loss = 0.17462341\n",
      "Iteration 33, loss = 0.17107173\n",
      "Iteration 34, loss = 0.16722472\n",
      "Iteration 35, loss = 0.16445802\n",
      "Iteration 36, loss = 0.16034070\n",
      "Iteration 37, loss = 0.15766591\n",
      "Iteration 38, loss = 0.15451566\n",
      "Iteration 39, loss = 0.15147070\n",
      "Iteration 40, loss = 0.14630349\n",
      "Iteration 41, loss = 0.14909806\n",
      "Iteration 42, loss = 0.14142507\n",
      "Iteration 43, loss = 0.13813164\n",
      "Iteration 44, loss = 0.13820371\n",
      "Iteration 45, loss = 0.13700053\n",
      "Iteration 46, loss = 0.13112189\n",
      "Iteration 47, loss = 0.12975563\n",
      "Iteration 48, loss = 0.13193680\n",
      "Iteration 49, loss = 0.13114158\n",
      "Iteration 50, loss = 0.12892767\n",
      "Iteration 51, loss = 0.12899808\n",
      "Iteration 52, loss = 0.11942291\n",
      "Iteration 53, loss = 0.12108359\n",
      "Iteration 54, loss = 0.11712581\n",
      "Iteration 55, loss = 0.11559803\n",
      "Iteration 56, loss = 0.11929493\n",
      "Iteration 57, loss = 0.10970081\n",
      "Iteration 58, loss = 0.10803055\n",
      "Iteration 59, loss = 0.11170948\n",
      "Iteration 60, loss = 0.11175984\n",
      "Iteration 61, loss = 0.10505590\n",
      "Iteration 62, loss = 0.10929970\n",
      "Iteration 63, loss = 0.10878419\n",
      "Iteration 64, loss = 0.10467197\n",
      "Iteration 65, loss = 0.10357802\n",
      "Iteration 66, loss = 0.09778867\n",
      "Iteration 67, loss = 0.10026385\n",
      "Iteration 68, loss = 0.09968575\n",
      "Iteration 69, loss = 0.09881298\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59813950\n",
      "Iteration 2, loss = 0.42554565\n",
      "Iteration 3, loss = 0.38747806\n",
      "Iteration 4, loss = 0.36488909\n",
      "Iteration 5, loss = 0.34632672\n",
      "Iteration 6, loss = 0.33264649\n",
      "Iteration 7, loss = 0.32272081\n",
      "Iteration 8, loss = 0.31092389\n",
      "Iteration 9, loss = 0.29340412\n",
      "Iteration 10, loss = 0.28690791\n",
      "Iteration 11, loss = 0.28329554\n",
      "Iteration 12, loss = 0.27330535\n",
      "Iteration 13, loss = 0.26671004\n",
      "Iteration 14, loss = 0.25944813\n",
      "Iteration 15, loss = 0.25148849\n",
      "Iteration 16, loss = 0.24563543\n",
      "Iteration 17, loss = 0.23847481\n",
      "Iteration 18, loss = 0.23811296\n",
      "Iteration 19, loss = 0.23558954\n",
      "Iteration 20, loss = 0.22555023\n",
      "Iteration 21, loss = 0.22224580\n",
      "Iteration 22, loss = 0.21666562\n",
      "Iteration 23, loss = 0.21120756\n",
      "Iteration 24, loss = 0.21110353\n",
      "Iteration 25, loss = 0.20416891\n",
      "Iteration 26, loss = 0.19761611\n",
      "Iteration 27, loss = 0.19446799\n",
      "Iteration 28, loss = 0.19288713\n",
      "Iteration 29, loss = 0.18496164\n",
      "Iteration 30, loss = 0.18428081\n",
      "Iteration 31, loss = 0.18466472\n",
      "Iteration 32, loss = 0.17188616\n",
      "Iteration 33, loss = 0.17211291\n",
      "Iteration 34, loss = 0.16937248\n",
      "Iteration 35, loss = 0.16702276\n",
      "Iteration 36, loss = 0.16875795\n",
      "Iteration 37, loss = 0.15849061\n",
      "Iteration 38, loss = 0.15650978\n",
      "Iteration 39, loss = 0.14933632\n",
      "Iteration 40, loss = 0.14967383\n",
      "Iteration 41, loss = 0.15205858\n",
      "Iteration 42, loss = 0.14819718\n",
      "Iteration 43, loss = 0.14047721\n",
      "Iteration 44, loss = 0.14391153\n",
      "Iteration 45, loss = 0.13998349\n",
      "Iteration 46, loss = 0.14490073\n",
      "Iteration 47, loss = 0.13307421\n",
      "Iteration 48, loss = 0.13095504\n",
      "Iteration 49, loss = 0.12980429\n",
      "Iteration 50, loss = 0.13155984\n",
      "Iteration 51, loss = 0.13019015\n",
      "Iteration 52, loss = 0.12292335\n",
      "Iteration 53, loss = 0.12557628\n",
      "Iteration 54, loss = 0.12440773\n",
      "Iteration 55, loss = 0.11615907\n",
      "Iteration 56, loss = 0.11998924\n",
      "Iteration 57, loss = 0.11936643\n",
      "Iteration 58, loss = 0.11709021\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60028125\n",
      "Iteration 2, loss = 0.42658200\n",
      "Iteration 3, loss = 0.38885141\n",
      "Iteration 4, loss = 0.36876360\n",
      "Iteration 5, loss = 0.35112298\n",
      "Iteration 6, loss = 0.33579230\n",
      "Iteration 7, loss = 0.32444546\n",
      "Iteration 8, loss = 0.30963356\n",
      "Iteration 9, loss = 0.29762345\n",
      "Iteration 10, loss = 0.29109486\n",
      "Iteration 11, loss = 0.27864902\n",
      "Iteration 12, loss = 0.27631334\n",
      "Iteration 13, loss = 0.26738363\n",
      "Iteration 14, loss = 0.25977421\n",
      "Iteration 15, loss = 0.25687490\n",
      "Iteration 16, loss = 0.24642604\n",
      "Iteration 17, loss = 0.23861757\n",
      "Iteration 18, loss = 0.23327624\n",
      "Iteration 19, loss = 0.22763241\n",
      "Iteration 20, loss = 0.22485565\n",
      "Iteration 21, loss = 0.21960902\n",
      "Iteration 22, loss = 0.21613630\n",
      "Iteration 23, loss = 0.20842757\n",
      "Iteration 24, loss = 0.20713477\n",
      "Iteration 25, loss = 0.20154841\n",
      "Iteration 26, loss = 0.19456760\n",
      "Iteration 27, loss = 0.18934456\n",
      "Iteration 28, loss = 0.18774729\n",
      "Iteration 29, loss = 0.18799315\n",
      "Iteration 30, loss = 0.18121665\n",
      "Iteration 31, loss = 0.18251595\n",
      "Iteration 32, loss = 0.17336362\n",
      "Iteration 33, loss = 0.16775578\n",
      "Iteration 34, loss = 0.16746174\n",
      "Iteration 35, loss = 0.16721562\n",
      "Iteration 36, loss = 0.15906645\n",
      "Iteration 37, loss = 0.16102041\n",
      "Iteration 38, loss = 0.15245039\n",
      "Iteration 39, loss = 0.15142049\n",
      "Iteration 40, loss = 0.14979534\n",
      "Iteration 41, loss = 0.14477277\n",
      "Iteration 42, loss = 0.14072887\n",
      "Iteration 43, loss = 0.13933067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 44, loss = 0.13680300\n",
      "Iteration 45, loss = 0.13761700\n",
      "Iteration 46, loss = 0.13540653\n",
      "Iteration 47, loss = 0.13383472\n",
      "Iteration 48, loss = 0.12920742\n",
      "Iteration 49, loss = 0.12465117\n",
      "Iteration 50, loss = 0.12456937\n",
      "Iteration 51, loss = 0.12132024\n",
      "Iteration 52, loss = 0.12758067\n",
      "Iteration 53, loss = 0.12254756\n",
      "Iteration 54, loss = 0.11413227\n",
      "Iteration 55, loss = 0.12242355\n",
      "Iteration 56, loss = 0.11064975\n",
      "Iteration 57, loss = 0.11585474\n",
      "Iteration 58, loss = 0.11607849\n",
      "Iteration 59, loss = 0.11520778\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56670249\n",
      "Iteration 2, loss = 0.41902834\n",
      "Iteration 3, loss = 0.38059353\n",
      "Iteration 4, loss = 0.36126557\n",
      "Iteration 5, loss = 0.33929912\n",
      "Iteration 6, loss = 0.32251648\n",
      "Iteration 7, loss = 0.31124775\n",
      "Iteration 8, loss = 0.30171908\n",
      "Iteration 9, loss = 0.29446280\n",
      "Iteration 10, loss = 0.28148126\n",
      "Iteration 11, loss = 0.27498660\n",
      "Iteration 12, loss = 0.26410683\n",
      "Iteration 13, loss = 0.25764440\n",
      "Iteration 14, loss = 0.25167379\n",
      "Iteration 15, loss = 0.24636466\n",
      "Iteration 16, loss = 0.24239809\n",
      "Iteration 17, loss = 0.23483664\n",
      "Iteration 18, loss = 0.23089707\n",
      "Iteration 19, loss = 0.22466361\n",
      "Iteration 20, loss = 0.22099814\n",
      "Iteration 21, loss = 0.21564146\n",
      "Iteration 22, loss = 0.21019402\n",
      "Iteration 23, loss = 0.20724829\n",
      "Iteration 24, loss = 0.20021636\n",
      "Iteration 25, loss = 0.20235592\n",
      "Iteration 26, loss = 0.19321468\n",
      "Iteration 27, loss = 0.18759098\n",
      "Iteration 28, loss = 0.17982212\n",
      "Iteration 29, loss = 0.17929318\n",
      "Iteration 30, loss = 0.18046449\n",
      "Iteration 31, loss = 0.17547590\n",
      "Iteration 32, loss = 0.17385525\n",
      "Iteration 33, loss = 0.16568263\n",
      "Iteration 34, loss = 0.16902486\n",
      "Iteration 35, loss = 0.16111388\n",
      "Iteration 36, loss = 0.15477294\n",
      "Iteration 37, loss = 0.15980520\n",
      "Iteration 38, loss = 0.14951575\n",
      "Iteration 39, loss = 0.14782148\n",
      "Iteration 40, loss = 0.14540132\n",
      "Iteration 41, loss = 0.14557242\n",
      "Iteration 42, loss = 0.14620400\n",
      "Iteration 43, loss = 0.14463508\n",
      "Iteration 44, loss = 0.14168833\n",
      "Iteration 45, loss = 0.13601784\n",
      "Iteration 46, loss = 0.13305449\n",
      "Iteration 47, loss = 0.13636144\n",
      "Iteration 48, loss = 0.13107788\n",
      "Iteration 49, loss = 0.13151846\n",
      "Iteration 50, loss = 0.12585634\n",
      "Iteration 51, loss = 0.12750882\n",
      "Iteration 52, loss = 0.12384821\n",
      "Iteration 53, loss = 0.12006049\n",
      "Iteration 54, loss = 0.12355645\n",
      "Iteration 55, loss = 0.12052130\n",
      "Iteration 56, loss = 0.12385434\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58497897\n",
      "Iteration 2, loss = 0.42745509\n",
      "Iteration 3, loss = 0.38637611\n",
      "Iteration 4, loss = 0.36488791\n",
      "Iteration 5, loss = 0.34743634\n",
      "Iteration 6, loss = 0.33469545\n",
      "Iteration 7, loss = 0.31400570\n",
      "Iteration 8, loss = 0.30691494\n",
      "Iteration 9, loss = 0.29322903\n",
      "Iteration 10, loss = 0.28750521\n",
      "Iteration 11, loss = 0.27611072\n",
      "Iteration 12, loss = 0.26787445\n",
      "Iteration 13, loss = 0.26164520\n",
      "Iteration 14, loss = 0.25407103\n",
      "Iteration 15, loss = 0.24658823\n",
      "Iteration 16, loss = 0.23831311\n",
      "Iteration 17, loss = 0.23768365\n",
      "Iteration 18, loss = 0.22956175\n",
      "Iteration 19, loss = 0.22635266\n",
      "Iteration 20, loss = 0.21809437\n",
      "Iteration 21, loss = 0.21582587\n",
      "Iteration 22, loss = 0.21285102\n",
      "Iteration 23, loss = 0.20975008\n",
      "Iteration 24, loss = 0.20529181\n",
      "Iteration 25, loss = 0.19627448\n",
      "Iteration 26, loss = 0.19409840\n",
      "Iteration 27, loss = 0.18957481\n",
      "Iteration 28, loss = 0.18401549\n",
      "Iteration 29, loss = 0.18179928\n",
      "Iteration 30, loss = 0.18373702\n",
      "Iteration 31, loss = 0.17729650\n",
      "Iteration 32, loss = 0.16856979\n",
      "Iteration 33, loss = 0.16987363\n",
      "Iteration 34, loss = 0.16531432\n",
      "Iteration 35, loss = 0.16200632\n",
      "Iteration 36, loss = 0.15688976\n",
      "Iteration 37, loss = 0.15113212\n",
      "Iteration 38, loss = 0.15554914\n",
      "Iteration 39, loss = 0.14932002\n",
      "Iteration 40, loss = 0.14777532\n",
      "Iteration 41, loss = 0.15050957\n",
      "Iteration 42, loss = 0.14553308\n",
      "Iteration 43, loss = 0.13731881\n",
      "Iteration 44, loss = 0.13835210\n",
      "Iteration 45, loss = 0.13507086\n",
      "Iteration 46, loss = 0.13730602\n",
      "Iteration 47, loss = 0.13460641\n",
      "Iteration 48, loss = 0.13140837\n",
      "Iteration 49, loss = 0.13354292\n",
      "Iteration 50, loss = 0.12689436\n",
      "Iteration 51, loss = 0.12551044\n",
      "Iteration 52, loss = 0.12536153\n",
      "Iteration 53, loss = 0.12266274\n",
      "Iteration 54, loss = 0.12632592\n",
      "Iteration 55, loss = 0.11535390\n",
      "Iteration 56, loss = 0.11422475\n",
      "Iteration 57, loss = 0.11444358\n",
      "Iteration 58, loss = 0.11945053\n",
      "Iteration 59, loss = 0.11383916\n",
      "Iteration 60, loss = 0.11098571\n",
      "Iteration 61, loss = 0.11554889\n",
      "Iteration 62, loss = 0.10989916\n",
      "Iteration 63, loss = 0.11351810\n",
      "Iteration 64, loss = 0.10902630\n",
      "Iteration 65, loss = 0.11089894\n",
      "Iteration 66, loss = 0.11038281\n",
      "Iteration 67, loss = 0.10968113\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56950224\n",
      "Iteration 2, loss = 0.42684363\n",
      "Iteration 3, loss = 0.38776774\n",
      "Iteration 4, loss = 0.35963972\n",
      "Iteration 5, loss = 0.34669756\n",
      "Iteration 6, loss = 0.33369000\n",
      "Iteration 7, loss = 0.31529952\n",
      "Iteration 8, loss = 0.30841770\n",
      "Iteration 9, loss = 0.28928989\n",
      "Iteration 10, loss = 0.28164293\n",
      "Iteration 11, loss = 0.27822083\n",
      "Iteration 12, loss = 0.26823437\n",
      "Iteration 13, loss = 0.25732506\n",
      "Iteration 14, loss = 0.25419253\n",
      "Iteration 15, loss = 0.24804490\n",
      "Iteration 16, loss = 0.24258343\n",
      "Iteration 17, loss = 0.23422693\n",
      "Iteration 18, loss = 0.22894226\n",
      "Iteration 19, loss = 0.22763479\n",
      "Iteration 20, loss = 0.21655522\n",
      "Iteration 21, loss = 0.21506907\n",
      "Iteration 22, loss = 0.20962109\n",
      "Iteration 23, loss = 0.19948831\n",
      "Iteration 24, loss = 0.20607952\n",
      "Iteration 25, loss = 0.19368193\n",
      "Iteration 26, loss = 0.19035515\n",
      "Iteration 27, loss = 0.18888837\n",
      "Iteration 28, loss = 0.18463387\n",
      "Iteration 29, loss = 0.18003614\n",
      "Iteration 30, loss = 0.17737672\n",
      "Iteration 31, loss = 0.17669199\n",
      "Iteration 32, loss = 0.16906242\n",
      "Iteration 33, loss = 0.17096626\n",
      "Iteration 34, loss = 0.16711004\n",
      "Iteration 35, loss = 0.15842804\n",
      "Iteration 36, loss = 0.15714565\n",
      "Iteration 37, loss = 0.15149279\n",
      "Iteration 38, loss = 0.15527790\n",
      "Iteration 39, loss = 0.15070559\n",
      "Iteration 40, loss = 0.14515057\n",
      "Iteration 41, loss = 0.15197797\n",
      "Iteration 42, loss = 0.13821847\n",
      "Iteration 43, loss = 0.13962687\n",
      "Iteration 44, loss = 0.13762735\n",
      "Iteration 45, loss = 0.13292756\n",
      "Iteration 46, loss = 0.13154259\n",
      "Iteration 47, loss = 0.13531508\n",
      "Iteration 48, loss = 0.13264435\n",
      "Iteration 49, loss = 0.12548418\n",
      "Iteration 50, loss = 0.12772324\n",
      "Iteration 51, loss = 0.12762782\n",
      "Iteration 52, loss = 0.12593142\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61875019\n",
      "Iteration 2, loss = 0.46059693\n",
      "Iteration 3, loss = 0.42470720\n",
      "Iteration 4, loss = 0.40606704\n",
      "Iteration 5, loss = 0.38607186\n",
      "Iteration 6, loss = 0.37333316\n",
      "Iteration 7, loss = 0.36156028\n",
      "Iteration 8, loss = 0.35097946\n",
      "Iteration 9, loss = 0.34681780\n",
      "Iteration 10, loss = 0.33779082\n",
      "Iteration 11, loss = 0.33343693\n",
      "Iteration 12, loss = 0.32803626\n",
      "Iteration 13, loss = 0.32121974\n",
      "Iteration 14, loss = 0.31759641\n",
      "Iteration 15, loss = 0.30990236\n",
      "Iteration 16, loss = 0.30835624\n",
      "Iteration 17, loss = 0.29882773\n",
      "Iteration 18, loss = 0.29636195\n",
      "Iteration 19, loss = 0.29585947\n",
      "Iteration 20, loss = 0.29614691\n",
      "Iteration 21, loss = 0.28891703\n",
      "Iteration 22, loss = 0.28410553\n",
      "Iteration 23, loss = 0.28178922\n",
      "Iteration 24, loss = 0.27902399\n",
      "Iteration 25, loss = 0.27636332\n",
      "Iteration 26, loss = 0.27451402\n",
      "Iteration 27, loss = 0.26892368\n",
      "Iteration 28, loss = 0.27107871\n",
      "Iteration 29, loss = 0.26739363\n",
      "Iteration 30, loss = 0.26355719\n",
      "Iteration 31, loss = 0.26034073\n",
      "Iteration 32, loss = 0.26288662\n",
      "Iteration 33, loss = 0.25655168\n",
      "Iteration 34, loss = 0.25270859\n",
      "Iteration 35, loss = 0.25172375\n",
      "Iteration 36, loss = 0.25179439\n",
      "Iteration 37, loss = 0.24932552\n",
      "Iteration 38, loss = 0.24988090\n",
      "Iteration 39, loss = 0.24617036\n",
      "Iteration 40, loss = 0.24372201\n",
      "Iteration 41, loss = 0.24737703\n",
      "Iteration 42, loss = 0.24162583\n",
      "Iteration 43, loss = 0.24222023\n",
      "Iteration 44, loss = 0.23959973\n",
      "Iteration 45, loss = 0.23948807\n",
      "Iteration 46, loss = 0.24166768\n",
      "Iteration 47, loss = 0.23302357\n",
      "Iteration 48, loss = 0.23610769\n",
      "Iteration 49, loss = 0.23312723\n",
      "Iteration 50, loss = 0.22907543\n",
      "Iteration 51, loss = 0.22916455\n",
      "Iteration 52, loss = 0.23096088\n",
      "Iteration 53, loss = 0.22639980\n",
      "Iteration 54, loss = 0.22744624\n",
      "Iteration 55, loss = 0.22697543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 56, loss = 0.22207731\n",
      "Iteration 57, loss = 0.22393078\n",
      "Iteration 58, loss = 0.22403374\n",
      "Iteration 59, loss = 0.22840796\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61527048\n",
      "Iteration 2, loss = 0.46032902\n",
      "Iteration 3, loss = 0.42984662\n",
      "Iteration 4, loss = 0.40629374\n",
      "Iteration 5, loss = 0.38736193\n",
      "Iteration 6, loss = 0.37902972\n",
      "Iteration 7, loss = 0.37130294\n",
      "Iteration 8, loss = 0.35986371\n",
      "Iteration 9, loss = 0.35042299\n",
      "Iteration 10, loss = 0.34540826\n",
      "Iteration 11, loss = 0.33705513\n",
      "Iteration 12, loss = 0.33185832\n",
      "Iteration 13, loss = 0.32602176\n",
      "Iteration 14, loss = 0.32356055\n",
      "Iteration 15, loss = 0.31438967\n",
      "Iteration 16, loss = 0.31209795\n",
      "Iteration 17, loss = 0.30502854\n",
      "Iteration 18, loss = 0.30126196\n",
      "Iteration 19, loss = 0.29703454\n",
      "Iteration 20, loss = 0.29676775\n",
      "Iteration 21, loss = 0.29049095\n",
      "Iteration 22, loss = 0.28557898\n",
      "Iteration 23, loss = 0.28279894\n",
      "Iteration 24, loss = 0.27810408\n",
      "Iteration 25, loss = 0.27589482\n",
      "Iteration 26, loss = 0.27855552\n",
      "Iteration 27, loss = 0.27517712\n",
      "Iteration 28, loss = 0.26940160\n",
      "Iteration 29, loss = 0.26973937\n",
      "Iteration 30, loss = 0.26735534\n",
      "Iteration 31, loss = 0.26390787\n",
      "Iteration 32, loss = 0.26526246\n",
      "Iteration 33, loss = 0.25879805\n",
      "Iteration 34, loss = 0.25493070\n",
      "Iteration 35, loss = 0.25778004\n",
      "Iteration 36, loss = 0.25246355\n",
      "Iteration 37, loss = 0.24956524\n",
      "Iteration 38, loss = 0.24876477\n",
      "Iteration 39, loss = 0.24755231\n",
      "Iteration 40, loss = 0.24709751\n",
      "Iteration 41, loss = 0.24377118\n",
      "Iteration 42, loss = 0.24082719\n",
      "Iteration 43, loss = 0.24794061\n",
      "Iteration 44, loss = 0.24594362\n",
      "Iteration 45, loss = 0.24125728\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62304552\n",
      "Iteration 2, loss = 0.46896737\n",
      "Iteration 3, loss = 0.42882886\n",
      "Iteration 4, loss = 0.40933742\n",
      "Iteration 5, loss = 0.39143307\n",
      "Iteration 6, loss = 0.38337937\n",
      "Iteration 7, loss = 0.36963482\n",
      "Iteration 8, loss = 0.35772138\n",
      "Iteration 9, loss = 0.34838907\n",
      "Iteration 10, loss = 0.34200429\n",
      "Iteration 11, loss = 0.33443836\n",
      "Iteration 12, loss = 0.33241524\n",
      "Iteration 13, loss = 0.32714823\n",
      "Iteration 14, loss = 0.32269706\n",
      "Iteration 15, loss = 0.31379893\n",
      "Iteration 16, loss = 0.30976122\n",
      "Iteration 17, loss = 0.30705066\n",
      "Iteration 18, loss = 0.29876527\n",
      "Iteration 19, loss = 0.29753982\n",
      "Iteration 20, loss = 0.29262162\n",
      "Iteration 21, loss = 0.29274309\n",
      "Iteration 22, loss = 0.28626508\n",
      "Iteration 23, loss = 0.28731171\n",
      "Iteration 24, loss = 0.27935424\n",
      "Iteration 25, loss = 0.27952714\n",
      "Iteration 26, loss = 0.27790754\n",
      "Iteration 27, loss = 0.27204489\n",
      "Iteration 28, loss = 0.27343713\n",
      "Iteration 29, loss = 0.26610429\n",
      "Iteration 30, loss = 0.26499102\n",
      "Iteration 31, loss = 0.26794404\n",
      "Iteration 32, loss = 0.26274384\n",
      "Iteration 33, loss = 0.25917725\n",
      "Iteration 34, loss = 0.25452118\n",
      "Iteration 35, loss = 0.25612547\n",
      "Iteration 36, loss = 0.25224064\n",
      "Iteration 37, loss = 0.25146101\n",
      "Iteration 38, loss = 0.24960515\n",
      "Iteration 39, loss = 0.24932246\n",
      "Iteration 40, loss = 0.24413725\n",
      "Iteration 41, loss = 0.24411683\n",
      "Iteration 42, loss = 0.24701920\n",
      "Iteration 43, loss = 0.24652369\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63485396\n",
      "Iteration 2, loss = 0.45937472\n",
      "Iteration 3, loss = 0.41923863\n",
      "Iteration 4, loss = 0.39557250\n",
      "Iteration 5, loss = 0.37780049\n",
      "Iteration 6, loss = 0.36610268\n",
      "Iteration 7, loss = 0.35813086\n",
      "Iteration 8, loss = 0.34613100\n",
      "Iteration 9, loss = 0.33713360\n",
      "Iteration 10, loss = 0.33059683\n",
      "Iteration 11, loss = 0.32779363\n",
      "Iteration 12, loss = 0.32187456\n",
      "Iteration 13, loss = 0.31232551\n",
      "Iteration 14, loss = 0.31073003\n",
      "Iteration 15, loss = 0.30356883\n",
      "Iteration 16, loss = 0.29962732\n",
      "Iteration 17, loss = 0.29556305\n",
      "Iteration 18, loss = 0.29213913\n",
      "Iteration 19, loss = 0.29007165\n",
      "Iteration 20, loss = 0.28304404\n",
      "Iteration 21, loss = 0.28242341\n",
      "Iteration 22, loss = 0.27937744\n",
      "Iteration 23, loss = 0.27270094\n",
      "Iteration 24, loss = 0.27422219\n",
      "Iteration 25, loss = 0.27135103\n",
      "Iteration 26, loss = 0.26708362\n",
      "Iteration 27, loss = 0.26457720\n",
      "Iteration 28, loss = 0.26552138\n",
      "Iteration 29, loss = 0.26005683\n",
      "Iteration 30, loss = 0.26192762\n",
      "Iteration 31, loss = 0.25641483\n",
      "Iteration 32, loss = 0.25470651\n",
      "Iteration 33, loss = 0.24890008\n",
      "Iteration 34, loss = 0.24941920\n",
      "Iteration 35, loss = 0.24840396\n",
      "Iteration 36, loss = 0.24885096\n",
      "Iteration 37, loss = 0.24367838\n",
      "Iteration 38, loss = 0.23897992\n",
      "Iteration 39, loss = 0.24036028\n",
      "Iteration 40, loss = 0.23712073\n",
      "Iteration 41, loss = 0.23977852\n",
      "Iteration 42, loss = 0.23775966\n",
      "Iteration 43, loss = 0.23423674\n",
      "Iteration 44, loss = 0.23476554\n",
      "Iteration 45, loss = 0.23153872\n",
      "Iteration 46, loss = 0.22977275\n",
      "Iteration 47, loss = 0.23033824\n",
      "Iteration 48, loss = 0.23011422\n",
      "Iteration 49, loss = 0.22492899\n",
      "Iteration 50, loss = 0.22698961\n",
      "Iteration 51, loss = 0.22351655\n",
      "Iteration 52, loss = 0.22345688\n",
      "Iteration 53, loss = 0.21861493\n",
      "Iteration 54, loss = 0.21865729\n",
      "Iteration 55, loss = 0.22256953\n",
      "Iteration 56, loss = 0.22380047\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63304557\n",
      "Iteration 2, loss = 0.45987928\n",
      "Iteration 3, loss = 0.42173453\n",
      "Iteration 4, loss = 0.39923231\n",
      "Iteration 5, loss = 0.38769841\n",
      "Iteration 6, loss = 0.37089183\n",
      "Iteration 7, loss = 0.36008664\n",
      "Iteration 8, loss = 0.35134430\n",
      "Iteration 9, loss = 0.34321652\n",
      "Iteration 10, loss = 0.33761426\n",
      "Iteration 11, loss = 0.33038598\n",
      "Iteration 12, loss = 0.32439379\n",
      "Iteration 13, loss = 0.31909337\n",
      "Iteration 14, loss = 0.31180407\n",
      "Iteration 15, loss = 0.30858218\n",
      "Iteration 16, loss = 0.30144088\n",
      "Iteration 17, loss = 0.29765189\n",
      "Iteration 18, loss = 0.29254117\n",
      "Iteration 19, loss = 0.29021739\n",
      "Iteration 20, loss = 0.28899999\n",
      "Iteration 21, loss = 0.28270883\n",
      "Iteration 22, loss = 0.28186760\n",
      "Iteration 23, loss = 0.27787975\n",
      "Iteration 24, loss = 0.27426596\n",
      "Iteration 25, loss = 0.26994622\n",
      "Iteration 26, loss = 0.26940401\n",
      "Iteration 27, loss = 0.26294056\n",
      "Iteration 28, loss = 0.26399339\n",
      "Iteration 29, loss = 0.26276910\n",
      "Iteration 30, loss = 0.25858349\n",
      "Iteration 31, loss = 0.25944246\n",
      "Iteration 32, loss = 0.25518171\n",
      "Iteration 33, loss = 0.25012467\n",
      "Iteration 34, loss = 0.25137090\n",
      "Iteration 35, loss = 0.24494387\n",
      "Iteration 36, loss = 0.24658897\n",
      "Iteration 37, loss = 0.24336624\n",
      "Iteration 38, loss = 0.24160470\n",
      "Iteration 39, loss = 0.24007793\n",
      "Iteration 40, loss = 0.24002815\n",
      "Iteration 41, loss = 0.23904598\n",
      "Iteration 42, loss = 0.23893656\n",
      "Iteration 43, loss = 0.23691187\n",
      "Iteration 44, loss = 0.23061490\n",
      "Iteration 45, loss = 0.23218297\n",
      "Iteration 46, loss = 0.22960165\n",
      "Iteration 47, loss = 0.23058613\n",
      "Iteration 48, loss = 0.22799170\n",
      "Iteration 49, loss = 0.22575147\n",
      "Iteration 50, loss = 0.22511007\n",
      "Iteration 51, loss = 0.22290221\n",
      "Iteration 52, loss = 0.22344230\n",
      "Iteration 53, loss = 0.22138545\n",
      "Iteration 54, loss = 0.22282137\n",
      "Iteration 55, loss = 0.21629840\n",
      "Iteration 56, loss = 0.21888109\n",
      "Iteration 57, loss = 0.21695045\n",
      "Iteration 58, loss = 0.21726981\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62799871\n",
      "Iteration 2, loss = 0.45770204\n",
      "Iteration 3, loss = 0.42240089\n",
      "Iteration 4, loss = 0.40269520\n",
      "Iteration 5, loss = 0.38404592\n",
      "Iteration 6, loss = 0.37338443\n",
      "Iteration 7, loss = 0.35908332\n",
      "Iteration 8, loss = 0.35130690\n",
      "Iteration 9, loss = 0.34341838\n",
      "Iteration 10, loss = 0.33710193\n",
      "Iteration 11, loss = 0.32910471\n",
      "Iteration 12, loss = 0.32506334\n",
      "Iteration 13, loss = 0.31643026\n",
      "Iteration 14, loss = 0.31386421\n",
      "Iteration 15, loss = 0.30821804\n",
      "Iteration 16, loss = 0.30178844\n",
      "Iteration 17, loss = 0.29849304\n",
      "Iteration 18, loss = 0.29123796\n",
      "Iteration 19, loss = 0.28924656\n",
      "Iteration 20, loss = 0.28786378\n",
      "Iteration 21, loss = 0.28455408\n",
      "Iteration 22, loss = 0.27963976\n",
      "Iteration 23, loss = 0.27835651\n",
      "Iteration 24, loss = 0.27379131\n",
      "Iteration 25, loss = 0.27353667\n",
      "Iteration 26, loss = 0.26967078\n",
      "Iteration 27, loss = 0.26579443\n",
      "Iteration 28, loss = 0.26232859\n",
      "Iteration 29, loss = 0.26149689\n",
      "Iteration 30, loss = 0.25979927\n",
      "Iteration 31, loss = 0.25925741\n",
      "Iteration 32, loss = 0.25277961\n",
      "Iteration 33, loss = 0.25386578\n",
      "Iteration 34, loss = 0.24988143\n",
      "Iteration 35, loss = 0.24958306\n",
      "Iteration 36, loss = 0.24674890\n",
      "Iteration 37, loss = 0.24168903\n",
      "Iteration 38, loss = 0.24078586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39, loss = 0.23779877\n",
      "Iteration 40, loss = 0.24274923\n",
      "Iteration 41, loss = 0.23766268\n",
      "Iteration 42, loss = 0.23368055\n",
      "Iteration 43, loss = 0.23334171\n",
      "Iteration 44, loss = 0.23281215\n",
      "Iteration 45, loss = 0.23185676\n",
      "Iteration 46, loss = 0.22935696\n",
      "Iteration 47, loss = 0.22896852\n",
      "Iteration 48, loss = 0.22652282\n",
      "Iteration 49, loss = 0.22442973\n",
      "Iteration 50, loss = 0.22150366\n",
      "Iteration 51, loss = 0.22236122\n",
      "Iteration 52, loss = 0.22549834\n",
      "Iteration 53, loss = 0.22169577\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60586519\n",
      "Iteration 2, loss = 0.46009633\n",
      "Iteration 3, loss = 0.42297304\n",
      "Iteration 4, loss = 0.39973172\n",
      "Iteration 5, loss = 0.38482217\n",
      "Iteration 6, loss = 0.37002914\n",
      "Iteration 7, loss = 0.35748158\n",
      "Iteration 8, loss = 0.34686706\n",
      "Iteration 9, loss = 0.33485898\n",
      "Iteration 10, loss = 0.32797292\n",
      "Iteration 11, loss = 0.32404919\n",
      "Iteration 12, loss = 0.31615132\n",
      "Iteration 13, loss = 0.30866554\n",
      "Iteration 14, loss = 0.30351945\n",
      "Iteration 15, loss = 0.29893118\n",
      "Iteration 16, loss = 0.29441741\n",
      "Iteration 17, loss = 0.29212117\n",
      "Iteration 18, loss = 0.28461829\n",
      "Iteration 19, loss = 0.28365418\n",
      "Iteration 20, loss = 0.27706822\n",
      "Iteration 21, loss = 0.27250019\n",
      "Iteration 22, loss = 0.27047080\n",
      "Iteration 23, loss = 0.26407750\n",
      "Iteration 24, loss = 0.25902457\n",
      "Iteration 25, loss = 0.25986253\n",
      "Iteration 26, loss = 0.25843755\n",
      "Iteration 27, loss = 0.24983455\n",
      "Iteration 28, loss = 0.25356312\n",
      "Iteration 29, loss = 0.24939679\n",
      "Iteration 30, loss = 0.24185292\n",
      "Iteration 31, loss = 0.24055619\n",
      "Iteration 32, loss = 0.23979161\n",
      "Iteration 33, loss = 0.23791212\n",
      "Iteration 34, loss = 0.23405140\n",
      "Iteration 35, loss = 0.23514100\n",
      "Iteration 36, loss = 0.23179439\n",
      "Iteration 37, loss = 0.22438576\n",
      "Iteration 38, loss = 0.22602063\n",
      "Iteration 39, loss = 0.22748750\n",
      "Iteration 40, loss = 0.22363617\n",
      "Iteration 41, loss = 0.21954348\n",
      "Iteration 42, loss = 0.22122796\n",
      "Iteration 43, loss = 0.21885918\n",
      "Iteration 44, loss = 0.21159504\n",
      "Iteration 45, loss = 0.21124460\n",
      "Iteration 46, loss = 0.20757911\n",
      "Iteration 47, loss = 0.20724190\n",
      "Iteration 48, loss = 0.21146243\n",
      "Iteration 49, loss = 0.20995891\n",
      "Iteration 50, loss = 0.20839938\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61381683\n",
      "Iteration 2, loss = 0.46812336\n",
      "Iteration 3, loss = 0.42657141\n",
      "Iteration 4, loss = 0.40632118\n",
      "Iteration 5, loss = 0.38698024\n",
      "Iteration 6, loss = 0.37200983\n",
      "Iteration 7, loss = 0.36073178\n",
      "Iteration 8, loss = 0.35246847\n",
      "Iteration 9, loss = 0.34537040\n",
      "Iteration 10, loss = 0.33436408\n",
      "Iteration 11, loss = 0.32112992\n",
      "Iteration 12, loss = 0.31684011\n",
      "Iteration 13, loss = 0.31378679\n",
      "Iteration 14, loss = 0.30927639\n",
      "Iteration 15, loss = 0.30096726\n",
      "Iteration 16, loss = 0.29753976\n",
      "Iteration 17, loss = 0.28694609\n",
      "Iteration 18, loss = 0.28581539\n",
      "Iteration 19, loss = 0.28802386\n",
      "Iteration 20, loss = 0.27494472\n",
      "Iteration 21, loss = 0.27637890\n",
      "Iteration 22, loss = 0.27045667\n",
      "Iteration 23, loss = 0.26583330\n",
      "Iteration 24, loss = 0.26213444\n",
      "Iteration 25, loss = 0.25835457\n",
      "Iteration 26, loss = 0.25537199\n",
      "Iteration 27, loss = 0.25610634\n",
      "Iteration 28, loss = 0.25450764\n",
      "Iteration 29, loss = 0.24863434\n",
      "Iteration 30, loss = 0.24850272\n",
      "Iteration 31, loss = 0.24006787\n",
      "Iteration 32, loss = 0.24688131\n",
      "Iteration 33, loss = 0.24090804\n",
      "Iteration 34, loss = 0.23171891\n",
      "Iteration 35, loss = 0.23611053\n",
      "Iteration 36, loss = 0.23191233\n",
      "Iteration 37, loss = 0.22369732\n",
      "Iteration 38, loss = 0.23103696\n",
      "Iteration 39, loss = 0.22589433\n",
      "Iteration 40, loss = 0.22457885\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60156946\n",
      "Iteration 2, loss = 0.47290155\n",
      "Iteration 3, loss = 0.42541885\n",
      "Iteration 4, loss = 0.40805846\n",
      "Iteration 5, loss = 0.38801862\n",
      "Iteration 6, loss = 0.37236022\n",
      "Iteration 7, loss = 0.35856561\n",
      "Iteration 8, loss = 0.35190634\n",
      "Iteration 9, loss = 0.34091819\n",
      "Iteration 10, loss = 0.32847208\n",
      "Iteration 11, loss = 0.32336727\n",
      "Iteration 12, loss = 0.31809014\n",
      "Iteration 13, loss = 0.31574423\n",
      "Iteration 14, loss = 0.30986792\n",
      "Iteration 15, loss = 0.29696971\n",
      "Iteration 16, loss = 0.29410698\n",
      "Iteration 17, loss = 0.28786921\n",
      "Iteration 18, loss = 0.28431959\n",
      "Iteration 19, loss = 0.28238121\n",
      "Iteration 20, loss = 0.27177072\n",
      "Iteration 21, loss = 0.27691555\n",
      "Iteration 22, loss = 0.27253221\n",
      "Iteration 23, loss = 0.26841140\n",
      "Iteration 24, loss = 0.25972571\n",
      "Iteration 25, loss = 0.26181679\n",
      "Iteration 26, loss = 0.25161975\n",
      "Iteration 27, loss = 0.25291844\n",
      "Iteration 28, loss = 0.24490511\n",
      "Iteration 29, loss = 0.24622918\n",
      "Iteration 30, loss = 0.24660479\n",
      "Iteration 31, loss = 0.24413305\n",
      "Iteration 32, loss = 0.24541257\n",
      "Iteration 33, loss = 0.23347715\n",
      "Iteration 34, loss = 0.23217256\n",
      "Iteration 35, loss = 0.23050230\n",
      "Iteration 36, loss = 0.22661563\n",
      "Iteration 37, loss = 0.22354632\n",
      "Iteration 38, loss = 0.22212668\n",
      "Iteration 39, loss = 0.22330829\n",
      "Iteration 40, loss = 0.22310783\n",
      "Iteration 41, loss = 0.21952311\n",
      "Iteration 42, loss = 0.21703715\n",
      "Iteration 43, loss = 0.21794065\n",
      "Iteration 44, loss = 0.21165752\n",
      "Iteration 45, loss = 0.21044754\n",
      "Iteration 46, loss = 0.21236151\n",
      "Iteration 47, loss = 0.20987802\n",
      "Iteration 48, loss = 0.20633810\n",
      "Iteration 49, loss = 0.20247046\n",
      "Iteration 50, loss = 0.20682133\n",
      "Iteration 51, loss = 0.20693796\n",
      "Iteration 52, loss = 0.20033172\n",
      "Iteration 53, loss = 0.20904713\n",
      "Iteration 54, loss = 0.19717450\n",
      "Iteration 55, loss = 0.20020611\n",
      "Iteration 56, loss = 0.19443559\n",
      "Iteration 57, loss = 0.19394545\n",
      "Iteration 58, loss = 0.19084398\n",
      "Iteration 59, loss = 0.19396121\n",
      "Iteration 60, loss = 0.18991996\n",
      "Iteration 61, loss = 0.18977059\n",
      "Iteration 62, loss = 0.20303982\n",
      "Iteration 63, loss = 0.18829425\n",
      "Iteration 64, loss = 0.19432168\n",
      "Iteration 65, loss = 0.18270078\n",
      "Iteration 66, loss = 0.18375708\n",
      "Iteration 67, loss = 0.18404652\n",
      "Iteration 68, loss = 0.18742026\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62127103\n",
      "Iteration 2, loss = 0.44094892\n",
      "Iteration 3, loss = 0.40788925\n",
      "Iteration 4, loss = 0.38708914\n",
      "Iteration 5, loss = 0.37116812\n",
      "Iteration 6, loss = 0.35936866\n",
      "Iteration 7, loss = 0.34170537\n",
      "Iteration 8, loss = 0.33526999\n",
      "Iteration 9, loss = 0.32268869\n",
      "Iteration 10, loss = 0.31932385\n",
      "Iteration 11, loss = 0.31297666\n",
      "Iteration 12, loss = 0.30567770\n",
      "Iteration 13, loss = 0.29825373\n",
      "Iteration 14, loss = 0.29588801\n",
      "Iteration 15, loss = 0.28433381\n",
      "Iteration 16, loss = 0.28297308\n",
      "Iteration 17, loss = 0.27516009\n",
      "Iteration 18, loss = 0.27313620\n",
      "Iteration 19, loss = 0.27426606\n",
      "Iteration 20, loss = 0.26806345\n",
      "Iteration 21, loss = 0.26088665\n",
      "Iteration 22, loss = 0.25469421\n",
      "Iteration 23, loss = 0.25402731\n",
      "Iteration 24, loss = 0.25352973\n",
      "Iteration 25, loss = 0.24594819\n",
      "Iteration 26, loss = 0.24488364\n",
      "Iteration 27, loss = 0.24133631\n",
      "Iteration 28, loss = 0.23746710\n",
      "Iteration 29, loss = 0.23211872\n",
      "Iteration 30, loss = 0.23446152\n",
      "Iteration 31, loss = 0.22658983\n",
      "Iteration 32, loss = 0.23057147\n",
      "Iteration 33, loss = 0.22555299\n",
      "Iteration 34, loss = 0.22087953\n",
      "Iteration 35, loss = 0.22036559\n",
      "Iteration 36, loss = 0.21516124\n",
      "Iteration 37, loss = 0.21440215\n",
      "Iteration 38, loss = 0.21237771\n",
      "Iteration 39, loss = 0.21097690\n",
      "Iteration 40, loss = 0.20438200\n",
      "Iteration 41, loss = 0.20873577\n",
      "Iteration 42, loss = 0.20018275\n",
      "Iteration 43, loss = 0.20123229\n",
      "Iteration 44, loss = 0.20155342\n",
      "Iteration 45, loss = 0.19942531\n",
      "Iteration 46, loss = 0.20056985\n",
      "Iteration 47, loss = 0.19154773\n",
      "Iteration 48, loss = 0.19349622\n",
      "Iteration 49, loss = 0.19435662\n",
      "Iteration 50, loss = 0.19340283\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61930900\n",
      "Iteration 2, loss = 0.44792265\n",
      "Iteration 3, loss = 0.41110149\n",
      "Iteration 4, loss = 0.38963232\n",
      "Iteration 5, loss = 0.37210368\n",
      "Iteration 6, loss = 0.35889043\n",
      "Iteration 7, loss = 0.35101942\n",
      "Iteration 8, loss = 0.34011226\n",
      "Iteration 9, loss = 0.32316522\n",
      "Iteration 10, loss = 0.31777313\n",
      "Iteration 11, loss = 0.31613717\n",
      "Iteration 12, loss = 0.30670646\n",
      "Iteration 13, loss = 0.30105105\n",
      "Iteration 14, loss = 0.29584683\n",
      "Iteration 15, loss = 0.28885220\n",
      "Iteration 16, loss = 0.28325669\n",
      "Iteration 17, loss = 0.27678199\n",
      "Iteration 18, loss = 0.27732360\n",
      "Iteration 19, loss = 0.27723612\n",
      "Iteration 20, loss = 0.26836423\n",
      "Iteration 21, loss = 0.26572157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, loss = 0.26092216\n",
      "Iteration 23, loss = 0.25633625\n",
      "Iteration 24, loss = 0.25859184\n",
      "Iteration 25, loss = 0.25337323\n",
      "Iteration 26, loss = 0.24599743\n",
      "Iteration 27, loss = 0.24366323\n",
      "Iteration 28, loss = 0.24371545\n",
      "Iteration 29, loss = 0.23770866\n",
      "Iteration 30, loss = 0.23624475\n",
      "Iteration 31, loss = 0.23897933\n",
      "Iteration 32, loss = 0.22758218\n",
      "Iteration 33, loss = 0.22799268\n",
      "Iteration 34, loss = 0.22320036\n",
      "Iteration 35, loss = 0.22114786\n",
      "Iteration 36, loss = 0.22389869\n",
      "Iteration 37, loss = 0.21405328\n",
      "Iteration 38, loss = 0.21399620\n",
      "Iteration 39, loss = 0.21027493\n",
      "Iteration 40, loss = 0.21037887\n",
      "Iteration 41, loss = 0.21161192\n",
      "Iteration 42, loss = 0.20906472\n",
      "Iteration 43, loss = 0.20198866\n",
      "Iteration 44, loss = 0.20441150\n",
      "Iteration 45, loss = 0.19993344\n",
      "Iteration 46, loss = 0.20628759\n",
      "Iteration 47, loss = 0.19820672\n",
      "Iteration 48, loss = 0.19694787\n",
      "Iteration 49, loss = 0.19459093\n",
      "Iteration 50, loss = 0.19512940\n",
      "Iteration 51, loss = 0.19702419\n",
      "Iteration 52, loss = 0.19063707\n",
      "Iteration 53, loss = 0.19165424\n",
      "Iteration 54, loss = 0.19348270\n",
      "Iteration 55, loss = 0.18648203\n",
      "Iteration 56, loss = 0.18421097\n",
      "Iteration 57, loss = 0.18684719\n",
      "Iteration 58, loss = 0.19036018\n",
      "Iteration 59, loss = 0.18048829\n",
      "Iteration 60, loss = 0.17975135\n",
      "Iteration 61, loss = 0.17921297\n",
      "Iteration 62, loss = 0.17749492\n",
      "Iteration 63, loss = 0.17691320\n",
      "Iteration 64, loss = 0.17137910\n",
      "Iteration 65, loss = 0.17778586\n",
      "Iteration 66, loss = 0.17688061\n",
      "Iteration 67, loss = 0.16594509\n",
      "Iteration 68, loss = 0.16849768\n",
      "Iteration 69, loss = 0.16549337\n",
      "Iteration 70, loss = 0.16921931\n",
      "Iteration 71, loss = 0.17156382\n",
      "Iteration 72, loss = 0.16999998\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62136566\n",
      "Iteration 2, loss = 0.44895010\n",
      "Iteration 3, loss = 0.41219590\n",
      "Iteration 4, loss = 0.39330751\n",
      "Iteration 5, loss = 0.37672388\n",
      "Iteration 6, loss = 0.36264383\n",
      "Iteration 7, loss = 0.35215009\n",
      "Iteration 8, loss = 0.33796899\n",
      "Iteration 9, loss = 0.32781130\n",
      "Iteration 10, loss = 0.32194081\n",
      "Iteration 11, loss = 0.31079269\n",
      "Iteration 12, loss = 0.30984864\n",
      "Iteration 13, loss = 0.30232637\n",
      "Iteration 14, loss = 0.29557909\n",
      "Iteration 15, loss = 0.29348859\n",
      "Iteration 16, loss = 0.28409121\n",
      "Iteration 17, loss = 0.27787142\n",
      "Iteration 18, loss = 0.27313892\n",
      "Iteration 19, loss = 0.26810904\n",
      "Iteration 20, loss = 0.26667642\n",
      "Iteration 21, loss = 0.26300200\n",
      "Iteration 22, loss = 0.26176447\n",
      "Iteration 23, loss = 0.25303392\n",
      "Iteration 24, loss = 0.25227838\n",
      "Iteration 25, loss = 0.24824476\n",
      "Iteration 26, loss = 0.24348010\n",
      "Iteration 27, loss = 0.23709154\n",
      "Iteration 28, loss = 0.23923317\n",
      "Iteration 29, loss = 0.23853784\n",
      "Iteration 30, loss = 0.23369737\n",
      "Iteration 31, loss = 0.23673969\n",
      "Iteration 32, loss = 0.22728614\n",
      "Iteration 33, loss = 0.22214605\n",
      "Iteration 34, loss = 0.22341196\n",
      "Iteration 35, loss = 0.22170523\n",
      "Iteration 36, loss = 0.21547180\n",
      "Iteration 37, loss = 0.22052760\n",
      "Iteration 38, loss = 0.21015180\n",
      "Iteration 39, loss = 0.21159143\n",
      "Iteration 40, loss = 0.20943664\n",
      "Iteration 41, loss = 0.20265379\n",
      "Iteration 42, loss = 0.20430356\n",
      "Iteration 43, loss = 0.20086246\n",
      "Iteration 44, loss = 0.19976785\n",
      "Iteration 45, loss = 0.19995285\n",
      "Iteration 46, loss = 0.19990594\n",
      "Iteration 47, loss = 0.19992339\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59844280\n",
      "Iteration 2, loss = 0.45137049\n",
      "Iteration 3, loss = 0.41403309\n",
      "Iteration 4, loss = 0.39537828\n",
      "Iteration 5, loss = 0.37468737\n",
      "Iteration 6, loss = 0.35926726\n",
      "Iteration 7, loss = 0.34853965\n",
      "Iteration 8, loss = 0.33949944\n",
      "Iteration 9, loss = 0.33342400\n",
      "Iteration 10, loss = 0.32239260\n",
      "Iteration 11, loss = 0.31654136\n",
      "Iteration 12, loss = 0.30717069\n",
      "Iteration 13, loss = 0.30129265\n",
      "Iteration 14, loss = 0.29848651\n",
      "Iteration 15, loss = 0.29250510\n",
      "Iteration 16, loss = 0.29053355\n",
      "Iteration 17, loss = 0.28330166\n",
      "Iteration 18, loss = 0.28146431\n",
      "Iteration 19, loss = 0.27499567\n",
      "Iteration 20, loss = 0.27426609\n",
      "Iteration 21, loss = 0.26760068\n",
      "Iteration 22, loss = 0.26539305\n",
      "Iteration 23, loss = 0.26316621\n",
      "Iteration 24, loss = 0.25773101\n",
      "Iteration 25, loss = 0.26196794\n",
      "Iteration 26, loss = 0.25312956\n",
      "Iteration 27, loss = 0.25095605\n",
      "Iteration 28, loss = 0.24543710\n",
      "Iteration 29, loss = 0.24088334\n",
      "Iteration 30, loss = 0.24198889\n",
      "Iteration 31, loss = 0.24295186\n",
      "Iteration 32, loss = 0.23977229\n",
      "Iteration 33, loss = 0.23410616\n",
      "Iteration 34, loss = 0.24018523\n",
      "Iteration 35, loss = 0.22901207\n",
      "Iteration 36, loss = 0.22465098\n",
      "Iteration 37, loss = 0.23123106\n",
      "Iteration 38, loss = 0.22353781\n",
      "Iteration 39, loss = 0.22117877\n",
      "Iteration 40, loss = 0.22139885\n",
      "Iteration 41, loss = 0.21725053\n",
      "Iteration 42, loss = 0.22488229\n",
      "Iteration 43, loss = 0.21959421\n",
      "Iteration 44, loss = 0.21119777\n",
      "Iteration 45, loss = 0.21404479\n",
      "Iteration 46, loss = 0.21467800\n",
      "Iteration 47, loss = 0.21254874\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61661137\n",
      "Iteration 2, loss = 0.46003264\n",
      "Iteration 3, loss = 0.41960233\n",
      "Iteration 4, loss = 0.39913821\n",
      "Iteration 5, loss = 0.38254597\n",
      "Iteration 6, loss = 0.37128465\n",
      "Iteration 7, loss = 0.35113324\n",
      "Iteration 8, loss = 0.34625350\n",
      "Iteration 9, loss = 0.33348312\n",
      "Iteration 10, loss = 0.32927089\n",
      "Iteration 11, loss = 0.31816880\n",
      "Iteration 12, loss = 0.31256397\n",
      "Iteration 13, loss = 0.30674256\n",
      "Iteration 14, loss = 0.29886444\n",
      "Iteration 15, loss = 0.29336629\n",
      "Iteration 16, loss = 0.28710366\n",
      "Iteration 17, loss = 0.28747771\n",
      "Iteration 18, loss = 0.27949537\n",
      "Iteration 19, loss = 0.27818638\n",
      "Iteration 20, loss = 0.27068750\n",
      "Iteration 21, loss = 0.27003212\n",
      "Iteration 22, loss = 0.26723490\n",
      "Iteration 23, loss = 0.26509442\n",
      "Iteration 24, loss = 0.26489839\n",
      "Iteration 25, loss = 0.25748216\n",
      "Iteration 26, loss = 0.25497530\n",
      "Iteration 27, loss = 0.25201178\n",
      "Iteration 28, loss = 0.24613054\n",
      "Iteration 29, loss = 0.24882271\n",
      "Iteration 30, loss = 0.24782765\n",
      "Iteration 31, loss = 0.24066602\n",
      "Iteration 32, loss = 0.23401662\n",
      "Iteration 33, loss = 0.23947676\n",
      "Iteration 34, loss = 0.23301970\n",
      "Iteration 35, loss = 0.22967351\n",
      "Iteration 36, loss = 0.22662502\n",
      "Iteration 37, loss = 0.22390747\n",
      "Iteration 38, loss = 0.22853019\n",
      "Iteration 39, loss = 0.22618568\n",
      "Iteration 40, loss = 0.21903385\n",
      "Iteration 41, loss = 0.22331299\n",
      "Iteration 42, loss = 0.21968973\n",
      "Iteration 43, loss = 0.20937848\n",
      "Iteration 44, loss = 0.21704333\n",
      "Iteration 45, loss = 0.20918933\n",
      "Iteration 46, loss = 0.21302024\n",
      "Iteration 47, loss = 0.21027604\n",
      "Iteration 48, loss = 0.21146659\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60114004\n",
      "Iteration 2, loss = 0.45953401\n",
      "Iteration 3, loss = 0.42103073\n",
      "Iteration 4, loss = 0.39408766\n",
      "Iteration 5, loss = 0.38260185\n",
      "Iteration 6, loss = 0.36966546\n",
      "Iteration 7, loss = 0.35239188\n",
      "Iteration 8, loss = 0.34730499\n",
      "Iteration 9, loss = 0.32924861\n",
      "Iteration 10, loss = 0.32344314\n",
      "Iteration 11, loss = 0.31914761\n",
      "Iteration 12, loss = 0.31139649\n",
      "Iteration 13, loss = 0.30123665\n",
      "Iteration 14, loss = 0.30160241\n",
      "Iteration 15, loss = 0.29594840\n",
      "Iteration 16, loss = 0.29135035\n",
      "Iteration 17, loss = 0.28422352\n",
      "Iteration 18, loss = 0.27931754\n",
      "Iteration 19, loss = 0.27843507\n",
      "Iteration 20, loss = 0.27094845\n",
      "Iteration 21, loss = 0.27205799\n",
      "Iteration 22, loss = 0.26574193\n",
      "Iteration 23, loss = 0.25805306\n",
      "Iteration 24, loss = 0.26256129\n",
      "Iteration 25, loss = 0.25371205\n",
      "Iteration 26, loss = 0.25087447\n",
      "Iteration 27, loss = 0.25088213\n",
      "Iteration 28, loss = 0.24622563\n",
      "Iteration 29, loss = 0.24347952\n",
      "Iteration 30, loss = 0.24356329\n",
      "Iteration 31, loss = 0.24110440\n",
      "Iteration 32, loss = 0.23603714\n",
      "Iteration 33, loss = 0.24126034\n",
      "Iteration 34, loss = 0.23367287\n",
      "Iteration 35, loss = 0.23018220\n",
      "Iteration 36, loss = 0.23100610\n",
      "Iteration 37, loss = 0.22720826\n",
      "Iteration 38, loss = 0.23004629\n",
      "Iteration 39, loss = 0.22277258\n",
      "Iteration 40, loss = 0.21937168\n",
      "Iteration 41, loss = 0.22305658\n",
      "Iteration 42, loss = 0.21820873\n",
      "Iteration 43, loss = 0.21400878\n",
      "Iteration 44, loss = 0.21039981\n",
      "Iteration 45, loss = 0.21200107\n",
      "Iteration 46, loss = 0.21017187\n",
      "Iteration 47, loss = 0.20960728\n",
      "Iteration 48, loss = 0.20718512\n",
      "Iteration 49, loss = 0.20216452\n",
      "Iteration 50, loss = 0.20857170\n",
      "Iteration 51, loss = 0.20479515\n",
      "Iteration 52, loss = 0.20478353\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69091159\n",
      "Iteration 2, loss = 0.52577898\n",
      "Iteration 3, loss = 0.48634313\n",
      "Iteration 4, loss = 0.46483768\n",
      "Iteration 5, loss = 0.44296084\n",
      "Iteration 6, loss = 0.42953837\n",
      "Iteration 7, loss = 0.41687041\n",
      "Iteration 8, loss = 0.40818526\n",
      "Iteration 9, loss = 0.40311766\n",
      "Iteration 10, loss = 0.39522858\n",
      "Iteration 11, loss = 0.39392860\n",
      "Iteration 12, loss = 0.38908106\n",
      "Iteration 13, loss = 0.38282443\n",
      "Iteration 14, loss = 0.38167612\n",
      "Iteration 15, loss = 0.37427515\n",
      "Iteration 16, loss = 0.37359185\n",
      "Iteration 17, loss = 0.36513291\n",
      "Iteration 18, loss = 0.36478055\n",
      "Iteration 19, loss = 0.36456858\n",
      "Iteration 20, loss = 0.36482251\n",
      "Iteration 21, loss = 0.36168856\n",
      "Iteration 22, loss = 0.35670075\n",
      "Iteration 23, loss = 0.35614851\n",
      "Iteration 24, loss = 0.35317937\n",
      "Iteration 25, loss = 0.35203261\n",
      "Iteration 26, loss = 0.35405483\n",
      "Iteration 27, loss = 0.34762906\n",
      "Iteration 28, loss = 0.34991299\n",
      "Iteration 29, loss = 0.34681637\n",
      "Iteration 30, loss = 0.34493812\n",
      "Iteration 31, loss = 0.34168763\n",
      "Iteration 32, loss = 0.34718253\n",
      "Iteration 33, loss = 0.34007006\n",
      "Iteration 34, loss = 0.33577158\n",
      "Iteration 35, loss = 0.33775289\n",
      "Iteration 36, loss = 0.33819708\n",
      "Iteration 37, loss = 0.33555517\n",
      "Iteration 38, loss = 0.33764377\n",
      "Iteration 39, loss = 0.33620254\n",
      "Iteration 40, loss = 0.33540204\n",
      "Iteration 41, loss = 0.33872607\n",
      "Iteration 42, loss = 0.33262695\n",
      "Iteration 43, loss = 0.33253729\n",
      "Iteration 44, loss = 0.33154399\n",
      "Iteration 45, loss = 0.33465621\n",
      "Iteration 46, loss = 0.33635594\n",
      "Iteration 47, loss = 0.32869209\n",
      "Iteration 48, loss = 0.33123566\n",
      "Iteration 49, loss = 0.32883343\n",
      "Iteration 50, loss = 0.32699086\n",
      "Iteration 51, loss = 0.32682135\n",
      "Iteration 52, loss = 0.32834982\n",
      "Iteration 53, loss = 0.32251302\n",
      "Iteration 54, loss = 0.32812040\n",
      "Iteration 55, loss = 0.32652605\n",
      "Iteration 56, loss = 0.32367086\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68749020\n",
      "Iteration 2, loss = 0.52502312\n",
      "Iteration 3, loss = 0.49141644\n",
      "Iteration 4, loss = 0.46350154\n",
      "Iteration 5, loss = 0.44329039\n",
      "Iteration 6, loss = 0.43534587\n",
      "Iteration 7, loss = 0.42460582\n",
      "Iteration 8, loss = 0.41922421\n",
      "Iteration 9, loss = 0.40780064\n",
      "Iteration 10, loss = 0.40389201\n",
      "Iteration 11, loss = 0.39610509\n",
      "Iteration 12, loss = 0.39188307\n",
      "Iteration 13, loss = 0.38921232\n",
      "Iteration 14, loss = 0.38535266\n",
      "Iteration 15, loss = 0.37926544\n",
      "Iteration 16, loss = 0.37939223\n",
      "Iteration 17, loss = 0.37220330\n",
      "Iteration 18, loss = 0.37060183\n",
      "Iteration 19, loss = 0.36605221\n",
      "Iteration 20, loss = 0.36567356\n",
      "Iteration 21, loss = 0.36614920\n",
      "Iteration 22, loss = 0.35985581\n",
      "Iteration 23, loss = 0.35757888\n",
      "Iteration 24, loss = 0.35567375\n",
      "Iteration 25, loss = 0.35196038\n",
      "Iteration 26, loss = 0.35753565\n",
      "Iteration 27, loss = 0.35407616\n",
      "Iteration 28, loss = 0.35069221\n",
      "Iteration 29, loss = 0.35111653\n",
      "Iteration 30, loss = 0.34841327\n",
      "Iteration 31, loss = 0.34792394\n",
      "Iteration 32, loss = 0.35269914\n",
      "Iteration 33, loss = 0.34319596\n",
      "Iteration 34, loss = 0.34167826\n",
      "Iteration 35, loss = 0.34598733\n",
      "Iteration 36, loss = 0.34090149\n",
      "Iteration 37, loss = 0.33916953\n",
      "Iteration 38, loss = 0.33929113\n",
      "Iteration 39, loss = 0.33819109\n",
      "Iteration 40, loss = 0.34002343\n",
      "Iteration 41, loss = 0.33538348\n",
      "Iteration 42, loss = 0.33423754\n",
      "Iteration 43, loss = 0.34160992\n",
      "Iteration 44, loss = 0.33885834\n",
      "Iteration 45, loss = 0.33543429\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69506417\n",
      "Iteration 2, loss = 0.53458930\n",
      "Iteration 3, loss = 0.48946598\n",
      "Iteration 4, loss = 0.46725619\n",
      "Iteration 5, loss = 0.44684829\n",
      "Iteration 6, loss = 0.43961323\n",
      "Iteration 7, loss = 0.42494933\n",
      "Iteration 8, loss = 0.41444870\n",
      "Iteration 9, loss = 0.40630813\n",
      "Iteration 10, loss = 0.39963149\n",
      "Iteration 11, loss = 0.39373250\n",
      "Iteration 12, loss = 0.39219018\n",
      "Iteration 13, loss = 0.39079164\n",
      "Iteration 14, loss = 0.38414797\n",
      "Iteration 15, loss = 0.37708589\n",
      "Iteration 16, loss = 0.37399509\n",
      "Iteration 17, loss = 0.37240825\n",
      "Iteration 18, loss = 0.36711129\n",
      "Iteration 19, loss = 0.36786526\n",
      "Iteration 20, loss = 0.36272641\n",
      "Iteration 21, loss = 0.36622434\n",
      "Iteration 22, loss = 0.36002738\n",
      "Iteration 23, loss = 0.36233634\n",
      "Iteration 24, loss = 0.35518095\n",
      "Iteration 25, loss = 0.35535096\n",
      "Iteration 26, loss = 0.35495102\n",
      "Iteration 27, loss = 0.35134321\n",
      "Iteration 28, loss = 0.35351302\n",
      "Iteration 29, loss = 0.34779367\n",
      "Iteration 30, loss = 0.34746393\n",
      "Iteration 31, loss = 0.34967897\n",
      "Iteration 32, loss = 0.34620961\n",
      "Iteration 33, loss = 0.34346439\n",
      "Iteration 34, loss = 0.34035144\n",
      "Iteration 35, loss = 0.34363226\n",
      "Iteration 36, loss = 0.34183097\n",
      "Iteration 37, loss = 0.33977921\n",
      "Iteration 38, loss = 0.33988985\n",
      "Iteration 39, loss = 0.34184289\n",
      "Iteration 40, loss = 0.33586381\n",
      "Iteration 41, loss = 0.33597427\n",
      "Iteration 42, loss = 0.33709956\n",
      "Iteration 43, loss = 0.34079739\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68060371\n",
      "Iteration 2, loss = 0.50469974\n",
      "Iteration 3, loss = 0.46433130\n",
      "Iteration 4, loss = 0.44148281\n",
      "Iteration 5, loss = 0.42349677\n",
      "Iteration 6, loss = 0.41326947\n",
      "Iteration 7, loss = 0.40558236\n",
      "Iteration 8, loss = 0.39570055\n",
      "Iteration 9, loss = 0.38802637\n",
      "Iteration 10, loss = 0.38074870\n",
      "Iteration 11, loss = 0.38039753\n",
      "Iteration 12, loss = 0.37639341\n",
      "Iteration 13, loss = 0.36769109\n",
      "Iteration 14, loss = 0.36786712\n",
      "Iteration 15, loss = 0.36296975\n",
      "Iteration 16, loss = 0.36049898\n",
      "Iteration 17, loss = 0.35784777\n",
      "Iteration 18, loss = 0.35504087\n",
      "Iteration 19, loss = 0.35435510\n",
      "Iteration 20, loss = 0.34692699\n",
      "Iteration 21, loss = 0.34859405\n",
      "Iteration 22, loss = 0.34575934\n",
      "Iteration 23, loss = 0.34185019\n",
      "Iteration 24, loss = 0.34399967\n",
      "Iteration 25, loss = 0.34253604\n",
      "Iteration 26, loss = 0.34021692\n",
      "Iteration 27, loss = 0.33833874\n",
      "Iteration 28, loss = 0.33849665\n",
      "Iteration 29, loss = 0.33639006\n",
      "Iteration 30, loss = 0.34058038\n",
      "Iteration 31, loss = 0.33310675\n",
      "Iteration 32, loss = 0.33218243\n",
      "Iteration 33, loss = 0.32751226\n",
      "Iteration 34, loss = 0.33128378\n",
      "Iteration 35, loss = 0.33056106\n",
      "Iteration 36, loss = 0.33018590\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67891910\n",
      "Iteration 2, loss = 0.50529905\n",
      "Iteration 3, loss = 0.46693604\n",
      "Iteration 4, loss = 0.44446804\n",
      "Iteration 5, loss = 0.43356514\n",
      "Iteration 6, loss = 0.41801428\n",
      "Iteration 7, loss = 0.40743178\n",
      "Iteration 8, loss = 0.40050051\n",
      "Iteration 9, loss = 0.39315700\n",
      "Iteration 10, loss = 0.38874232\n",
      "Iteration 11, loss = 0.38324878\n",
      "Iteration 12, loss = 0.38040760\n",
      "Iteration 13, loss = 0.37477223\n",
      "Iteration 14, loss = 0.37006677\n",
      "Iteration 15, loss = 0.36792487\n",
      "Iteration 16, loss = 0.36108157\n",
      "Iteration 17, loss = 0.35915584\n",
      "Iteration 18, loss = 0.35569609\n",
      "Iteration 19, loss = 0.35607799\n",
      "Iteration 20, loss = 0.35451999\n",
      "Iteration 21, loss = 0.35016419\n",
      "Iteration 22, loss = 0.35152447\n",
      "Iteration 23, loss = 0.34749234\n",
      "Iteration 24, loss = 0.34504632\n",
      "Iteration 25, loss = 0.34238206\n",
      "Iteration 26, loss = 0.34454751\n",
      "Iteration 27, loss = 0.33847573\n",
      "Iteration 28, loss = 0.34061914\n",
      "Iteration 29, loss = 0.33951499\n",
      "Iteration 30, loss = 0.33786170\n",
      "Iteration 31, loss = 0.33862294\n",
      "Iteration 32, loss = 0.33434112\n",
      "Iteration 33, loss = 0.33138212\n",
      "Iteration 34, loss = 0.33240565\n",
      "Iteration 35, loss = 0.32825878\n",
      "Iteration 36, loss = 0.33122860\n",
      "Iteration 37, loss = 0.32876454\n",
      "Iteration 38, loss = 0.32778155\n",
      "Iteration 39, loss = 0.32542406\n",
      "Iteration 40, loss = 0.32647075\n",
      "Iteration 41, loss = 0.32735837\n",
      "Iteration 42, loss = 0.32850485\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67366499\n",
      "Iteration 2, loss = 0.50270152\n",
      "Iteration 3, loss = 0.46741171\n",
      "Iteration 4, loss = 0.44720315\n",
      "Iteration 5, loss = 0.42951301\n",
      "Iteration 6, loss = 0.42051084\n",
      "Iteration 7, loss = 0.40656289\n",
      "Iteration 8, loss = 0.39897562\n",
      "Iteration 9, loss = 0.39194047\n",
      "Iteration 10, loss = 0.38839688\n",
      "Iteration 11, loss = 0.38257815\n",
      "Iteration 12, loss = 0.37958743\n",
      "Iteration 13, loss = 0.37207737\n",
      "Iteration 14, loss = 0.37143718\n",
      "Iteration 15, loss = 0.36837703\n",
      "Iteration 16, loss = 0.36112548\n",
      "Iteration 17, loss = 0.35821150\n",
      "Iteration 18, loss = 0.35340929\n",
      "Iteration 19, loss = 0.35416098\n",
      "Iteration 20, loss = 0.35356876\n",
      "Iteration 21, loss = 0.34951834\n",
      "Iteration 22, loss = 0.34838144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23, loss = 0.34734749\n",
      "Iteration 24, loss = 0.34671252\n",
      "Iteration 25, loss = 0.34586196\n",
      "Iteration 26, loss = 0.34314324\n",
      "Iteration 27, loss = 0.34180013\n",
      "Iteration 28, loss = 0.33712802\n",
      "Iteration 29, loss = 0.33815249\n",
      "Iteration 30, loss = 0.33554320\n",
      "Iteration 31, loss = 0.33917090\n",
      "Iteration 32, loss = 0.33175894\n",
      "Iteration 33, loss = 0.33547604\n",
      "Iteration 34, loss = 0.33126535\n",
      "Iteration 35, loss = 0.33206487\n",
      "Iteration 36, loss = 0.33015008\n",
      "Iteration 37, loss = 0.32715271\n",
      "Iteration 38, loss = 0.32531464\n",
      "Iteration 39, loss = 0.32359448\n",
      "Iteration 40, loss = 0.32847895\n",
      "Iteration 41, loss = 0.32628521\n",
      "Iteration 42, loss = 0.32330788\n",
      "Iteration 43, loss = 0.32333794\n",
      "Iteration 44, loss = 0.32232633\n",
      "Iteration 45, loss = 0.32314490\n",
      "Iteration 46, loss = 0.32098875\n",
      "Iteration 47, loss = 0.32081922\n",
      "Iteration 48, loss = 0.32020323\n",
      "Iteration 49, loss = 0.31757383\n",
      "Iteration 50, loss = 0.31551706\n",
      "Iteration 51, loss = 0.31590316\n",
      "Iteration 52, loss = 0.32093822\n",
      "Iteration 53, loss = 0.31838490\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72400939\n",
      "Iteration 2, loss = 0.56484420\n",
      "Iteration 3, loss = 0.51906552\n",
      "Iteration 4, loss = 0.48847310\n",
      "Iteration 5, loss = 0.46788756\n",
      "Iteration 6, loss = 0.45072919\n",
      "Iteration 7, loss = 0.43423080\n",
      "Iteration 8, loss = 0.42256750\n",
      "Iteration 9, loss = 0.40832214\n",
      "Iteration 10, loss = 0.40209467\n",
      "Iteration 11, loss = 0.39737548\n",
      "Iteration 12, loss = 0.39267238\n",
      "Iteration 13, loss = 0.38339361\n",
      "Iteration 14, loss = 0.37672668\n",
      "Iteration 15, loss = 0.37214744\n",
      "Iteration 16, loss = 0.37147491\n",
      "Iteration 17, loss = 0.36889582\n",
      "Iteration 18, loss = 0.36299441\n",
      "Iteration 19, loss = 0.36217430\n",
      "Iteration 20, loss = 0.35702815\n",
      "Iteration 21, loss = 0.35264538\n",
      "Iteration 22, loss = 0.34864401\n",
      "Iteration 23, loss = 0.34598222\n",
      "Iteration 24, loss = 0.34239979\n",
      "Iteration 25, loss = 0.34742345\n",
      "Iteration 26, loss = 0.34273882\n",
      "Iteration 27, loss = 0.33567386\n",
      "Iteration 28, loss = 0.33974631\n",
      "Iteration 29, loss = 0.33593918\n",
      "Iteration 30, loss = 0.33101925\n",
      "Iteration 31, loss = 0.33232798\n",
      "Iteration 32, loss = 0.33142390\n",
      "Iteration 33, loss = 0.33103061\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73214056\n",
      "Iteration 2, loss = 0.57313733\n",
      "Iteration 3, loss = 0.52320297\n",
      "Iteration 4, loss = 0.49535691\n",
      "Iteration 5, loss = 0.47146489\n",
      "Iteration 6, loss = 0.45278828\n",
      "Iteration 7, loss = 0.43892885\n",
      "Iteration 8, loss = 0.42952750\n",
      "Iteration 9, loss = 0.41959427\n",
      "Iteration 10, loss = 0.40839175\n",
      "Iteration 11, loss = 0.39475878\n",
      "Iteration 12, loss = 0.39103155\n",
      "Iteration 13, loss = 0.38578905\n",
      "Iteration 14, loss = 0.38481035\n",
      "Iteration 15, loss = 0.37716060\n",
      "Iteration 16, loss = 0.37488848\n",
      "Iteration 17, loss = 0.36439431\n",
      "Iteration 18, loss = 0.36357635\n",
      "Iteration 19, loss = 0.36534409\n",
      "Iteration 20, loss = 0.35307461\n",
      "Iteration 21, loss = 0.35801548\n",
      "Iteration 22, loss = 0.35052921\n",
      "Iteration 23, loss = 0.34886190\n",
      "Iteration 24, loss = 0.34369690\n",
      "Iteration 25, loss = 0.34336570\n",
      "Iteration 26, loss = 0.34205268\n",
      "Iteration 27, loss = 0.34626910\n",
      "Iteration 28, loss = 0.34145311\n",
      "Iteration 29, loss = 0.33379069\n",
      "Iteration 30, loss = 0.33434979\n",
      "Iteration 31, loss = 0.32983937\n",
      "Iteration 32, loss = 0.33056158\n",
      "Iteration 33, loss = 0.33384959\n",
      "Iteration 34, loss = 0.32188031\n",
      "Iteration 35, loss = 0.32797949\n",
      "Iteration 36, loss = 0.31960570\n",
      "Iteration 37, loss = 0.31454771\n",
      "Iteration 38, loss = 0.32333374\n",
      "Iteration 39, loss = 0.31914489\n",
      "Iteration 40, loss = 0.31276328\n",
      "Iteration 41, loss = 0.31735113\n",
      "Iteration 42, loss = 0.31651497\n",
      "Iteration 43, loss = 0.31106796\n",
      "Iteration 44, loss = 0.31107740\n",
      "Iteration 45, loss = 0.31042509\n",
      "Iteration 46, loss = 0.31034089\n",
      "Iteration 47, loss = 0.31009884\n",
      "Iteration 48, loss = 0.30689475\n",
      "Iteration 49, loss = 0.30482149\n",
      "Iteration 50, loss = 0.30727102\n",
      "Iteration 51, loss = 0.30901942\n",
      "Iteration 52, loss = 0.30615703\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71963406\n",
      "Iteration 2, loss = 0.57855405\n",
      "Iteration 3, loss = 0.52116665\n",
      "Iteration 4, loss = 0.49699277\n",
      "Iteration 5, loss = 0.47274970\n",
      "Iteration 6, loss = 0.45341298\n",
      "Iteration 7, loss = 0.43501181\n",
      "Iteration 8, loss = 0.42608321\n",
      "Iteration 9, loss = 0.41726171\n",
      "Iteration 10, loss = 0.40188294\n",
      "Iteration 11, loss = 0.39666595\n",
      "Iteration 12, loss = 0.39452742\n",
      "Iteration 13, loss = 0.38870481\n",
      "Iteration 14, loss = 0.38542035\n",
      "Iteration 15, loss = 0.37028920\n",
      "Iteration 16, loss = 0.36951947\n",
      "Iteration 17, loss = 0.36784227\n",
      "Iteration 18, loss = 0.36183575\n",
      "Iteration 19, loss = 0.36195512\n",
      "Iteration 20, loss = 0.35389698\n",
      "Iteration 21, loss = 0.35219197\n",
      "Iteration 22, loss = 0.35010829\n",
      "Iteration 23, loss = 0.35243617\n",
      "Iteration 24, loss = 0.34537302\n",
      "Iteration 25, loss = 0.34624230\n",
      "Iteration 26, loss = 0.33655162\n",
      "Iteration 27, loss = 0.34282492\n",
      "Iteration 28, loss = 0.33328162\n",
      "Iteration 29, loss = 0.33227488\n",
      "Iteration 30, loss = 0.33162056\n",
      "Iteration 31, loss = 0.32887482\n",
      "Iteration 32, loss = 0.33498437\n",
      "Iteration 33, loss = 0.32533706\n",
      "Iteration 34, loss = 0.32760002\n",
      "Iteration 35, loss = 0.32359933\n",
      "Iteration 36, loss = 0.32337671\n",
      "Iteration 37, loss = 0.31462152\n",
      "Iteration 38, loss = 0.31345096\n",
      "Iteration 39, loss = 0.31844611\n",
      "Iteration 40, loss = 0.31577804\n",
      "Iteration 41, loss = 0.31634445\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69123149\n",
      "Iteration 2, loss = 0.50863056\n",
      "Iteration 3, loss = 0.47385608\n",
      "Iteration 4, loss = 0.45263216\n",
      "Iteration 5, loss = 0.43573107\n",
      "Iteration 6, loss = 0.42257409\n",
      "Iteration 7, loss = 0.40642653\n",
      "Iteration 8, loss = 0.39969416\n",
      "Iteration 9, loss = 0.38659299\n",
      "Iteration 10, loss = 0.38462157\n",
      "Iteration 11, loss = 0.37809390\n",
      "Iteration 12, loss = 0.37477225\n",
      "Iteration 13, loss = 0.36648725\n",
      "Iteration 14, loss = 0.36525483\n",
      "Iteration 15, loss = 0.35479313\n",
      "Iteration 16, loss = 0.35370419\n",
      "Iteration 17, loss = 0.34786004\n",
      "Iteration 18, loss = 0.34407249\n",
      "Iteration 19, loss = 0.35015181\n",
      "Iteration 20, loss = 0.34272701\n",
      "Iteration 21, loss = 0.33449673\n",
      "Iteration 22, loss = 0.33163448\n",
      "Iteration 23, loss = 0.33070064\n",
      "Iteration 24, loss = 0.33516521\n",
      "Iteration 25, loss = 0.32635165\n",
      "Iteration 26, loss = 0.32515779\n",
      "Iteration 27, loss = 0.32418757\n",
      "Iteration 28, loss = 0.32218590\n",
      "Iteration 29, loss = 0.31561918\n",
      "Iteration 30, loss = 0.31948432\n",
      "Iteration 31, loss = 0.31080390\n",
      "Iteration 32, loss = 0.31902413\n",
      "Iteration 33, loss = 0.31107341\n",
      "Iteration 34, loss = 0.31115046\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68924020\n",
      "Iteration 2, loss = 0.51506340\n",
      "Iteration 3, loss = 0.47685974\n",
      "Iteration 4, loss = 0.45399137\n",
      "Iteration 5, loss = 0.43617346\n",
      "Iteration 6, loss = 0.42085664\n",
      "Iteration 7, loss = 0.41516716\n",
      "Iteration 8, loss = 0.40468593\n",
      "Iteration 9, loss = 0.38704149\n",
      "Iteration 10, loss = 0.38236854\n",
      "Iteration 11, loss = 0.38229220\n",
      "Iteration 12, loss = 0.37293459\n",
      "Iteration 13, loss = 0.36931830\n",
      "Iteration 14, loss = 0.36449946\n",
      "Iteration 15, loss = 0.35848828\n",
      "Iteration 16, loss = 0.35230280\n",
      "Iteration 17, loss = 0.34686321\n",
      "Iteration 18, loss = 0.34848594\n",
      "Iteration 19, loss = 0.34766500\n",
      "Iteration 20, loss = 0.34368248\n",
      "Iteration 21, loss = 0.34150848\n",
      "Iteration 22, loss = 0.33703814\n",
      "Iteration 23, loss = 0.33316554\n",
      "Iteration 24, loss = 0.33497179\n",
      "Iteration 25, loss = 0.33207391\n",
      "Iteration 26, loss = 0.32453458\n",
      "Iteration 27, loss = 0.32354251\n",
      "Iteration 28, loss = 0.32512018\n",
      "Iteration 29, loss = 0.32291626\n",
      "Iteration 30, loss = 0.32066990\n",
      "Iteration 31, loss = 0.32067598\n",
      "Iteration 32, loss = 0.31457036\n",
      "Iteration 33, loss = 0.31703218\n",
      "Iteration 34, loss = 0.31234113\n",
      "Iteration 35, loss = 0.31005637\n",
      "Iteration 36, loss = 0.31509375\n",
      "Iteration 37, loss = 0.30774613\n",
      "Iteration 38, loss = 0.30782012\n",
      "Iteration 39, loss = 0.30330849\n",
      "Iteration 40, loss = 0.30359161\n",
      "Iteration 41, loss = 0.30476322\n",
      "Iteration 42, loss = 0.30317311\n",
      "Iteration 43, loss = 0.30139837\n",
      "Iteration 44, loss = 0.30105469\n",
      "Iteration 45, loss = 0.29643064\n",
      "Iteration 46, loss = 0.30242177\n",
      "Iteration 47, loss = 0.29366835\n",
      "Iteration 48, loss = 0.29498720\n",
      "Iteration 49, loss = 0.29183807\n",
      "Iteration 50, loss = 0.29770689\n",
      "Iteration 51, loss = 0.29696089\n",
      "Iteration 52, loss = 0.29559505\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69114740\n",
      "Iteration 2, loss = 0.51622641\n",
      "Iteration 3, loss = 0.47724596\n",
      "Iteration 4, loss = 0.45783629\n",
      "Iteration 5, loss = 0.44017295\n",
      "Iteration 6, loss = 0.42588478\n",
      "Iteration 7, loss = 0.41448928\n",
      "Iteration 8, loss = 0.40126510\n",
      "Iteration 9, loss = 0.39167735\n",
      "Iteration 10, loss = 0.38604839\n",
      "Iteration 11, loss = 0.37658470\n",
      "Iteration 12, loss = 0.37543818\n",
      "Iteration 13, loss = 0.36969433\n",
      "Iteration 14, loss = 0.36439151\n",
      "Iteration 15, loss = 0.36239112\n",
      "Iteration 16, loss = 0.35352582\n",
      "Iteration 17, loss = 0.34910559\n",
      "Iteration 18, loss = 0.34580268\n",
      "Iteration 19, loss = 0.34111949\n",
      "Iteration 20, loss = 0.33971924\n",
      "Iteration 21, loss = 0.34170386\n",
      "Iteration 22, loss = 0.33996922\n",
      "Iteration 23, loss = 0.33127497\n",
      "Iteration 24, loss = 0.32838286\n",
      "Iteration 25, loss = 0.32854436\n",
      "Iteration 26, loss = 0.32483467\n",
      "Iteration 27, loss = 0.31632167\n",
      "Iteration 28, loss = 0.32536286\n",
      "Iteration 29, loss = 0.32003885\n",
      "Iteration 30, loss = 0.31449271\n",
      "Iteration 31, loss = 0.32168892\n",
      "Iteration 32, loss = 0.31189160\n",
      "Iteration 33, loss = 0.31149037\n",
      "Iteration 34, loss = 0.31312944\n",
      "Iteration 35, loss = 0.30871949\n",
      "Iteration 36, loss = 0.30615385\n",
      "Iteration 37, loss = 0.31074273\n",
      "Iteration 38, loss = 0.30145498\n",
      "Iteration 39, loss = 0.30477068\n",
      "Iteration 40, loss = 0.29958811\n",
      "Iteration 41, loss = 0.29596275\n",
      "Iteration 42, loss = 0.29584450\n",
      "Iteration 43, loss = 0.29618068\n",
      "Iteration 44, loss = 0.29618723\n",
      "Iteration 45, loss = 0.29443449\n",
      "Iteration 46, loss = 0.29367730\n",
      "Iteration 47, loss = 0.29828174\n",
      "Iteration 48, loss = 0.29207716\n",
      "Iteration 49, loss = 0.28955687\n",
      "Iteration 50, loss = 0.29029768\n",
      "Iteration 51, loss = 0.29031832\n",
      "Iteration 52, loss = 0.28899595\n",
      "Iteration 53, loss = 0.29066210\n",
      "Iteration 54, loss = 0.28756649\n",
      "Iteration 55, loss = 0.28516916\n",
      "Iteration 56, loss = 0.28949746\n",
      "Iteration 57, loss = 0.28700562\n",
      "Iteration 58, loss = 0.28282163\n",
      "Iteration 59, loss = 0.28615241\n",
      "Iteration 60, loss = 0.28786155\n",
      "Iteration 61, loss = 0.27848657\n",
      "Iteration 62, loss = 0.28386682\n",
      "Iteration 63, loss = 0.27499179\n",
      "Iteration 64, loss = 0.27849105\n",
      "Iteration 65, loss = 0.28473223\n",
      "Iteration 66, loss = 0.27870304\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70137328\n",
      "Iteration 2, loss = 0.54376681\n",
      "Iteration 3, loss = 0.50096869\n",
      "Iteration 4, loss = 0.47614658\n",
      "Iteration 5, loss = 0.45484851\n",
      "Iteration 6, loss = 0.43702556\n",
      "Iteration 7, loss = 0.42317424\n",
      "Iteration 8, loss = 0.41203371\n",
      "Iteration 9, loss = 0.40645043\n",
      "Iteration 10, loss = 0.39672911\n",
      "Iteration 11, loss = 0.38998211\n",
      "Iteration 12, loss = 0.38102424\n",
      "Iteration 13, loss = 0.37484290\n",
      "Iteration 14, loss = 0.37348296\n",
      "Iteration 15, loss = 0.36881294\n",
      "Iteration 16, loss = 0.36752237\n",
      "Iteration 17, loss = 0.35942195\n",
      "Iteration 18, loss = 0.35761059\n",
      "Iteration 19, loss = 0.35317312\n",
      "Iteration 20, loss = 0.35447385\n",
      "Iteration 21, loss = 0.34970558\n",
      "Iteration 22, loss = 0.34820862\n",
      "Iteration 23, loss = 0.34395982\n",
      "Iteration 24, loss = 0.34194337\n",
      "Iteration 25, loss = 0.34433431\n",
      "Iteration 26, loss = 0.33982058\n",
      "Iteration 27, loss = 0.33683543\n",
      "Iteration 28, loss = 0.33502648\n",
      "Iteration 29, loss = 0.33210564\n",
      "Iteration 30, loss = 0.33365549\n",
      "Iteration 31, loss = 0.33182342\n",
      "Iteration 32, loss = 0.32985526\n",
      "Iteration 33, loss = 0.32494326\n",
      "Iteration 34, loss = 0.32921312\n",
      "Iteration 35, loss = 0.32125087\n",
      "Iteration 36, loss = 0.31878269\n",
      "Iteration 37, loss = 0.32451180\n",
      "Iteration 38, loss = 0.31741274\n",
      "Iteration 39, loss = 0.31918727\n",
      "Iteration 40, loss = 0.31549052\n",
      "Iteration 41, loss = 0.31820360\n",
      "Iteration 42, loss = 0.32185738\n",
      "Iteration 43, loss = 0.31492013\n",
      "Iteration 44, loss = 0.31271080\n",
      "Iteration 45, loss = 0.31160014\n",
      "Iteration 46, loss = 0.31495159\n",
      "Iteration 47, loss = 0.31133363\n",
      "Iteration 48, loss = 0.31118183\n",
      "Iteration 49, loss = 0.30914423\n",
      "Iteration 50, loss = 0.30368094\n",
      "Iteration 51, loss = 0.30428284\n",
      "Iteration 52, loss = 0.30351695\n",
      "Iteration 53, loss = 0.30195896\n",
      "Iteration 54, loss = 0.30132588\n",
      "Iteration 55, loss = 0.30209557\n",
      "Iteration 56, loss = 0.29843946\n",
      "Iteration 57, loss = 0.30228639\n",
      "Iteration 58, loss = 0.30306033\n",
      "Iteration 59, loss = 0.29662105\n",
      "Iteration 60, loss = 0.30343068\n",
      "Iteration 61, loss = 0.29678469\n",
      "Iteration 62, loss = 0.30116154\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71946377\n",
      "Iteration 2, loss = 0.55367684\n",
      "Iteration 3, loss = 0.50629788\n",
      "Iteration 4, loss = 0.48176032\n",
      "Iteration 5, loss = 0.46147555\n",
      "Iteration 6, loss = 0.44689140\n",
      "Iteration 7, loss = 0.42681294\n",
      "Iteration 8, loss = 0.42155102\n",
      "Iteration 9, loss = 0.40743409\n",
      "Iteration 10, loss = 0.40235501\n",
      "Iteration 11, loss = 0.39143194\n",
      "Iteration 12, loss = 0.38690333\n",
      "Iteration 13, loss = 0.38493490\n",
      "Iteration 14, loss = 0.37502628\n",
      "Iteration 15, loss = 0.37047828\n",
      "Iteration 16, loss = 0.36610895\n",
      "Iteration 17, loss = 0.36500935\n",
      "Iteration 18, loss = 0.35884711\n",
      "Iteration 19, loss = 0.35959688\n",
      "Iteration 20, loss = 0.35268450\n",
      "Iteration 21, loss = 0.35172180\n",
      "Iteration 22, loss = 0.34993453\n",
      "Iteration 23, loss = 0.34870890\n",
      "Iteration 24, loss = 0.35013180\n",
      "Iteration 25, loss = 0.34342611\n",
      "Iteration 26, loss = 0.34010499\n",
      "Iteration 27, loss = 0.34082563\n",
      "Iteration 28, loss = 0.33315492\n",
      "Iteration 29, loss = 0.34371429\n",
      "Iteration 30, loss = 0.33507110\n",
      "Iteration 31, loss = 0.33346278\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70373624\n",
      "Iteration 2, loss = 0.55323461\n",
      "Iteration 3, loss = 0.50732944\n",
      "Iteration 4, loss = 0.47667872\n",
      "Iteration 5, loss = 0.46253731\n",
      "Iteration 6, loss = 0.44543341\n",
      "Iteration 7, loss = 0.42724899\n",
      "Iteration 8, loss = 0.42027594\n",
      "Iteration 9, loss = 0.40384771\n",
      "Iteration 10, loss = 0.39866765\n",
      "Iteration 11, loss = 0.39138877\n",
      "Iteration 12, loss = 0.38379099\n",
      "Iteration 13, loss = 0.37688404\n",
      "Iteration 14, loss = 0.37786008\n",
      "Iteration 15, loss = 0.37334717\n",
      "Iteration 16, loss = 0.36719917\n",
      "Iteration 17, loss = 0.36431170\n",
      "Iteration 18, loss = 0.35751590\n",
      "Iteration 19, loss = 0.35732685\n",
      "Iteration 20, loss = 0.35296930\n",
      "Iteration 21, loss = 0.35194817\n",
      "Iteration 22, loss = 0.34736625\n",
      "Iteration 23, loss = 0.34480903\n",
      "Iteration 24, loss = 0.34441667\n",
      "Iteration 25, loss = 0.34200742\n",
      "Iteration 26, loss = 0.33691673\n",
      "Iteration 27, loss = 0.33952568\n",
      "Iteration 28, loss = 0.33344392\n",
      "Iteration 29, loss = 0.33397607\n",
      "Iteration 30, loss = 0.33091560\n",
      "Iteration 31, loss = 0.33396852\n",
      "Iteration 32, loss = 0.33380182\n",
      "Iteration 33, loss = 0.33048240\n",
      "Iteration 34, loss = 0.32990048\n",
      "Iteration 35, loss = 0.32578579\n",
      "Iteration 36, loss = 0.32470397\n",
      "Iteration 37, loss = 0.32337694\n",
      "Iteration 38, loss = 0.32759633\n",
      "Iteration 39, loss = 0.32267410\n",
      "Iteration 40, loss = 0.32310359\n",
      "Iteration 41, loss = 0.32048099\n",
      "Iteration 42, loss = 0.31632321\n",
      "Iteration 43, loss = 0.31569632\n",
      "Iteration 44, loss = 0.31313408\n",
      "Iteration 45, loss = 0.30807607\n",
      "Iteration 46, loss = 0.31644647\n",
      "Iteration 47, loss = 0.31531788\n",
      "Iteration 48, loss = 0.30962496\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86488749\n",
      "Iteration 2, loss = 0.64462880\n",
      "Iteration 3, loss = 0.57872692\n",
      "Iteration 4, loss = 0.54109853\n",
      "Iteration 5, loss = 0.51207334\n",
      "Iteration 6, loss = 0.49344248\n",
      "Iteration 7, loss = 0.48232726\n",
      "Iteration 8, loss = 0.47576829\n",
      "Iteration 9, loss = 0.46919047\n",
      "Iteration 10, loss = 0.46473940\n",
      "Iteration 11, loss = 0.46675868\n",
      "Iteration 12, loss = 0.46262361\n",
      "Iteration 13, loss = 0.45893686\n",
      "Iteration 14, loss = 0.45892523\n",
      "Iteration 15, loss = 0.45174451\n",
      "Iteration 16, loss = 0.45195759\n",
      "Iteration 17, loss = 0.44469565\n",
      "Iteration 18, loss = 0.44685761\n",
      "Iteration 19, loss = 0.44763346\n",
      "Iteration 20, loss = 0.44796787\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86092829\n",
      "Iteration 2, loss = 0.64090244\n",
      "Iteration 3, loss = 0.58230845\n",
      "Iteration 4, loss = 0.53722077\n",
      "Iteration 5, loss = 0.50967461\n",
      "Iteration 6, loss = 0.50241579\n",
      "Iteration 7, loss = 0.48698180\n",
      "Iteration 8, loss = 0.48937856\n",
      "Iteration 9, loss = 0.47221304\n",
      "Iteration 10, loss = 0.47386113\n",
      "Iteration 11, loss = 0.46504586\n",
      "Iteration 12, loss = 0.46455349\n",
      "Iteration 13, loss = 0.46282431\n",
      "Iteration 14, loss = 0.45988016\n",
      "Iteration 15, loss = 0.45556676\n",
      "Iteration 16, loss = 0.45668973\n",
      "Iteration 17, loss = 0.45168197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.45229661\n",
      "Iteration 19, loss = 0.44788811\n",
      "Iteration 20, loss = 0.44770152\n",
      "Iteration 21, loss = 0.45191968\n",
      "Iteration 22, loss = 0.44617378\n",
      "Iteration 23, loss = 0.44477098\n",
      "Iteration 24, loss = 0.44154652\n",
      "Iteration 25, loss = 0.43894568\n",
      "Iteration 26, loss = 0.44514466\n",
      "Iteration 27, loss = 0.44170937\n",
      "Iteration 28, loss = 0.43875253\n",
      "Iteration 29, loss = 0.43968640\n",
      "Iteration 30, loss = 0.43649507\n",
      "Iteration 31, loss = 0.43972531\n",
      "Iteration 32, loss = 0.44177993\n",
      "Iteration 33, loss = 0.43427751\n",
      "Iteration 34, loss = 0.43259455\n",
      "Iteration 35, loss = 0.43804272\n",
      "Iteration 36, loss = 0.43395864\n",
      "Iteration 37, loss = 0.43340921\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86983638\n",
      "Iteration 2, loss = 0.65411379\n",
      "Iteration 3, loss = 0.58042225\n",
      "Iteration 4, loss = 0.54195452\n",
      "Iteration 5, loss = 0.51354551\n",
      "Iteration 6, loss = 0.50502574\n",
      "Iteration 7, loss = 0.48969637\n",
      "Iteration 8, loss = 0.48177388\n",
      "Iteration 9, loss = 0.47397345\n",
      "Iteration 10, loss = 0.46679844\n",
      "Iteration 11, loss = 0.46280329\n",
      "Iteration 12, loss = 0.46219824\n",
      "Iteration 13, loss = 0.46444638\n",
      "Iteration 14, loss = 0.45919510\n",
      "Iteration 15, loss = 0.45373409\n",
      "Iteration 16, loss = 0.45382729\n",
      "Iteration 17, loss = 0.45114594\n",
      "Iteration 18, loss = 0.44882173\n",
      "Iteration 19, loss = 0.44880673\n",
      "Iteration 20, loss = 0.44662346\n",
      "Iteration 21, loss = 0.45199456\n",
      "Iteration 22, loss = 0.44559114\n",
      "Iteration 23, loss = 0.44742882\n",
      "Iteration 24, loss = 0.44204506\n",
      "Iteration 25, loss = 0.44157717\n",
      "Iteration 26, loss = 0.44272217\n",
      "Iteration 27, loss = 0.43911915\n",
      "Iteration 28, loss = 0.44200791\n",
      "Iteration 29, loss = 0.43890718\n",
      "Iteration 30, loss = 0.43843869\n",
      "Iteration 31, loss = 0.44021288\n",
      "Iteration 32, loss = 0.43722354\n",
      "Iteration 33, loss = 0.43572493\n",
      "Iteration 34, loss = 0.43412498\n",
      "Iteration 35, loss = 0.43660364\n",
      "Iteration 36, loss = 0.43653457\n",
      "Iteration 37, loss = 0.43206794\n",
      "Iteration 38, loss = 0.43461732\n",
      "Iteration 39, loss = 0.43890883\n",
      "Iteration 40, loss = 0.43034036\n",
      "Iteration 41, loss = 0.43146545\n",
      "Iteration 42, loss = 0.43149500\n",
      "Iteration 43, loss = 0.43456871\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79498196\n",
      "Iteration 2, loss = 0.59622959\n",
      "Iteration 3, loss = 0.54331051\n",
      "Iteration 4, loss = 0.51506430\n",
      "Iteration 5, loss = 0.49245508\n",
      "Iteration 6, loss = 0.48127031\n",
      "Iteration 7, loss = 0.47266983\n",
      "Iteration 8, loss = 0.46228651\n",
      "Iteration 9, loss = 0.45798387\n",
      "Iteration 10, loss = 0.44958103\n",
      "Iteration 11, loss = 0.45044863\n",
      "Iteration 12, loss = 0.44901529\n",
      "Iteration 13, loss = 0.44251444\n",
      "Iteration 14, loss = 0.44478650\n",
      "Iteration 15, loss = 0.44002432\n",
      "Iteration 16, loss = 0.44065037\n",
      "Iteration 17, loss = 0.43789251\n",
      "Iteration 18, loss = 0.43644834\n",
      "Iteration 19, loss = 0.43801134\n",
      "Iteration 20, loss = 0.43134396\n",
      "Iteration 21, loss = 0.43244572\n",
      "Iteration 22, loss = 0.43353176\n",
      "Iteration 23, loss = 0.42899231\n",
      "Iteration 24, loss = 0.43350337\n",
      "Iteration 25, loss = 0.43086241\n",
      "Iteration 26, loss = 0.43069501\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79341416\n",
      "Iteration 2, loss = 0.59676851\n",
      "Iteration 3, loss = 0.54589459\n",
      "Iteration 4, loss = 0.51632134\n",
      "Iteration 5, loss = 0.50113854\n",
      "Iteration 6, loss = 0.48584977\n",
      "Iteration 7, loss = 0.47400777\n",
      "Iteration 8, loss = 0.46708032\n",
      "Iteration 9, loss = 0.46253207\n",
      "Iteration 10, loss = 0.45795273\n",
      "Iteration 11, loss = 0.45459621\n",
      "Iteration 12, loss = 0.45335667\n",
      "Iteration 13, loss = 0.44793653\n",
      "Iteration 14, loss = 0.44816577\n",
      "Iteration 15, loss = 0.44581443\n",
      "Iteration 16, loss = 0.43952025\n",
      "Iteration 17, loss = 0.44021787\n",
      "Iteration 18, loss = 0.43889723\n",
      "Iteration 19, loss = 0.43881313\n",
      "Iteration 20, loss = 0.43744056\n",
      "Iteration 21, loss = 0.43463050\n",
      "Iteration 22, loss = 0.44054860\n",
      "Iteration 23, loss = 0.43443968\n",
      "Iteration 24, loss = 0.43448253\n",
      "Iteration 25, loss = 0.43110548\n",
      "Iteration 26, loss = 0.43547116\n",
      "Iteration 27, loss = 0.43030033\n",
      "Iteration 28, loss = 0.43265566\n",
      "Iteration 29, loss = 0.43117641\n",
      "Iteration 30, loss = 0.43264131\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78753884\n",
      "Iteration 2, loss = 0.59290069\n",
      "Iteration 3, loss = 0.54533023\n",
      "Iteration 4, loss = 0.51636356\n",
      "Iteration 5, loss = 0.49573905\n",
      "Iteration 6, loss = 0.48708343\n",
      "Iteration 7, loss = 0.47362141\n",
      "Iteration 8, loss = 0.46643285\n",
      "Iteration 9, loss = 0.45883318\n",
      "Iteration 10, loss = 0.45606799\n",
      "Iteration 11, loss = 0.45582088\n",
      "Iteration 12, loss = 0.45133773\n",
      "Iteration 13, loss = 0.44627650\n",
      "Iteration 14, loss = 0.44900602\n",
      "Iteration 15, loss = 0.44727955\n",
      "Iteration 16, loss = 0.44064230\n",
      "Iteration 17, loss = 0.43878547\n",
      "Iteration 18, loss = 0.43500764\n",
      "Iteration 19, loss = 0.43935570\n",
      "Iteration 20, loss = 0.43686220\n",
      "Iteration 21, loss = 0.43473575\n",
      "Iteration 22, loss = 0.43510402\n",
      "Iteration 23, loss = 0.43614288\n",
      "Iteration 24, loss = 0.43713183\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.01078314\n",
      "Iteration 2, loss = 0.75051768\n",
      "Iteration 3, loss = 0.65074518\n",
      "Iteration 4, loss = 0.58765915\n",
      "Iteration 5, loss = 0.54905972\n",
      "Iteration 6, loss = 0.52334069\n",
      "Iteration 7, loss = 0.50605049\n",
      "Iteration 8, loss = 0.48618364\n",
      "Iteration 9, loss = 0.47790008\n",
      "Iteration 10, loss = 0.46709917\n",
      "Iteration 11, loss = 0.46258994\n",
      "Iteration 12, loss = 0.46118470\n",
      "Iteration 13, loss = 0.45141866\n",
      "Iteration 14, loss = 0.44264788\n",
      "Iteration 15, loss = 0.43896744\n",
      "Iteration 16, loss = 0.43900786\n",
      "Iteration 17, loss = 0.43761208\n",
      "Iteration 18, loss = 0.43430828\n",
      "Iteration 19, loss = 0.43046427\n",
      "Iteration 20, loss = 0.42628470\n",
      "Iteration 21, loss = 0.42548518\n",
      "Iteration 22, loss = 0.42101489\n",
      "Iteration 23, loss = 0.42040585\n",
      "Iteration 24, loss = 0.41799910\n",
      "Iteration 25, loss = 0.42265062\n",
      "Iteration 26, loss = 0.41351162\n",
      "Iteration 27, loss = 0.41116877\n",
      "Iteration 28, loss = 0.41795833\n",
      "Iteration 29, loss = 0.41641209\n",
      "Iteration 30, loss = 0.40495885\n",
      "Iteration 31, loss = 0.41269981\n",
      "Iteration 32, loss = 0.40863474\n",
      "Iteration 33, loss = 0.41360136\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.01943207\n",
      "Iteration 2, loss = 0.76142038\n",
      "Iteration 3, loss = 0.65654755\n",
      "Iteration 4, loss = 0.59486433\n",
      "Iteration 5, loss = 0.55469354\n",
      "Iteration 6, loss = 0.52589317\n",
      "Iteration 7, loss = 0.51096750\n",
      "Iteration 8, loss = 0.49513324\n",
      "Iteration 9, loss = 0.48641987\n",
      "Iteration 10, loss = 0.47336253\n",
      "Iteration 11, loss = 0.46138466\n",
      "Iteration 12, loss = 0.46020346\n",
      "Iteration 13, loss = 0.45260762\n",
      "Iteration 14, loss = 0.45336362\n",
      "Iteration 15, loss = 0.44854247\n",
      "Iteration 16, loss = 0.44309627\n",
      "Iteration 17, loss = 0.43669720\n",
      "Iteration 18, loss = 0.43361447\n",
      "Iteration 19, loss = 0.43456905\n",
      "Iteration 20, loss = 0.42828977\n",
      "Iteration 21, loss = 0.43122567\n",
      "Iteration 22, loss = 0.42378024\n",
      "Iteration 23, loss = 0.42335320\n",
      "Iteration 24, loss = 0.41707788\n",
      "Iteration 25, loss = 0.42180711\n",
      "Iteration 26, loss = 0.42141767\n",
      "Iteration 27, loss = 0.42288434\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.00525843\n",
      "Iteration 2, loss = 0.76487460\n",
      "Iteration 3, loss = 0.65251341\n",
      "Iteration 4, loss = 0.59269532\n",
      "Iteration 5, loss = 0.55312913\n",
      "Iteration 6, loss = 0.52715413\n",
      "Iteration 7, loss = 0.50567083\n",
      "Iteration 8, loss = 0.49426893\n",
      "Iteration 9, loss = 0.48281938\n",
      "Iteration 10, loss = 0.47082198\n",
      "Iteration 11, loss = 0.46190291\n",
      "Iteration 12, loss = 0.46264352\n",
      "Iteration 13, loss = 0.45746119\n",
      "Iteration 14, loss = 0.45714565\n",
      "Iteration 15, loss = 0.43932197\n",
      "Iteration 16, loss = 0.44179081\n",
      "Iteration 17, loss = 0.43704203\n",
      "Iteration 18, loss = 0.43154437\n",
      "Iteration 19, loss = 0.43626128\n",
      "Iteration 20, loss = 0.43033865\n",
      "Iteration 21, loss = 0.42437847\n",
      "Iteration 22, loss = 0.42403117\n",
      "Iteration 23, loss = 0.42571935\n",
      "Iteration 24, loss = 0.41980223\n",
      "Iteration 25, loss = 0.42232615\n",
      "Iteration 26, loss = 0.41093633\n",
      "Iteration 27, loss = 0.41761889\n",
      "Iteration 28, loss = 0.41079238\n",
      "Iteration 29, loss = 0.41187679\n",
      "Iteration 30, loss = 0.40649981\n",
      "Iteration 31, loss = 0.41011388\n",
      "Iteration 32, loss = 0.41180365\n",
      "Iteration 33, loss = 0.40699784\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.86807904\n",
      "Iteration 2, loss = 0.64680570\n",
      "Iteration 3, loss = 0.58745692\n",
      "Iteration 4, loss = 0.55208289\n",
      "Iteration 5, loss = 0.52466914\n",
      "Iteration 6, loss = 0.50381218\n",
      "Iteration 7, loss = 0.48707094\n",
      "Iteration 8, loss = 0.47888216\n",
      "Iteration 9, loss = 0.46498604\n",
      "Iteration 10, loss = 0.46194887\n",
      "Iteration 11, loss = 0.45307992\n",
      "Iteration 12, loss = 0.45308159\n",
      "Iteration 13, loss = 0.44721265\n",
      "Iteration 14, loss = 0.44382860\n",
      "Iteration 15, loss = 0.43378112\n",
      "Iteration 16, loss = 0.43695060\n",
      "Iteration 17, loss = 0.43134568\n",
      "Iteration 18, loss = 0.42819672\n",
      "Iteration 19, loss = 0.43285234\n",
      "Iteration 20, loss = 0.42638654\n",
      "Iteration 21, loss = 0.41849309\n",
      "Iteration 22, loss = 0.41901566\n",
      "Iteration 23, loss = 0.41838364\n",
      "Iteration 24, loss = 0.42136818\n",
      "Iteration 25, loss = 0.41676376\n",
      "Iteration 26, loss = 0.41394407\n",
      "Iteration 27, loss = 0.41229845\n",
      "Iteration 28, loss = 0.41337098\n",
      "Iteration 29, loss = 0.40780402\n",
      "Iteration 30, loss = 0.41217843\n",
      "Iteration 31, loss = 0.40500410\n",
      "Iteration 32, loss = 0.41158923\n",
      "Iteration 33, loss = 0.40724888\n",
      "Iteration 34, loss = 0.40819865\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86538268\n",
      "Iteration 2, loss = 0.65101457\n",
      "Iteration 3, loss = 0.58956349\n",
      "Iteration 4, loss = 0.55098025\n",
      "Iteration 5, loss = 0.52536840\n",
      "Iteration 6, loss = 0.50145661\n",
      "Iteration 7, loss = 0.49501697\n",
      "Iteration 8, loss = 0.48347699\n",
      "Iteration 9, loss = 0.46514671\n",
      "Iteration 10, loss = 0.45940216\n",
      "Iteration 11, loss = 0.45970457\n",
      "Iteration 12, loss = 0.45083378\n",
      "Iteration 13, loss = 0.44956835\n",
      "Iteration 14, loss = 0.44478684\n",
      "Iteration 15, loss = 0.44046848\n",
      "Iteration 16, loss = 0.43508007\n",
      "Iteration 17, loss = 0.42924447\n",
      "Iteration 18, loss = 0.43171844\n",
      "Iteration 19, loss = 0.42854537\n",
      "Iteration 20, loss = 0.42683054\n",
      "Iteration 21, loss = 0.42644514\n",
      "Iteration 22, loss = 0.42301720\n",
      "Iteration 23, loss = 0.42086278\n",
      "Iteration 24, loss = 0.42182299\n",
      "Iteration 25, loss = 0.41960440\n",
      "Iteration 26, loss = 0.41394126\n",
      "Iteration 27, loss = 0.41247429\n",
      "Iteration 28, loss = 0.41486349\n",
      "Iteration 29, loss = 0.41246269\n",
      "Iteration 30, loss = 0.41176817\n",
      "Iteration 31, loss = 0.40881536\n",
      "Iteration 32, loss = 0.40914870\n",
      "Iteration 33, loss = 0.41084977\n",
      "Iteration 34, loss = 0.40858688\n",
      "Iteration 35, loss = 0.40514280\n",
      "Iteration 36, loss = 0.40849121\n",
      "Iteration 37, loss = 0.40234420\n",
      "Iteration 38, loss = 0.40375475\n",
      "Iteration 39, loss = 0.40331693\n",
      "Iteration 40, loss = 0.40243145\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86741161\n",
      "Iteration 2, loss = 0.65323491\n",
      "Iteration 3, loss = 0.58938651\n",
      "Iteration 4, loss = 0.55591262\n",
      "Iteration 5, loss = 0.52880740\n",
      "Iteration 6, loss = 0.50816177\n",
      "Iteration 7, loss = 0.49277235\n",
      "Iteration 8, loss = 0.47960052\n",
      "Iteration 9, loss = 0.46683682\n",
      "Iteration 10, loss = 0.46219204\n",
      "Iteration 11, loss = 0.45351443\n",
      "Iteration 12, loss = 0.45152720\n",
      "Iteration 13, loss = 0.44688047\n",
      "Iteration 14, loss = 0.44383630\n",
      "Iteration 15, loss = 0.44142176\n",
      "Iteration 16, loss = 0.43288451\n",
      "Iteration 17, loss = 0.43039635\n",
      "Iteration 18, loss = 0.42929115\n",
      "Iteration 19, loss = 0.42361262\n",
      "Iteration 20, loss = 0.42269053\n",
      "Iteration 21, loss = 0.42679732\n",
      "Iteration 22, loss = 0.42497063\n",
      "Iteration 23, loss = 0.41825756\n",
      "Iteration 24, loss = 0.41539696\n",
      "Iteration 25, loss = 0.41501482\n",
      "Iteration 26, loss = 0.41399793\n",
      "Iteration 27, loss = 0.40718693\n",
      "Iteration 28, loss = 0.41413899\n",
      "Iteration 29, loss = 0.41123845\n",
      "Iteration 30, loss = 0.40624820\n",
      "Iteration 31, loss = 0.40879721\n",
      "Iteration 32, loss = 0.40874223\n",
      "Iteration 33, loss = 0.40845687\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.95198700\n",
      "Iteration 2, loss = 0.71359789\n",
      "Iteration 3, loss = 0.63099455\n",
      "Iteration 4, loss = 0.58007037\n",
      "Iteration 5, loss = 0.54675127\n",
      "Iteration 6, loss = 0.51833620\n",
      "Iteration 7, loss = 0.50133378\n",
      "Iteration 8, loss = 0.48845431\n",
      "Iteration 9, loss = 0.47940310\n",
      "Iteration 10, loss = 0.47007289\n",
      "Iteration 11, loss = 0.46686039\n",
      "Iteration 12, loss = 0.45754467\n",
      "Iteration 13, loss = 0.45423801\n",
      "Iteration 14, loss = 0.45164670\n",
      "Iteration 15, loss = 0.44822249\n",
      "Iteration 16, loss = 0.44325272\n",
      "Iteration 17, loss = 0.44089426\n",
      "Iteration 18, loss = 0.43911946\n",
      "Iteration 19, loss = 0.43683482\n",
      "Iteration 20, loss = 0.43519135\n",
      "Iteration 21, loss = 0.43088819\n",
      "Iteration 22, loss = 0.42796145\n",
      "Iteration 23, loss = 0.42647323\n",
      "Iteration 24, loss = 0.42531271\n",
      "Iteration 25, loss = 0.42649173\n",
      "Iteration 26, loss = 0.42171246\n",
      "Iteration 27, loss = 0.41886481\n",
      "Iteration 28, loss = 0.41794131\n",
      "Iteration 29, loss = 0.41730225\n",
      "Iteration 30, loss = 0.41750274\n",
      "Iteration 31, loss = 0.41376258\n",
      "Iteration 32, loss = 0.41282394\n",
      "Iteration 33, loss = 0.40998616\n",
      "Iteration 34, loss = 0.41125738\n",
      "Iteration 35, loss = 0.40654730\n",
      "Iteration 36, loss = 0.40476326\n",
      "Iteration 37, loss = 0.41036906\n",
      "Iteration 38, loss = 0.40491198\n",
      "Iteration 39, loss = 0.40436655\n",
      "Iteration 40, loss = 0.40438725\n",
      "Iteration 41, loss = 0.40821656\n",
      "Iteration 42, loss = 0.40633049\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.97146557\n",
      "Iteration 2, loss = 0.72834651\n",
      "Iteration 3, loss = 0.63640927\n",
      "Iteration 4, loss = 0.58757718\n",
      "Iteration 5, loss = 0.55402851\n",
      "Iteration 6, loss = 0.52496931\n",
      "Iteration 7, loss = 0.50753460\n",
      "Iteration 8, loss = 0.49826314\n",
      "Iteration 9, loss = 0.48248644\n",
      "Iteration 10, loss = 0.47457289\n",
      "Iteration 11, loss = 0.46787914\n",
      "Iteration 12, loss = 0.46297699\n",
      "Iteration 13, loss = 0.46194611\n",
      "Iteration 14, loss = 0.45544148\n",
      "Iteration 15, loss = 0.44936886\n",
      "Iteration 16, loss = 0.44578614\n",
      "Iteration 17, loss = 0.44437796\n",
      "Iteration 18, loss = 0.43987533\n",
      "Iteration 19, loss = 0.43953344\n",
      "Iteration 20, loss = 0.43681965\n",
      "Iteration 21, loss = 0.43162800\n",
      "Iteration 22, loss = 0.43359416\n",
      "Iteration 23, loss = 0.43122386\n",
      "Iteration 24, loss = 0.43056990\n",
      "Iteration 25, loss = 0.42820740\n",
      "Iteration 26, loss = 0.42373656\n",
      "Iteration 27, loss = 0.42110644\n",
      "Iteration 28, loss = 0.41874395\n",
      "Iteration 29, loss = 0.42543986\n",
      "Iteration 30, loss = 0.42066463\n",
      "Iteration 31, loss = 0.41411285\n",
      "Iteration 32, loss = 0.41472779\n",
      "Iteration 33, loss = 0.41549406\n",
      "Iteration 34, loss = 0.41163355\n",
      "Iteration 35, loss = 0.41595782\n",
      "Iteration 36, loss = 0.41281196\n",
      "Iteration 37, loss = 0.40618635\n",
      "Iteration 38, loss = 0.40857470\n",
      "Iteration 39, loss = 0.41398247\n",
      "Iteration 40, loss = 0.40688913\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.95326090\n",
      "Iteration 2, loss = 0.72555287\n",
      "Iteration 3, loss = 0.63509944\n",
      "Iteration 4, loss = 0.58022184\n",
      "Iteration 5, loss = 0.55076447\n",
      "Iteration 6, loss = 0.52543439\n",
      "Iteration 7, loss = 0.50403398\n",
      "Iteration 8, loss = 0.49352623\n",
      "Iteration 9, loss = 0.48067943\n",
      "Iteration 10, loss = 0.47342977\n",
      "Iteration 11, loss = 0.46708734\n",
      "Iteration 12, loss = 0.46201052\n",
      "Iteration 13, loss = 0.45634235\n",
      "Iteration 14, loss = 0.45629393\n",
      "Iteration 15, loss = 0.45321632\n",
      "Iteration 16, loss = 0.44650651\n",
      "Iteration 17, loss = 0.44661297\n",
      "Iteration 18, loss = 0.44032613\n",
      "Iteration 19, loss = 0.43993643\n",
      "Iteration 20, loss = 0.43813656\n",
      "Iteration 21, loss = 0.43403980\n",
      "Iteration 22, loss = 0.42970017\n",
      "Iteration 23, loss = 0.43056743\n",
      "Iteration 24, loss = 0.42987668\n",
      "Iteration 25, loss = 0.42874136\n",
      "Iteration 26, loss = 0.42089085\n",
      "Iteration 27, loss = 0.42578152\n",
      "Iteration 28, loss = 0.42036004\n",
      "Iteration 29, loss = 0.42033632\n",
      "Iteration 30, loss = 0.41875810\n",
      "Iteration 31, loss = 0.41783134\n",
      "Iteration 32, loss = 0.42073763\n",
      "Iteration 33, loss = 0.41530872\n",
      "Iteration 34, loss = 0.41555142\n",
      "Iteration 35, loss = 0.41290234\n",
      "Iteration 36, loss = 0.41263234\n",
      "Iteration 37, loss = 0.40918298\n",
      "Iteration 38, loss = 0.41309176\n",
      "Iteration 39, loss = 0.40858691\n",
      "Iteration 40, loss = 0.41423975\n",
      "Iteration 41, loss = 0.40734356\n",
      "Iteration 42, loss = 0.40552455\n",
      "Iteration 43, loss = 0.40491402\n",
      "Iteration 44, loss = 0.40397465\n",
      "Iteration 45, loss = 0.40333330\n",
      "Iteration 46, loss = 0.40762190\n",
      "Iteration 47, loss = 0.40495356\n",
      "Iteration 48, loss = 0.40462059\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.07295204\n",
      "Iteration 2, loss = 0.73713218\n",
      "Iteration 3, loss = 0.63618429\n",
      "Iteration 4, loss = 0.58598322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.55734844\n",
      "Iteration 6, loss = 0.54011015\n",
      "Iteration 7, loss = 0.53359501\n",
      "Iteration 8, loss = 0.53136554\n",
      "Iteration 9, loss = 0.52456342\n",
      "Iteration 10, loss = 0.52458285\n",
      "Iteration 11, loss = 0.52717725\n",
      "Iteration 12, loss = 0.52348189\n",
      "Iteration 13, loss = 0.52150178\n",
      "Iteration 14, loss = 0.52131818\n",
      "Iteration 15, loss = 0.51487605\n",
      "Iteration 16, loss = 0.51346979\n",
      "Iteration 17, loss = 0.50897488\n",
      "Iteration 18, loss = 0.51083728\n",
      "Iteration 19, loss = 0.51341766\n",
      "Iteration 20, loss = 0.51260758\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.06656454\n",
      "Iteration 2, loss = 0.72843256\n",
      "Iteration 3, loss = 0.63705347\n",
      "Iteration 4, loss = 0.58311347\n",
      "Iteration 5, loss = 0.55514463\n",
      "Iteration 6, loss = 0.55189882\n",
      "Iteration 7, loss = 0.53733751\n",
      "Iteration 8, loss = 0.54459663\n",
      "Iteration 9, loss = 0.52676380\n",
      "Iteration 10, loss = 0.53284133\n",
      "Iteration 11, loss = 0.52412923\n",
      "Iteration 12, loss = 0.52499314\n",
      "Iteration 13, loss = 0.52296937\n",
      "Iteration 14, loss = 0.52196583\n",
      "Iteration 15, loss = 0.51798267\n",
      "Iteration 16, loss = 0.51888048\n",
      "Iteration 17, loss = 0.51577948\n",
      "Iteration 18, loss = 0.51701873\n",
      "Iteration 19, loss = 0.51241142\n",
      "Iteration 20, loss = 0.51207069\n",
      "Iteration 21, loss = 0.51624950\n",
      "Iteration 22, loss = 0.51130361\n",
      "Iteration 23, loss = 0.51129219\n",
      "Iteration 24, loss = 0.50612918\n",
      "Iteration 25, loss = 0.50517343\n",
      "Iteration 26, loss = 0.50997039\n",
      "Iteration 27, loss = 0.50747635\n",
      "Iteration 28, loss = 0.50521982\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08078058\n",
      "Iteration 2, loss = 0.74635832\n",
      "Iteration 3, loss = 0.63611055\n",
      "Iteration 4, loss = 0.58630689\n",
      "Iteration 5, loss = 0.55760531\n",
      "Iteration 6, loss = 0.55047328\n",
      "Iteration 7, loss = 0.53940055\n",
      "Iteration 8, loss = 0.53643408\n",
      "Iteration 9, loss = 0.52886553\n",
      "Iteration 10, loss = 0.52494659\n",
      "Iteration 11, loss = 0.52140028\n",
      "Iteration 12, loss = 0.52148832\n",
      "Iteration 13, loss = 0.52469287\n",
      "Iteration 14, loss = 0.52022671\n",
      "Iteration 15, loss = 0.51623346\n",
      "Iteration 16, loss = 0.51704659\n",
      "Iteration 17, loss = 0.51458325\n",
      "Iteration 18, loss = 0.51242653\n",
      "Iteration 19, loss = 0.51226605\n",
      "Iteration 20, loss = 0.51067769\n",
      "Iteration 21, loss = 0.51547747\n",
      "Iteration 22, loss = 0.51047918\n",
      "Iteration 23, loss = 0.51170723\n",
      "Iteration 24, loss = 0.50681482\n",
      "Iteration 25, loss = 0.50598509\n",
      "Iteration 26, loss = 0.50845958\n",
      "Iteration 27, loss = 0.50355483\n",
      "Iteration 28, loss = 0.50694876\n",
      "Iteration 29, loss = 0.50421876\n",
      "Iteration 30, loss = 0.50474736\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93807717\n",
      "Iteration 2, loss = 0.68079180\n",
      "Iteration 3, loss = 0.60635182\n",
      "Iteration 4, loss = 0.56982247\n",
      "Iteration 5, loss = 0.54540526\n",
      "Iteration 6, loss = 0.53438499\n",
      "Iteration 7, loss = 0.52723834\n",
      "Iteration 8, loss = 0.51752590\n",
      "Iteration 9, loss = 0.51627280\n",
      "Iteration 10, loss = 0.50872811\n",
      "Iteration 11, loss = 0.51151134\n",
      "Iteration 12, loss = 0.51105987\n",
      "Iteration 13, loss = 0.50520663\n",
      "Iteration 14, loss = 0.50757530\n",
      "Iteration 15, loss = 0.50475544\n",
      "Iteration 16, loss = 0.50696499\n",
      "Iteration 17, loss = 0.50410281\n",
      "Iteration 18, loss = 0.50319549\n",
      "Iteration 19, loss = 0.50488096\n",
      "Iteration 20, loss = 0.49895261\n",
      "Iteration 21, loss = 0.50040542\n",
      "Iteration 22, loss = 0.50308255\n",
      "Iteration 23, loss = 0.49858644\n",
      "Iteration 24, loss = 0.50274878\n",
      "Iteration 25, loss = 0.49903288\n",
      "Iteration 26, loss = 0.50061502\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93616299\n",
      "Iteration 2, loss = 0.68070865\n",
      "Iteration 3, loss = 0.60851356\n",
      "Iteration 4, loss = 0.57061034\n",
      "Iteration 5, loss = 0.55307512\n",
      "Iteration 6, loss = 0.53951010\n",
      "Iteration 7, loss = 0.52881037\n",
      "Iteration 8, loss = 0.52152645\n",
      "Iteration 9, loss = 0.52127782\n",
      "Iteration 10, loss = 0.51728248\n",
      "Iteration 11, loss = 0.51493031\n",
      "Iteration 12, loss = 0.51466583\n",
      "Iteration 13, loss = 0.51018184\n",
      "Iteration 14, loss = 0.51077508\n",
      "Iteration 15, loss = 0.51058561\n",
      "Iteration 16, loss = 0.50587071\n",
      "Iteration 17, loss = 0.50653763\n",
      "Iteration 18, loss = 0.50682149\n",
      "Iteration 19, loss = 0.50550564\n",
      "Iteration 20, loss = 0.50502461\n",
      "Iteration 21, loss = 0.50272649\n",
      "Iteration 22, loss = 0.50978293\n",
      "Iteration 23, loss = 0.50276571\n",
      "Iteration 24, loss = 0.50433285\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.92929781\n",
      "Iteration 2, loss = 0.67483588\n",
      "Iteration 3, loss = 0.60602015\n",
      "Iteration 4, loss = 0.56829432\n",
      "Iteration 5, loss = 0.54618390\n",
      "Iteration 6, loss = 0.53905426\n",
      "Iteration 7, loss = 0.52698102\n",
      "Iteration 8, loss = 0.52210421\n",
      "Iteration 9, loss = 0.51576427\n",
      "Iteration 10, loss = 0.51452331\n",
      "Iteration 11, loss = 0.51749054\n",
      "Iteration 12, loss = 0.51142953\n",
      "Iteration 13, loss = 0.50909545\n",
      "Iteration 14, loss = 0.51352482\n",
      "Iteration 15, loss = 0.50922390\n",
      "Iteration 16, loss = 0.50584523\n",
      "Iteration 17, loss = 0.50501798\n",
      "Iteration 18, loss = 0.50160488\n",
      "Iteration 19, loss = 0.50739389\n",
      "Iteration 20, loss = 0.50344619\n",
      "Iteration 21, loss = 0.50148978\n",
      "Iteration 22, loss = 0.50272833\n",
      "Iteration 23, loss = 0.50445676\n",
      "Iteration 24, loss = 0.50582775\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34918202\n",
      "Iteration 2, loss = 0.87512333\n",
      "Iteration 3, loss = 0.70764632\n",
      "Iteration 4, loss = 0.62496054\n",
      "Iteration 5, loss = 0.58269075\n",
      "Iteration 6, loss = 0.56116509\n",
      "Iteration 7, loss = 0.54723704\n",
      "Iteration 8, loss = 0.53255241\n",
      "Iteration 9, loss = 0.52603135\n",
      "Iteration 10, loss = 0.51913496\n",
      "Iteration 11, loss = 0.51601037\n",
      "Iteration 12, loss = 0.51738734\n",
      "Iteration 13, loss = 0.50870980\n",
      "Iteration 14, loss = 0.50501693\n",
      "Iteration 15, loss = 0.50314189\n",
      "Iteration 16, loss = 0.50228706\n",
      "Iteration 17, loss = 0.50116353\n",
      "Iteration 18, loss = 0.50398059\n",
      "Iteration 19, loss = 0.50045201\n",
      "Iteration 20, loss = 0.49430629\n",
      "Iteration 21, loss = 0.49629651\n",
      "Iteration 22, loss = 0.49203475\n",
      "Iteration 23, loss = 0.49180107\n",
      "Iteration 24, loss = 0.49192572\n",
      "Iteration 25, loss = 0.49364569\n",
      "Iteration 26, loss = 0.48905678\n",
      "Iteration 27, loss = 0.48743799\n",
      "Iteration 28, loss = 0.49204668\n",
      "Iteration 29, loss = 0.49108797\n",
      "Iteration 30, loss = 0.48168785\n",
      "Iteration 31, loss = 0.48937014\n",
      "Iteration 32, loss = 0.48325145\n",
      "Iteration 33, loss = 0.49236678\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35809891\n",
      "Iteration 2, loss = 0.89003265\n",
      "Iteration 3, loss = 0.71433647\n",
      "Iteration 4, loss = 0.63258447\n",
      "Iteration 5, loss = 0.58576884\n",
      "Iteration 6, loss = 0.56204674\n",
      "Iteration 7, loss = 0.55089386\n",
      "Iteration 8, loss = 0.54276259\n",
      "Iteration 9, loss = 0.53384043\n",
      "Iteration 10, loss = 0.52546061\n",
      "Iteration 11, loss = 0.51685570\n",
      "Iteration 12, loss = 0.51838339\n",
      "Iteration 13, loss = 0.51308864\n",
      "Iteration 14, loss = 0.51316422\n",
      "Iteration 15, loss = 0.50995272\n",
      "Iteration 16, loss = 0.50786082\n",
      "Iteration 17, loss = 0.50281521\n",
      "Iteration 18, loss = 0.50210586\n",
      "Iteration 19, loss = 0.50460916\n",
      "Iteration 20, loss = 0.49823658\n",
      "Iteration 21, loss = 0.49950121\n",
      "Iteration 22, loss = 0.49551391\n",
      "Iteration 23, loss = 0.49851605\n",
      "Iteration 24, loss = 0.49017455\n",
      "Iteration 25, loss = 0.49590270\n",
      "Iteration 26, loss = 0.49505863\n",
      "Iteration 27, loss = 0.49706879\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34045302\n",
      "Iteration 2, loss = 0.88685894\n",
      "Iteration 3, loss = 0.70700564\n",
      "Iteration 4, loss = 0.62864452\n",
      "Iteration 5, loss = 0.58482239\n",
      "Iteration 6, loss = 0.56510264\n",
      "Iteration 7, loss = 0.54747917\n",
      "Iteration 8, loss = 0.53967009\n",
      "Iteration 9, loss = 0.53100608\n",
      "Iteration 10, loss = 0.52401315\n",
      "Iteration 11, loss = 0.51538205\n",
      "Iteration 12, loss = 0.51821953\n",
      "Iteration 13, loss = 0.51592660\n",
      "Iteration 14, loss = 0.51535323\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.09024860\n",
      "Iteration 2, loss = 0.76965926\n",
      "Iteration 3, loss = 0.66722149\n",
      "Iteration 4, loss = 0.61266699\n",
      "Iteration 5, loss = 0.58059204\n",
      "Iteration 6, loss = 0.55856458\n",
      "Iteration 7, loss = 0.54074805\n",
      "Iteration 8, loss = 0.53733476\n",
      "Iteration 9, loss = 0.52271914\n",
      "Iteration 10, loss = 0.52234701\n",
      "Iteration 11, loss = 0.51529418\n",
      "Iteration 12, loss = 0.51651086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.51329197\n",
      "Iteration 14, loss = 0.50972804\n",
      "Iteration 15, loss = 0.49933209\n",
      "Iteration 16, loss = 0.50499389\n",
      "Iteration 17, loss = 0.50033580\n",
      "Iteration 18, loss = 0.49576616\n",
      "Iteration 19, loss = 0.50188113\n",
      "Iteration 20, loss = 0.49656018\n",
      "Iteration 21, loss = 0.49076424\n",
      "Iteration 22, loss = 0.49023801\n",
      "Iteration 23, loss = 0.49137406\n",
      "Iteration 24, loss = 0.49401602\n",
      "Iteration 25, loss = 0.49012157\n",
      "Iteration 26, loss = 0.48728822\n",
      "Iteration 27, loss = 0.48676825\n",
      "Iteration 28, loss = 0.48929347\n",
      "Iteration 29, loss = 0.48392311\n",
      "Iteration 30, loss = 0.48909596\n",
      "Iteration 31, loss = 0.48126752\n",
      "Iteration 32, loss = 0.48728370\n",
      "Iteration 33, loss = 0.48703357\n",
      "Iteration 34, loss = 0.48495852\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08536830\n",
      "Iteration 2, loss = 0.77044374\n",
      "Iteration 3, loss = 0.66699489\n",
      "Iteration 4, loss = 0.61090317\n",
      "Iteration 5, loss = 0.57895061\n",
      "Iteration 6, loss = 0.55334995\n",
      "Iteration 7, loss = 0.54818259\n",
      "Iteration 8, loss = 0.53835463\n",
      "Iteration 9, loss = 0.52345229\n",
      "Iteration 10, loss = 0.51714322\n",
      "Iteration 11, loss = 0.51934706\n",
      "Iteration 12, loss = 0.51452407\n",
      "Iteration 13, loss = 0.51599954\n",
      "Iteration 14, loss = 0.51154269\n",
      "Iteration 15, loss = 0.50756299\n",
      "Iteration 16, loss = 0.50445199\n",
      "Iteration 17, loss = 0.49850268\n",
      "Iteration 18, loss = 0.50050385\n",
      "Iteration 19, loss = 0.49851466\n",
      "Iteration 20, loss = 0.49633137\n",
      "Iteration 21, loss = 0.49817119\n",
      "Iteration 22, loss = 0.49546768\n",
      "Iteration 23, loss = 0.49228295\n",
      "Iteration 24, loss = 0.49474515\n",
      "Iteration 25, loss = 0.49482750\n",
      "Iteration 26, loss = 0.48938944\n",
      "Iteration 27, loss = 0.48862609\n",
      "Iteration 28, loss = 0.48884108\n",
      "Iteration 29, loss = 0.48938364\n",
      "Iteration 30, loss = 0.48757292\n",
      "Iteration 31, loss = 0.48582947\n",
      "Iteration 32, loss = 0.48735991\n",
      "Iteration 33, loss = 0.48839174\n",
      "Iteration 34, loss = 0.48725386\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08846751\n",
      "Iteration 2, loss = 0.77460374\n",
      "Iteration 3, loss = 0.66679072\n",
      "Iteration 4, loss = 0.61612679\n",
      "Iteration 5, loss = 0.58295151\n",
      "Iteration 6, loss = 0.56159803\n",
      "Iteration 7, loss = 0.54641857\n",
      "Iteration 8, loss = 0.53474557\n",
      "Iteration 9, loss = 0.52467958\n",
      "Iteration 10, loss = 0.52084140\n",
      "Iteration 11, loss = 0.51460610\n",
      "Iteration 12, loss = 0.51320450\n",
      "Iteration 13, loss = 0.51053464\n",
      "Iteration 14, loss = 0.50829695\n",
      "Iteration 15, loss = 0.50921298\n",
      "Iteration 16, loss = 0.50019242\n",
      "Iteration 17, loss = 0.49804667\n",
      "Iteration 18, loss = 0.49905680\n",
      "Iteration 19, loss = 0.49538603\n",
      "Iteration 20, loss = 0.49674767\n",
      "Iteration 21, loss = 0.50057656\n",
      "Iteration 22, loss = 0.49949841\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.25170194\n",
      "Iteration 2, loss = 0.84199234\n",
      "Iteration 3, loss = 0.70369730\n",
      "Iteration 4, loss = 0.62989088\n",
      "Iteration 5, loss = 0.59342471\n",
      "Iteration 6, loss = 0.56472483\n",
      "Iteration 7, loss = 0.54865789\n",
      "Iteration 8, loss = 0.54124536\n",
      "Iteration 9, loss = 0.53020960\n",
      "Iteration 10, loss = 0.52434202\n",
      "Iteration 11, loss = 0.52487916\n",
      "Iteration 12, loss = 0.51649022\n",
      "Iteration 13, loss = 0.51543562\n",
      "Iteration 14, loss = 0.51328442\n",
      "Iteration 15, loss = 0.51064501\n",
      "Iteration 16, loss = 0.50517980\n",
      "Iteration 17, loss = 0.50492963\n",
      "Iteration 18, loss = 0.50271515\n",
      "Iteration 19, loss = 0.50165077\n",
      "Iteration 20, loss = 0.50071178\n",
      "Iteration 21, loss = 0.49544012\n",
      "Iteration 22, loss = 0.49493860\n",
      "Iteration 23, loss = 0.49506535\n",
      "Iteration 24, loss = 0.49400773\n",
      "Iteration 25, loss = 0.49626182\n",
      "Iteration 26, loss = 0.49156730\n",
      "Iteration 27, loss = 0.48967639\n",
      "Iteration 28, loss = 0.48967916\n",
      "Iteration 29, loss = 0.48710206\n",
      "Iteration 30, loss = 0.49070334\n",
      "Iteration 31, loss = 0.48631728\n",
      "Iteration 32, loss = 0.48595265\n",
      "Iteration 33, loss = 0.48589962\n",
      "Iteration 34, loss = 0.48662891\n",
      "Iteration 35, loss = 0.48287326\n",
      "Iteration 36, loss = 0.48128626\n",
      "Iteration 37, loss = 0.48360052\n",
      "Iteration 38, loss = 0.48142251\n",
      "Iteration 39, loss = 0.48236706\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.27639287\n",
      "Iteration 2, loss = 0.86367147\n",
      "Iteration 3, loss = 0.70774231\n",
      "Iteration 4, loss = 0.63838293\n",
      "Iteration 5, loss = 0.59963193\n",
      "Iteration 6, loss = 0.56822800\n",
      "Iteration 7, loss = 0.55625854\n",
      "Iteration 8, loss = 0.55056065\n",
      "Iteration 9, loss = 0.53704237\n",
      "Iteration 10, loss = 0.53192549\n",
      "Iteration 11, loss = 0.52820361\n",
      "Iteration 12, loss = 0.52213259\n",
      "Iteration 13, loss = 0.52228664\n",
      "Iteration 14, loss = 0.52057141\n",
      "Iteration 15, loss = 0.51387865\n",
      "Iteration 16, loss = 0.50961422\n",
      "Iteration 17, loss = 0.50871725\n",
      "Iteration 18, loss = 0.50513793\n",
      "Iteration 19, loss = 0.50532257\n",
      "Iteration 20, loss = 0.50505434\n",
      "Iteration 21, loss = 0.49988427\n",
      "Iteration 22, loss = 0.50021835\n",
      "Iteration 23, loss = 0.50130105\n",
      "Iteration 24, loss = 0.50075255\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.25116007\n",
      "Iteration 2, loss = 0.85195513\n",
      "Iteration 3, loss = 0.70391182\n",
      "Iteration 4, loss = 0.62840193\n",
      "Iteration 5, loss = 0.59546550\n",
      "Iteration 6, loss = 0.57019904\n",
      "Iteration 7, loss = 0.55178925\n",
      "Iteration 8, loss = 0.54613945\n",
      "Iteration 9, loss = 0.53638369\n",
      "Iteration 10, loss = 0.53079480\n",
      "Iteration 11, loss = 0.52448573\n",
      "Iteration 12, loss = 0.52191474\n",
      "Iteration 13, loss = 0.51624466\n",
      "Iteration 14, loss = 0.51982473\n",
      "Iteration 15, loss = 0.51444437\n",
      "Iteration 16, loss = 0.51023703\n",
      "Iteration 17, loss = 0.50888024\n",
      "Iteration 18, loss = 0.50450679\n",
      "Iteration 19, loss = 0.50722549\n",
      "Iteration 20, loss = 0.50136334\n",
      "Iteration 21, loss = 0.49952364\n",
      "Iteration 22, loss = 0.49762852\n",
      "Iteration 23, loss = 0.49813710\n",
      "Iteration 24, loss = 0.50086575\n",
      "Iteration 25, loss = 0.49813776\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.29050484\n",
      "Iteration 2, loss = 0.79452098\n",
      "Iteration 3, loss = 0.67084003\n",
      "Iteration 4, loss = 0.62163381\n",
      "Iteration 5, loss = 0.60075618\n",
      "Iteration 6, loss = 0.58697007\n",
      "Iteration 7, loss = 0.58559821\n",
      "Iteration 8, loss = 0.58617699\n",
      "Iteration 9, loss = 0.57882867\n",
      "Iteration 10, loss = 0.58090418\n",
      "Iteration 11, loss = 0.58308598\n",
      "Iteration 12, loss = 0.57942969\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.27934165\n",
      "Iteration 2, loss = 0.78039982\n",
      "Iteration 3, loss = 0.66985856\n",
      "Iteration 4, loss = 0.62172191\n",
      "Iteration 5, loss = 0.59871236\n",
      "Iteration 6, loss = 0.60086151\n",
      "Iteration 7, loss = 0.58840143\n",
      "Iteration 8, loss = 0.59781003\n",
      "Iteration 9, loss = 0.58124463\n",
      "Iteration 10, loss = 0.58879759\n",
      "Iteration 11, loss = 0.57934715\n",
      "Iteration 12, loss = 0.58081035\n",
      "Iteration 13, loss = 0.57941787\n",
      "Iteration 14, loss = 0.57929095\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.30372718\n",
      "Iteration 2, loss = 0.80244803\n",
      "Iteration 3, loss = 0.66857282\n",
      "Iteration 4, loss = 0.62237636\n",
      "Iteration 5, loss = 0.59945560\n",
      "Iteration 6, loss = 0.59640360\n",
      "Iteration 7, loss = 0.58815433\n",
      "Iteration 8, loss = 0.58999949\n",
      "Iteration 9, loss = 0.58235013\n",
      "Iteration 10, loss = 0.58079699\n",
      "Iteration 11, loss = 0.57725401\n",
      "Iteration 12, loss = 0.57747330\n",
      "Iteration 13, loss = 0.58045074\n",
      "Iteration 14, loss = 0.57641310\n",
      "Iteration 15, loss = 0.57350982\n",
      "Iteration 16, loss = 0.57445831\n",
      "Iteration 17, loss = 0.57218781\n",
      "Iteration 18, loss = 0.56958501\n",
      "Iteration 19, loss = 0.57032774\n",
      "Iteration 20, loss = 0.56761159\n",
      "Iteration 21, loss = 0.57232826\n",
      "Iteration 22, loss = 0.56867841\n",
      "Iteration 23, loss = 0.56935803\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.09471002\n",
      "Iteration 2, loss = 0.74686378\n",
      "Iteration 3, loss = 0.65392409\n",
      "Iteration 4, loss = 0.61400766\n",
      "Iteration 5, loss = 0.59311242\n",
      "Iteration 6, loss = 0.58428818\n",
      "Iteration 7, loss = 0.58013904\n",
      "Iteration 8, loss = 0.57272956\n",
      "Iteration 9, loss = 0.57278195\n",
      "Iteration 10, loss = 0.56626762\n",
      "Iteration 11, loss = 0.56940674\n",
      "Iteration 12, loss = 0.56949706\n",
      "Iteration 13, loss = 0.56437808\n",
      "Iteration 14, loss = 0.56639971\n",
      "Iteration 15, loss = 0.56428692\n",
      "Iteration 16, loss = 0.56717197\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.09166495\n",
      "Iteration 2, loss = 0.74593578\n",
      "Iteration 3, loss = 0.65559322\n",
      "Iteration 4, loss = 0.61544005\n",
      "Iteration 5, loss = 0.59979343\n",
      "Iteration 6, loss = 0.59054982\n",
      "Iteration 7, loss = 0.58224167\n",
      "Iteration 8, loss = 0.57506452\n",
      "Iteration 9, loss = 0.57763805\n",
      "Iteration 10, loss = 0.57438934\n",
      "Iteration 11, loss = 0.57249815\n",
      "Iteration 12, loss = 0.57209468\n",
      "Iteration 13, loss = 0.56983089\n",
      "Iteration 14, loss = 0.56906887\n",
      "Iteration 15, loss = 0.57018132\n",
      "Iteration 16, loss = 0.56580759\n",
      "Iteration 17, loss = 0.56670999\n",
      "Iteration 18, loss = 0.56733391\n",
      "Iteration 19, loss = 0.56566177\n",
      "Iteration 20, loss = 0.56606657\n",
      "Iteration 21, loss = 0.56378220\n",
      "Iteration 22, loss = 0.57002805\n",
      "Iteration 23, loss = 0.56310631\n",
      "Iteration 24, loss = 0.56496689\n",
      "Iteration 25, loss = 0.56294076\n",
      "Iteration 26, loss = 0.56578993\n",
      "Iteration 27, loss = 0.56245743\n",
      "Iteration 28, loss = 0.56429295\n",
      "Iteration 29, loss = 0.56472294\n",
      "Iteration 30, loss = 0.56386449\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08368091\n",
      "Iteration 2, loss = 0.73789530\n",
      "Iteration 3, loss = 0.65136447\n",
      "Iteration 4, loss = 0.61196596\n",
      "Iteration 5, loss = 0.59253584\n",
      "Iteration 6, loss = 0.58834107\n",
      "Iteration 7, loss = 0.57825975\n",
      "Iteration 8, loss = 0.57552065\n",
      "Iteration 9, loss = 0.57114848\n",
      "Iteration 10, loss = 0.57148969\n",
      "Iteration 11, loss = 0.57554483\n",
      "Iteration 12, loss = 0.56844515\n",
      "Iteration 13, loss = 0.56800901\n",
      "Iteration 14, loss = 0.57236994\n",
      "Iteration 15, loss = 0.56644565\n",
      "Iteration 16, loss = 0.56523584\n",
      "Iteration 17, loss = 0.56457849\n",
      "Iteration 18, loss = 0.56148765\n",
      "Iteration 19, loss = 0.56719268\n",
      "Iteration 20, loss = 0.56361437\n",
      "Iteration 21, loss = 0.56123850\n",
      "Iteration 22, loss = 0.56399333\n",
      "Iteration 23, loss = 0.56465376\n",
      "Iteration 24, loss = 0.56568207\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.69100567\n",
      "Iteration 2, loss = 0.92568942\n",
      "Iteration 3, loss = 0.72500075\n",
      "Iteration 4, loss = 0.64999759\n",
      "Iteration 5, loss = 0.61722743\n",
      "Iteration 6, loss = 0.60682684\n",
      "Iteration 7, loss = 0.59733755\n",
      "Iteration 8, loss = 0.58886631\n",
      "Iteration 9, loss = 0.58257770\n",
      "Iteration 10, loss = 0.57976208\n",
      "Iteration 11, loss = 0.57746672\n",
      "Iteration 12, loss = 0.58160241\n",
      "Iteration 13, loss = 0.57532161\n",
      "Iteration 14, loss = 0.57256043\n",
      "Iteration 15, loss = 0.57028781\n",
      "Iteration 16, loss = 0.57006619\n",
      "Iteration 17, loss = 0.56873388\n",
      "Iteration 18, loss = 0.57141113\n",
      "Iteration 19, loss = 0.56689755\n",
      "Iteration 20, loss = 0.56270902\n",
      "Iteration 21, loss = 0.56655713\n",
      "Iteration 22, loss = 0.56041032\n",
      "Iteration 23, loss = 0.55983816\n",
      "Iteration 24, loss = 0.56350738\n",
      "Iteration 25, loss = 0.56174244\n",
      "Iteration 26, loss = 0.55762038\n",
      "Iteration 27, loss = 0.55678743\n",
      "Iteration 28, loss = 0.56249404\n",
      "Iteration 29, loss = 0.56183311\n",
      "Iteration 30, loss = 0.55189148\n",
      "Iteration 31, loss = 0.55685929\n",
      "Iteration 32, loss = 0.55332293\n",
      "Iteration 33, loss = 0.56316730\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.70241368\n",
      "Iteration 2, loss = 0.94209252\n",
      "Iteration 3, loss = 0.73032007\n",
      "Iteration 4, loss = 0.65661626\n",
      "Iteration 5, loss = 0.62151614\n",
      "Iteration 6, loss = 0.60781687\n",
      "Iteration 7, loss = 0.60044170\n",
      "Iteration 8, loss = 0.59774646\n",
      "Iteration 9, loss = 0.59096588\n",
      "Iteration 10, loss = 0.58681350\n",
      "Iteration 11, loss = 0.58049061\n",
      "Iteration 12, loss = 0.58392079\n",
      "Iteration 13, loss = 0.57931227\n",
      "Iteration 14, loss = 0.57995216\n",
      "Iteration 15, loss = 0.57727755\n",
      "Iteration 16, loss = 0.57591096\n",
      "Iteration 17, loss = 0.57216835\n",
      "Iteration 18, loss = 0.57094056\n",
      "Iteration 19, loss = 0.57419967\n",
      "Iteration 20, loss = 0.56712295\n",
      "Iteration 21, loss = 0.57118681\n",
      "Iteration 22, loss = 0.56597044\n",
      "Iteration 23, loss = 0.56773949\n",
      "Iteration 24, loss = 0.56580027\n",
      "Iteration 25, loss = 0.56937331\n",
      "Iteration 26, loss = 0.56779773\n",
      "Iteration 27, loss = 0.56560558\n",
      "Iteration 28, loss = 0.56399772\n",
      "Iteration 29, loss = 0.57191647\n",
      "Iteration 30, loss = 0.56573539\n",
      "Iteration 31, loss = 0.56107882\n",
      "Iteration 32, loss = 0.56048768\n",
      "Iteration 33, loss = 0.56720399\n",
      "Iteration 34, loss = 0.55892088\n",
      "Iteration 35, loss = 0.56256573\n",
      "Iteration 36, loss = 0.56087658\n",
      "Iteration 37, loss = 0.55641345\n",
      "Iteration 38, loss = 0.56181348\n",
      "Iteration 39, loss = 0.55859796\n",
      "Iteration 40, loss = 0.55986632\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.67720547\n",
      "Iteration 2, loss = 0.93486834\n",
      "Iteration 3, loss = 0.72348175\n",
      "Iteration 4, loss = 0.65357988\n",
      "Iteration 5, loss = 0.62166993\n",
      "Iteration 6, loss = 0.60914102\n",
      "Iteration 7, loss = 0.59819014\n",
      "Iteration 8, loss = 0.59372166\n",
      "Iteration 9, loss = 0.58976620\n",
      "Iteration 10, loss = 0.58401924\n",
      "Iteration 11, loss = 0.57786821\n",
      "Iteration 12, loss = 0.58135832\n",
      "Iteration 13, loss = 0.57997843\n",
      "Iteration 14, loss = 0.58051595\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.33199452\n",
      "Iteration 2, loss = 0.85539067\n",
      "Iteration 3, loss = 0.71501335\n",
      "Iteration 4, loss = 0.65450452\n",
      "Iteration 5, loss = 0.62638955\n",
      "Iteration 6, loss = 0.60880930\n",
      "Iteration 7, loss = 0.59392317\n",
      "Iteration 8, loss = 0.59660663\n",
      "Iteration 9, loss = 0.58310045\n",
      "Iteration 10, loss = 0.58531954\n",
      "Iteration 11, loss = 0.58068796\n",
      "Iteration 12, loss = 0.58212000\n",
      "Iteration 13, loss = 0.58206562\n",
      "Iteration 14, loss = 0.57797444\n",
      "Iteration 15, loss = 0.56841554\n",
      "Iteration 16, loss = 0.57398271\n",
      "Iteration 17, loss = 0.57120369\n",
      "Iteration 18, loss = 0.56665249\n",
      "Iteration 19, loss = 0.57422968\n",
      "Iteration 20, loss = 0.56954279\n",
      "Iteration 21, loss = 0.56581768\n",
      "Iteration 22, loss = 0.56418215\n",
      "Iteration 23, loss = 0.56622272\n",
      "Iteration 24, loss = 0.56784984\n",
      "Iteration 25, loss = 0.56358897\n",
      "Iteration 26, loss = 0.56187066\n",
      "Iteration 27, loss = 0.56133286\n",
      "Iteration 28, loss = 0.56457068\n",
      "Iteration 29, loss = 0.56023001\n",
      "Iteration 30, loss = 0.56650937\n",
      "Iteration 31, loss = 0.55687992\n",
      "Iteration 32, loss = 0.56315446\n",
      "Iteration 33, loss = 0.56389044\n",
      "Iteration 34, loss = 0.56070322\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.32299329\n",
      "Iteration 2, loss = 0.85288939\n",
      "Iteration 3, loss = 0.71269662\n",
      "Iteration 4, loss = 0.65332606\n",
      "Iteration 5, loss = 0.62450216\n",
      "Iteration 6, loss = 0.60340785\n",
      "Iteration 7, loss = 0.60328570\n",
      "Iteration 8, loss = 0.59737205\n",
      "Iteration 9, loss = 0.58454469\n",
      "Iteration 10, loss = 0.57967638\n",
      "Iteration 11, loss = 0.58426779\n",
      "Iteration 12, loss = 0.58075036\n",
      "Iteration 13, loss = 0.58325465\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.32839207\n",
      "Iteration 2, loss = 0.85846066\n",
      "Iteration 3, loss = 0.71168109\n",
      "Iteration 4, loss = 0.65745428\n",
      "Iteration 5, loss = 0.62668641\n",
      "Iteration 6, loss = 0.61071207\n",
      "Iteration 7, loss = 0.59876935\n",
      "Iteration 8, loss = 0.59190885\n",
      "Iteration 9, loss = 0.58474344\n",
      "Iteration 10, loss = 0.58270867\n",
      "Iteration 11, loss = 0.57865310\n",
      "Iteration 12, loss = 0.57870464\n",
      "Iteration 13, loss = 0.57631014\n",
      "Iteration 14, loss = 0.57528681\n",
      "Iteration 15, loss = 0.57695228\n",
      "Iteration 16, loss = 0.57013389\n",
      "Iteration 17, loss = 0.56753586\n",
      "Iteration 18, loss = 0.56897406\n",
      "Iteration 19, loss = 0.56784284\n",
      "Iteration 20, loss = 0.56864453\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.56140902\n",
      "Iteration 2, loss = 0.91242099\n",
      "Iteration 3, loss = 0.73597708\n",
      "Iteration 4, loss = 0.66122625\n",
      "Iteration 5, loss = 0.63475084\n",
      "Iteration 6, loss = 0.61208167\n",
      "Iteration 7, loss = 0.60059460\n",
      "Iteration 8, loss = 0.59623166\n",
      "Iteration 9, loss = 0.58868154\n",
      "Iteration 10, loss = 0.58614137\n",
      "Iteration 11, loss = 0.58848635\n",
      "Iteration 12, loss = 0.58148576\n",
      "Iteration 13, loss = 0.57947699\n",
      "Iteration 14, loss = 0.57858766\n",
      "Iteration 15, loss = 0.57755961\n",
      "Iteration 16, loss = 0.57276583\n",
      "Iteration 17, loss = 0.57212704\n",
      "Iteration 18, loss = 0.57082300\n",
      "Iteration 19, loss = 0.57178957\n",
      "Iteration 20, loss = 0.57247256\n",
      "Iteration 21, loss = 0.56669184\n",
      "Iteration 22, loss = 0.56582458\n",
      "Iteration 23, loss = 0.56819811\n",
      "Iteration 24, loss = 0.56677188\n",
      "Iteration 25, loss = 0.56751470\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.59638984\n",
      "Iteration 2, loss = 0.94023172\n",
      "Iteration 3, loss = 0.73968230\n",
      "Iteration 4, loss = 0.66950074\n",
      "Iteration 5, loss = 0.63965392\n",
      "Iteration 6, loss = 0.61483821\n",
      "Iteration 7, loss = 0.60892425\n",
      "Iteration 8, loss = 0.60582326\n",
      "Iteration 9, loss = 0.59729349\n",
      "Iteration 10, loss = 0.59245168\n",
      "Iteration 11, loss = 0.59057145\n",
      "Iteration 12, loss = 0.58567427\n",
      "Iteration 13, loss = 0.58737594\n",
      "Iteration 14, loss = 0.58751638\n",
      "Iteration 15, loss = 0.58160395\n",
      "Iteration 16, loss = 0.57650965\n",
      "Iteration 17, loss = 0.57677202\n",
      "Iteration 18, loss = 0.57441211\n",
      "Iteration 19, loss = 0.57477049\n",
      "Iteration 20, loss = 0.57441738\n",
      "Iteration 21, loss = 0.56990116\n",
      "Iteration 22, loss = 0.57070715\n",
      "Iteration 23, loss = 0.57359870\n",
      "Iteration 24, loss = 0.57340378\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.55821258\n",
      "Iteration 2, loss = 0.91740577\n",
      "Iteration 3, loss = 0.73362959\n",
      "Iteration 4, loss = 0.65946582\n",
      "Iteration 5, loss = 0.63445387\n",
      "Iteration 6, loss = 0.61703888\n",
      "Iteration 7, loss = 0.60231334\n",
      "Iteration 8, loss = 0.60189041\n",
      "Iteration 9, loss = 0.59479638\n",
      "Iteration 10, loss = 0.59070926\n",
      "Iteration 11, loss = 0.58614150\n",
      "Iteration 12, loss = 0.58313800\n",
      "Iteration 13, loss = 0.58005427\n",
      "Iteration 14, loss = 0.58532633\n",
      "Iteration 15, loss = 0.57935242\n",
      "Iteration 16, loss = 0.57775204\n",
      "Iteration 17, loss = 0.57786528\n",
      "Iteration 18, loss = 0.57287224\n",
      "Iteration 19, loss = 0.57757034\n",
      "Iteration 20, loss = 0.57107673\n",
      "Iteration 21, loss = 0.56904097\n",
      "Iteration 22, loss = 0.56792335\n",
      "Iteration 23, loss = 0.56929624\n",
      "Iteration 24, loss = 0.57225320\n",
      "Iteration 25, loss = 0.56997852\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.94031514\n",
      "Iteration 2, loss = 0.86769184\n",
      "Iteration 3, loss = 0.78645552\n",
      "Iteration 4, loss = 0.76745444\n",
      "Iteration 5, loss = 0.76434061\n",
      "Iteration 6, loss = 0.75410777\n",
      "Iteration 7, loss = 0.75773851\n",
      "Iteration 8, loss = 0.75944678\n",
      "Iteration 9, loss = 0.75364860\n",
      "Iteration 10, loss = 0.75706007\n",
      "Iteration 11, loss = 0.75568632\n",
      "Iteration 12, loss = 0.75304912\n",
      "Iteration 13, loss = 0.75351148\n",
      "Iteration 14, loss = 0.75447796\n",
      "Iteration 15, loss = 0.74989046\n",
      "Iteration 16, loss = 0.74560038\n",
      "Iteration 17, loss = 0.74614909\n",
      "Iteration 18, loss = 0.74924743\n",
      "Iteration 19, loss = 0.75247173\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.90182987\n",
      "Iteration 2, loss = 0.85044218\n",
      "Iteration 3, loss = 0.78985620\n",
      "Iteration 4, loss = 0.77174804\n",
      "Iteration 5, loss = 0.76132797\n",
      "Iteration 6, loss = 0.76829614\n",
      "Iteration 7, loss = 0.75991607\n",
      "Iteration 8, loss = 0.76782332\n",
      "Iteration 9, loss = 0.75588838\n",
      "Iteration 10, loss = 0.76093991\n",
      "Iteration 11, loss = 0.75325309\n",
      "Iteration 12, loss = 0.75421636\n",
      "Iteration 13, loss = 0.75359433\n",
      "Iteration 14, loss = 0.75596307\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.97966145\n",
      "Iteration 2, loss = 0.87378060\n",
      "Iteration 3, loss = 0.78000169\n",
      "Iteration 4, loss = 0.76667935\n",
      "Iteration 5, loss = 0.76018585\n",
      "Iteration 6, loss = 0.76418268\n",
      "Iteration 7, loss = 0.75526292\n",
      "Iteration 8, loss = 0.76371386\n",
      "Iteration 9, loss = 0.75526944\n",
      "Iteration 10, loss = 0.75468992\n",
      "Iteration 11, loss = 0.75183914\n",
      "Iteration 12, loss = 0.75310783\n",
      "Iteration 13, loss = 0.75537025\n",
      "Iteration 14, loss = 0.75316957\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.59865092\n",
      "Iteration 2, loss = 0.87031158\n",
      "Iteration 3, loss = 0.78808934\n",
      "Iteration 4, loss = 0.76741255\n",
      "Iteration 5, loss = 0.76085782\n",
      "Iteration 6, loss = 0.75686343\n",
      "Iteration 7, loss = 0.75772235\n",
      "Iteration 8, loss = 0.75335074\n",
      "Iteration 9, loss = 0.75278505\n",
      "Iteration 10, loss = 0.74922356\n",
      "Iteration 11, loss = 0.75059230\n",
      "Iteration 12, loss = 0.75224015\n",
      "Iteration 13, loss = 0.74863845\n",
      "Iteration 14, loss = 0.75094375\n",
      "Iteration 15, loss = 0.74892850\n",
      "Iteration 16, loss = 0.75232760\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.58667700\n",
      "Iteration 2, loss = 0.86902010\n",
      "Iteration 3, loss = 0.78732576\n",
      "Iteration 4, loss = 0.76813459\n",
      "Iteration 5, loss = 0.76475169\n",
      "Iteration 6, loss = 0.76372785\n",
      "Iteration 7, loss = 0.75980392\n",
      "Iteration 8, loss = 0.75316575\n",
      "Iteration 9, loss = 0.75749677\n",
      "Iteration 10, loss = 0.75776859\n",
      "Iteration 11, loss = 0.75305859\n",
      "Iteration 12, loss = 0.75191347\n",
      "Iteration 13, loss = 0.75357646\n",
      "Iteration 14, loss = 0.75368723\n",
      "Iteration 15, loss = 0.75466879\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.57779495\n",
      "Iteration 2, loss = 0.86023005\n",
      "Iteration 3, loss = 0.78460018\n",
      "Iteration 4, loss = 0.76612192\n",
      "Iteration 5, loss = 0.75889399\n",
      "Iteration 6, loss = 0.75991032\n",
      "Iteration 7, loss = 0.75531063\n",
      "Iteration 8, loss = 0.75170484\n",
      "Iteration 9, loss = 0.75130272\n",
      "Iteration 10, loss = 0.75235514\n",
      "Iteration 11, loss = 0.75497043\n",
      "Iteration 12, loss = 0.74870663\n",
      "Iteration 13, loss = 0.75119100\n",
      "Iteration 14, loss = 0.75422355\n",
      "Iteration 15, loss = 0.74771338\n",
      "Iteration 16, loss = 0.74922993\n",
      "Iteration 17, loss = 0.74709600\n",
      "Iteration 18, loss = 0.74555604\n",
      "Iteration 19, loss = 0.75044903\n",
      "Iteration 20, loss = 0.74840234\n",
      "Iteration 21, loss = 0.74471968\n",
      "Iteration 22, loss = 0.75041360\n",
      "Iteration 23, loss = 0.74782957\n",
      "Iteration 24, loss = 0.75015473\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.62372701\n",
      "Iteration 2, loss = 0.93405634\n",
      "Iteration 3, loss = 0.83704092\n",
      "Iteration 4, loss = 0.81831522\n",
      "Iteration 5, loss = 0.80997752\n",
      "Iteration 6, loss = 0.81228768\n",
      "Iteration 7, loss = 0.80693247\n",
      "Iteration 8, loss = 0.80296184\n",
      "Iteration 9, loss = 0.79743582\n",
      "Iteration 10, loss = 0.79882658\n",
      "Iteration 11, loss = 0.79500719\n",
      "Iteration 12, loss = 0.80134916\n",
      "Iteration 13, loss = 0.80050678\n",
      "Iteration 14, loss = 0.80033371\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.64576420\n",
      "Iteration 2, loss = 0.94356515\n",
      "Iteration 3, loss = 0.84348757\n",
      "Iteration 4, loss = 0.82388272\n",
      "Iteration 5, loss = 0.81532005\n",
      "Iteration 6, loss = 0.81343471\n",
      "Iteration 7, loss = 0.80850785\n",
      "Iteration 8, loss = 0.81075460\n",
      "Iteration 9, loss = 0.80310557\n",
      "Iteration 10, loss = 0.80566856\n",
      "Iteration 11, loss = 0.80089749\n",
      "Iteration 12, loss = 0.80759699\n",
      "Iteration 13, loss = 0.80151882\n",
      "Iteration 14, loss = 0.80410488\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.58954259\n",
      "Iteration 2, loss = 0.94083082\n",
      "Iteration 3, loss = 0.83731859\n",
      "Iteration 4, loss = 0.81840511\n",
      "Iteration 5, loss = 0.80909080\n",
      "Iteration 6, loss = 0.80747767\n",
      "Iteration 7, loss = 0.80120275\n",
      "Iteration 8, loss = 0.80441082\n",
      "Iteration 9, loss = 0.80311725\n",
      "Iteration 10, loss = 0.80003388\n",
      "Iteration 11, loss = 0.79527014\n",
      "Iteration 12, loss = 0.80287008\n",
      "Iteration 13, loss = 0.79920950\n",
      "Iteration 14, loss = 0.80078017\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.08760091\n",
      "Iteration 2, loss = 0.97395664\n",
      "Iteration 3, loss = 0.85375682\n",
      "Iteration 4, loss = 0.83172681\n",
      "Iteration 5, loss = 0.82399764\n",
      "Iteration 6, loss = 0.81703500\n",
      "Iteration 7, loss = 0.81106195\n",
      "Iteration 8, loss = 0.81461800\n",
      "Iteration 9, loss = 0.80529244\n",
      "Iteration 10, loss = 0.80802373\n",
      "Iteration 11, loss = 0.80701216\n",
      "Iteration 12, loss = 0.80495363\n",
      "Iteration 13, loss = 0.80650499\n",
      "Iteration 14, loss = 0.80364965\n",
      "Iteration 15, loss = 0.79719846\n",
      "Iteration 16, loss = 0.80268772\n",
      "Iteration 17, loss = 0.80164619\n",
      "Iteration 18, loss = 0.79485025\n",
      "Iteration 19, loss = 0.80284398\n",
      "Iteration 20, loss = 0.80006629\n",
      "Iteration 21, loss = 0.79757377\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.05813629\n",
      "Iteration 2, loss = 0.96802581\n",
      "Iteration 3, loss = 0.85516394\n",
      "Iteration 4, loss = 0.83220040\n",
      "Iteration 5, loss = 0.82781446\n",
      "Iteration 6, loss = 0.81472052\n",
      "Iteration 7, loss = 0.81760076\n",
      "Iteration 8, loss = 0.81775936\n",
      "Iteration 9, loss = 0.80510751\n",
      "Iteration 10, loss = 0.80470515\n",
      "Iteration 11, loss = 0.80833223\n",
      "Iteration 12, loss = 0.80357797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.80850162\n",
      "Iteration 14, loss = 0.80302585\n",
      "Iteration 15, loss = 0.80361472\n",
      "Iteration 16, loss = 0.80870167\n",
      "Iteration 17, loss = 0.80179750\n",
      "Iteration 18, loss = 0.79999413\n",
      "Iteration 19, loss = 0.80305420\n",
      "Iteration 20, loss = 0.80118694\n",
      "Iteration 21, loss = 0.80421881\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.07569542\n",
      "Iteration 2, loss = 0.97256475\n",
      "Iteration 3, loss = 0.85026451\n",
      "Iteration 4, loss = 0.83208986\n",
      "Iteration 5, loss = 0.82203157\n",
      "Iteration 6, loss = 0.81754032\n",
      "Iteration 7, loss = 0.81197255\n",
      "Iteration 8, loss = 0.81032269\n",
      "Iteration 9, loss = 0.80255143\n",
      "Iteration 10, loss = 0.80592959\n",
      "Iteration 11, loss = 0.80312797\n",
      "Iteration 12, loss = 0.79998743\n",
      "Iteration 13, loss = 0.79979317\n",
      "Iteration 14, loss = 0.79804516\n",
      "Iteration 15, loss = 0.80033588\n",
      "Iteration 16, loss = 0.79780220\n",
      "Iteration 17, loss = 0.79662668\n",
      "Iteration 18, loss = 0.79759364\n",
      "Iteration 19, loss = 0.79901395\n",
      "Iteration 20, loss = 0.80148394\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.45165006\n",
      "Iteration 2, loss = 0.96872632\n",
      "Iteration 3, loss = 0.85504828\n",
      "Iteration 4, loss = 0.83061360\n",
      "Iteration 5, loss = 0.82968561\n",
      "Iteration 6, loss = 0.82260316\n",
      "Iteration 7, loss = 0.81411140\n",
      "Iteration 8, loss = 0.80903733\n",
      "Iteration 9, loss = 0.80795650\n",
      "Iteration 10, loss = 0.80393132\n",
      "Iteration 11, loss = 0.80777044\n",
      "Iteration 12, loss = 0.80375843\n",
      "Iteration 13, loss = 0.80226116\n",
      "Iteration 14, loss = 0.80287086\n",
      "Iteration 15, loss = 0.80266237\n",
      "Iteration 16, loss = 0.80130232\n",
      "Iteration 17, loss = 0.79744604\n",
      "Iteration 18, loss = 0.80099545\n",
      "Iteration 19, loss = 0.80282715\n",
      "Iteration 20, loss = 0.79934081\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.53537400\n",
      "Iteration 2, loss = 0.99238281\n",
      "Iteration 3, loss = 0.85726530\n",
      "Iteration 4, loss = 0.83677543\n",
      "Iteration 5, loss = 0.83275770\n",
      "Iteration 6, loss = 0.82519162\n",
      "Iteration 7, loss = 0.82153282\n",
      "Iteration 8, loss = 0.82263464\n",
      "Iteration 9, loss = 0.81695541\n",
      "Iteration 10, loss = 0.80917842\n",
      "Iteration 11, loss = 0.81072426\n",
      "Iteration 12, loss = 0.80522155\n",
      "Iteration 13, loss = 0.80784329\n",
      "Iteration 14, loss = 0.80820514\n",
      "Iteration 15, loss = 0.80721847\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.43238041\n",
      "Iteration 2, loss = 0.96695958\n",
      "Iteration 3, loss = 0.85419205\n",
      "Iteration 4, loss = 0.83155083\n",
      "Iteration 5, loss = 0.82930429\n",
      "Iteration 6, loss = 0.81935664\n",
      "Iteration 7, loss = 0.80980601\n",
      "Iteration 8, loss = 0.81261056\n",
      "Iteration 9, loss = 0.80987066\n",
      "Iteration 10, loss = 0.80540544\n",
      "Iteration 11, loss = 0.80419548\n",
      "Iteration 12, loss = 0.80362021\n",
      "Iteration 13, loss = 0.80189674\n",
      "Iteration 14, loss = 0.80003121\n",
      "Iteration 15, loss = 0.80091559\n",
      "Iteration 16, loss = 0.80198507\n",
      "Iteration 17, loss = 0.80107150\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.04882196\n",
      "Iteration 2, loss = 1.12271788\n",
      "Iteration 3, loss = 1.10453359\n",
      "Iteration 4, loss = 1.09201506\n",
      "Iteration 5, loss = 1.08991906\n",
      "Iteration 6, loss = 1.08536983\n",
      "Iteration 7, loss = 1.08872642\n",
      "Iteration 8, loss = 1.08959406\n",
      "Iteration 9, loss = 1.08788070\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.98162359\n",
      "Iteration 2, loss = 1.12362265\n",
      "Iteration 3, loss = 1.10461017\n",
      "Iteration 4, loss = 1.09185094\n",
      "Iteration 5, loss = 1.08702243\n",
      "Iteration 6, loss = 1.09405666\n",
      "Iteration 7, loss = 1.08663796\n",
      "Iteration 8, loss = 1.09174029\n",
      "Iteration 9, loss = 1.08865648\n",
      "Iteration 10, loss = 1.09269240\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.12236747\n",
      "Iteration 2, loss = 1.12295995\n",
      "Iteration 3, loss = 1.10035075\n",
      "Iteration 4, loss = 1.09206445\n",
      "Iteration 5, loss = 1.08797424\n",
      "Iteration 6, loss = 1.08932131\n",
      "Iteration 7, loss = 1.08597663\n",
      "Iteration 8, loss = 1.08955853\n",
      "Iteration 9, loss = 1.08490840\n",
      "Iteration 10, loss = 1.08569613\n",
      "Iteration 11, loss = 1.08630511\n",
      "Iteration 12, loss = 1.08708919\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.48448124\n",
      "Iteration 2, loss = 1.13813581\n",
      "Iteration 3, loss = 1.11226513\n",
      "Iteration 4, loss = 1.10422527\n",
      "Iteration 5, loss = 1.09826273\n",
      "Iteration 6, loss = 1.09510130\n",
      "Iteration 7, loss = 1.09455105\n",
      "Iteration 8, loss = 1.09220376\n",
      "Iteration 9, loss = 1.09295789\n",
      "Iteration 10, loss = 1.09076345\n",
      "Iteration 11, loss = 1.08969696\n",
      "Iteration 12, loss = 1.09201864\n",
      "Iteration 13, loss = 1.08913216\n",
      "Iteration 14, loss = 1.09227204\n",
      "Iteration 15, loss = 1.09123691\n",
      "Iteration 16, loss = 1.08987987\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.44800852\n",
      "Iteration 2, loss = 1.13558934\n",
      "Iteration 3, loss = 1.11087561\n",
      "Iteration 4, loss = 1.10216267\n",
      "Iteration 5, loss = 1.09864668\n",
      "Iteration 6, loss = 1.09699849\n",
      "Iteration 7, loss = 1.09589773\n",
      "Iteration 8, loss = 1.09109226\n",
      "Iteration 9, loss = 1.09495710\n",
      "Iteration 10, loss = 1.09580046\n",
      "Iteration 11, loss = 1.09217697\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.45389111\n",
      "Iteration 2, loss = 1.13787761\n",
      "Iteration 3, loss = 1.11045772\n",
      "Iteration 4, loss = 1.10261798\n",
      "Iteration 5, loss = 1.09570264\n",
      "Iteration 6, loss = 1.09633144\n",
      "Iteration 7, loss = 1.09504629\n",
      "Iteration 8, loss = 1.09107403\n",
      "Iteration 9, loss = 1.09063348\n",
      "Iteration 10, loss = 1.09360193\n",
      "Iteration 11, loss = 1.09125960\n",
      "Iteration 12, loss = 1.09031329\n",
      "Iteration 13, loss = 1.09203604\n",
      "Iteration 14, loss = 1.09511502\n",
      "Iteration 15, loss = 1.08798430\n",
      "Iteration 16, loss = 1.08910826\n",
      "Iteration 17, loss = 1.08859828\n",
      "Iteration 18, loss = 1.08889230\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.33500998\n",
      "Iteration 2, loss = 1.25846608\n",
      "Iteration 3, loss = 1.23503318\n",
      "Iteration 4, loss = 1.22979423\n",
      "Iteration 5, loss = 1.22660200\n",
      "Iteration 6, loss = 1.23161365\n",
      "Iteration 7, loss = 1.22420600\n",
      "Iteration 8, loss = 1.22483868\n",
      "Iteration 9, loss = 1.22452011\n",
      "Iteration 10, loss = 1.22408643\n",
      "Iteration 11, loss = 1.22202320\n",
      "Iteration 12, loss = 1.22405145\n",
      "Iteration 13, loss = 1.22305335\n",
      "Iteration 14, loss = 1.22681096\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.36592366\n",
      "Iteration 2, loss = 1.26382406\n",
      "Iteration 3, loss = 1.23465005\n",
      "Iteration 4, loss = 1.23495983\n",
      "Iteration 5, loss = 1.22843493\n",
      "Iteration 6, loss = 1.23115290\n",
      "Iteration 7, loss = 1.22633258\n",
      "Iteration 8, loss = 1.23135342\n",
      "Iteration 9, loss = 1.23007308\n",
      "Iteration 10, loss = 1.22763346\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.28212055\n",
      "Iteration 2, loss = 1.26181171\n",
      "Iteration 3, loss = 1.23230440\n",
      "Iteration 4, loss = 1.23009062\n",
      "Iteration 5, loss = 1.22723417\n",
      "Iteration 6, loss = 1.22590865\n",
      "Iteration 7, loss = 1.22368263\n",
      "Iteration 8, loss = 1.22549784\n",
      "Iteration 9, loss = 1.22470618\n",
      "Iteration 10, loss = 1.22810691\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.48350371\n",
      "Iteration 2, loss = 1.31063708\n",
      "Iteration 3, loss = 1.26034063\n",
      "Iteration 4, loss = 1.24970764\n",
      "Iteration 5, loss = 1.24576525\n",
      "Iteration 6, loss = 1.24136604\n",
      "Iteration 7, loss = 1.24081916\n",
      "Iteration 8, loss = 1.23977560\n",
      "Iteration 9, loss = 1.23701531\n",
      "Iteration 10, loss = 1.24004821\n",
      "Iteration 11, loss = 1.23914709\n",
      "Iteration 12, loss = 1.23967567\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.43008783\n",
      "Iteration 2, loss = 1.30769511\n",
      "Iteration 3, loss = 1.26340337\n",
      "Iteration 4, loss = 1.25199817\n",
      "Iteration 5, loss = 1.24876482\n",
      "Iteration 6, loss = 1.24158602\n",
      "Iteration 7, loss = 1.24281282\n",
      "Iteration 8, loss = 1.24373635\n",
      "Iteration 9, loss = 1.23721503\n",
      "Iteration 10, loss = 1.23760289\n",
      "Iteration 11, loss = 1.23833445\n",
      "Iteration 12, loss = 1.23712900\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.45937660\n",
      "Iteration 2, loss = 1.30698605\n",
      "Iteration 3, loss = 1.25950844\n",
      "Iteration 4, loss = 1.25225249\n",
      "Iteration 5, loss = 1.24680862\n",
      "Iteration 6, loss = 1.24335812\n",
      "Iteration 7, loss = 1.24101822\n",
      "Iteration 8, loss = 1.24018394\n",
      "Iteration 9, loss = 1.23612555\n",
      "Iteration 10, loss = 1.23922283\n",
      "Iteration 11, loss = 1.23876653\n",
      "Iteration 12, loss = 1.23560963\n",
      "Iteration 13, loss = 1.23926288\n",
      "Iteration 14, loss = 1.23681901\n",
      "Iteration 15, loss = 1.23635887\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.06703783\n",
      "Iteration 2, loss = 1.29478826\n",
      "Iteration 3, loss = 1.24875158\n",
      "Iteration 4, loss = 1.24226989\n",
      "Iteration 5, loss = 1.24342300\n",
      "Iteration 6, loss = 1.23740214\n",
      "Iteration 7, loss = 1.23548882\n",
      "Iteration 8, loss = 1.23344473\n",
      "Iteration 9, loss = 1.23433815\n",
      "Iteration 10, loss = 1.23153675\n",
      "Iteration 11, loss = 1.23624737\n",
      "Iteration 12, loss = 1.23516196\n",
      "Iteration 13, loss = 1.23539109\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.18865936\n",
      "Iteration 2, loss = 1.30042270\n",
      "Iteration 3, loss = 1.25159356\n",
      "Iteration 4, loss = 1.24267376\n",
      "Iteration 5, loss = 1.24215149\n",
      "Iteration 6, loss = 1.24091293\n",
      "Iteration 7, loss = 1.23838579\n",
      "Iteration 8, loss = 1.24214491\n",
      "Iteration 9, loss = 1.23815985\n",
      "Iteration 10, loss = 1.23502795\n",
      "Iteration 11, loss = 1.23624680\n",
      "Iteration 12, loss = 1.23700707\n",
      "Iteration 13, loss = 1.23687308\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.06370634\n",
      "Iteration 2, loss = 1.29946639\n",
      "Iteration 3, loss = 1.25237885\n",
      "Iteration 4, loss = 1.24479785\n",
      "Iteration 5, loss = 1.23999949\n",
      "Iteration 6, loss = 1.23773638\n",
      "Iteration 7, loss = 1.23461995\n",
      "Iteration 8, loss = 1.23425672\n",
      "Iteration 9, loss = 1.23793552\n",
      "Iteration 10, loss = 1.23799151\n",
      "Iteration 11, loss = 1.23347206\n",
      "Iteration 12, loss = 1.23517313\n",
      "Iteration 13, loss = 1.23584426\n",
      "Iteration 14, loss = 1.23388334\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61881221\n",
      "Iteration 2, loss = 0.44363304\n",
      "Iteration 3, loss = 0.40227534\n",
      "Iteration 4, loss = 0.37496086\n",
      "Iteration 5, loss = 0.35057483\n",
      "Iteration 6, loss = 0.33561815\n",
      "Iteration 7, loss = 0.32022862\n",
      "Iteration 8, loss = 0.30736697\n",
      "Iteration 9, loss = 0.30229645\n",
      "Iteration 10, loss = 0.29040099\n",
      "Iteration 11, loss = 0.28250259\n",
      "Iteration 12, loss = 0.28059953\n",
      "Iteration 13, loss = 0.27053582\n",
      "Iteration 14, loss = 0.26355262\n",
      "Iteration 15, loss = 0.25735351\n",
      "Iteration 16, loss = 0.25248359\n",
      "Iteration 17, loss = 0.24409184\n",
      "Iteration 18, loss = 0.24167479\n",
      "Iteration 19, loss = 0.23498145\n",
      "Iteration 20, loss = 0.23646373\n",
      "Iteration 21, loss = 0.22743998\n",
      "Iteration 22, loss = 0.22281369\n",
      "Iteration 23, loss = 0.21971493\n",
      "Iteration 24, loss = 0.21641991\n",
      "Iteration 25, loss = 0.21007319\n",
      "Iteration 26, loss = 0.20797272\n",
      "Iteration 27, loss = 0.20081182\n",
      "Iteration 28, loss = 0.20106076\n",
      "Iteration 29, loss = 0.19835157\n",
      "Iteration 30, loss = 0.19576846\n",
      "Iteration 31, loss = 0.19241850\n",
      "Iteration 32, loss = 0.19020192\n",
      "Iteration 33, loss = 0.18508594\n",
      "Iteration 34, loss = 0.18123202\n",
      "Iteration 35, loss = 0.17818850\n",
      "Iteration 36, loss = 0.17501301\n",
      "Iteration 37, loss = 0.17349784\n",
      "Iteration 38, loss = 0.17281518\n",
      "Iteration 39, loss = 0.16982254\n",
      "Iteration 40, loss = 0.16788649\n",
      "Iteration 41, loss = 0.17161211\n",
      "Iteration 42, loss = 0.16370751\n",
      "Iteration 43, loss = 0.15987383\n",
      "Iteration 44, loss = 0.15726454\n",
      "Iteration 45, loss = 0.16026753\n",
      "Iteration 46, loss = 0.15554359\n",
      "Iteration 47, loss = 0.15499108\n",
      "Iteration 48, loss = 0.15291227\n",
      "Iteration 49, loss = 0.15106497\n",
      "Iteration 50, loss = 0.14770833\n",
      "Iteration 51, loss = 0.14784523\n",
      "Iteration 52, loss = 0.14321659\n",
      "Iteration 53, loss = 0.13998389\n",
      "Iteration 54, loss = 0.13912677\n",
      "Iteration 55, loss = 0.14265584\n",
      "Iteration 56, loss = 0.13897495\n",
      "Iteration 57, loss = 0.13473656\n",
      "Iteration 58, loss = 0.13810224\n",
      "Iteration 59, loss = 0.13906156\n",
      "Iteration 60, loss = 0.13394607\n",
      "Iteration 61, loss = 0.12995351\n",
      "Iteration 62, loss = 0.12770361\n",
      "Iteration 63, loss = 0.13073542\n",
      "Iteration 64, loss = 0.12819166\n",
      "Iteration 65, loss = 0.12394925\n",
      "Iteration 66, loss = 0.12155663\n",
      "Iteration 67, loss = 0.12883993\n",
      "Iteration 68, loss = 0.12238364\n",
      "Iteration 69, loss = 0.12284180\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61929840\n",
      "Iteration 2, loss = 0.44327349\n",
      "Iteration 3, loss = 0.41146952\n",
      "Iteration 4, loss = 0.37910448\n",
      "Iteration 5, loss = 0.35304462\n",
      "Iteration 6, loss = 0.34421491\n",
      "Iteration 7, loss = 0.32801452\n",
      "Iteration 8, loss = 0.31867143\n",
      "Iteration 9, loss = 0.30417402\n",
      "Iteration 10, loss = 0.29985107\n",
      "Iteration 11, loss = 0.28588425\n",
      "Iteration 12, loss = 0.27826548\n",
      "Iteration 13, loss = 0.27313084\n",
      "Iteration 14, loss = 0.26888189\n",
      "Iteration 15, loss = 0.25815306\n",
      "Iteration 16, loss = 0.25621436\n",
      "Iteration 17, loss = 0.24893657\n",
      "Iteration 18, loss = 0.24079128\n",
      "Iteration 19, loss = 0.23942637\n",
      "Iteration 20, loss = 0.23539397\n",
      "Iteration 21, loss = 0.22658039\n",
      "Iteration 22, loss = 0.22119540\n",
      "Iteration 23, loss = 0.21809339\n",
      "Iteration 24, loss = 0.21101784\n",
      "Iteration 25, loss = 0.21094661\n",
      "Iteration 26, loss = 0.20990148\n",
      "Iteration 27, loss = 0.20464057\n",
      "Iteration 28, loss = 0.20263636\n",
      "Iteration 29, loss = 0.20154454\n",
      "Iteration 30, loss = 0.19550440\n",
      "Iteration 31, loss = 0.19167355\n",
      "Iteration 32, loss = 0.19070138\n",
      "Iteration 33, loss = 0.18420247\n",
      "Iteration 34, loss = 0.17940470\n",
      "Iteration 35, loss = 0.17918088\n",
      "Iteration 36, loss = 0.17705747\n",
      "Iteration 37, loss = 0.17059582\n",
      "Iteration 38, loss = 0.17185608\n",
      "Iteration 39, loss = 0.16936083\n",
      "Iteration 40, loss = 0.16813953\n",
      "Iteration 41, loss = 0.16502269\n",
      "Iteration 42, loss = 0.15971241\n",
      "Iteration 43, loss = 0.16371038\n",
      "Iteration 44, loss = 0.15960950\n",
      "Iteration 45, loss = 0.15461877\n",
      "Iteration 46, loss = 0.15259827\n",
      "Iteration 47, loss = 0.15458332\n",
      "Iteration 48, loss = 0.15340901\n",
      "Iteration 49, loss = 0.14953546\n",
      "Iteration 50, loss = 0.14841162\n",
      "Iteration 51, loss = 0.14580384\n",
      "Iteration 52, loss = 0.14061183\n",
      "Iteration 53, loss = 0.13870093\n",
      "Iteration 54, loss = 0.14340232\n",
      "Iteration 55, loss = 0.13885573\n",
      "Iteration 56, loss = 0.13497618\n",
      "Iteration 57, loss = 0.13548880\n",
      "Iteration 58, loss = 0.13491908\n",
      "Iteration 59, loss = 0.13058966\n",
      "Iteration 60, loss = 0.13019222\n",
      "Iteration 61, loss = 0.13789319\n",
      "Iteration 62, loss = 0.12851098\n",
      "Iteration 63, loss = 0.12673884\n",
      "Iteration 64, loss = 0.12689226\n",
      "Iteration 65, loss = 0.12410006\n",
      "Iteration 66, loss = 0.12989884\n",
      "Iteration 67, loss = 0.12436123\n",
      "Iteration 68, loss = 0.11941407\n",
      "Iteration 69, loss = 0.12477560\n",
      "Iteration 70, loss = 0.12020651\n",
      "Iteration 71, loss = 0.12038580\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62527026\n",
      "Iteration 2, loss = 0.45072022\n",
      "Iteration 3, loss = 0.40113345\n",
      "Iteration 4, loss = 0.37484196\n",
      "Iteration 5, loss = 0.35035952\n",
      "Iteration 6, loss = 0.33922680\n",
      "Iteration 7, loss = 0.32258855\n",
      "Iteration 8, loss = 0.31300688\n",
      "Iteration 9, loss = 0.30049986\n",
      "Iteration 10, loss = 0.29065244\n",
      "Iteration 11, loss = 0.28220139\n",
      "Iteration 12, loss = 0.27701145\n",
      "Iteration 13, loss = 0.27176190\n",
      "Iteration 14, loss = 0.26617012\n",
      "Iteration 15, loss = 0.25669369\n",
      "Iteration 16, loss = 0.25404427\n",
      "Iteration 17, loss = 0.24526559\n",
      "Iteration 18, loss = 0.23734930\n",
      "Iteration 19, loss = 0.23284315\n",
      "Iteration 20, loss = 0.23063290\n",
      "Iteration 21, loss = 0.22671295\n",
      "Iteration 22, loss = 0.21907352\n",
      "Iteration 23, loss = 0.21739293\n",
      "Iteration 24, loss = 0.21085986\n",
      "Iteration 25, loss = 0.21024074\n",
      "Iteration 26, loss = 0.20829583\n",
      "Iteration 27, loss = 0.19831553\n",
      "Iteration 28, loss = 0.20047483\n",
      "Iteration 29, loss = 0.19348343\n",
      "Iteration 30, loss = 0.18996034\n",
      "Iteration 31, loss = 0.19050566\n",
      "Iteration 32, loss = 0.18821284\n",
      "Iteration 33, loss = 0.18360565\n",
      "Iteration 34, loss = 0.18059489\n",
      "Iteration 35, loss = 0.17506509\n",
      "Iteration 36, loss = 0.17577807\n",
      "Iteration 37, loss = 0.17224238\n",
      "Iteration 38, loss = 0.16791659\n",
      "Iteration 39, loss = 0.16726023\n",
      "Iteration 40, loss = 0.16557443\n",
      "Iteration 41, loss = 0.16214937\n",
      "Iteration 42, loss = 0.16139015\n",
      "Iteration 43, loss = 0.15786761\n",
      "Iteration 44, loss = 0.15616150\n",
      "Iteration 45, loss = 0.15395664\n",
      "Iteration 46, loss = 0.15055753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47, loss = 0.14925476\n",
      "Iteration 48, loss = 0.15213873\n",
      "Iteration 49, loss = 0.14683569\n",
      "Iteration 50, loss = 0.14684384\n",
      "Iteration 51, loss = 0.14818239\n",
      "Iteration 52, loss = 0.14353432\n",
      "Iteration 53, loss = 0.13977863\n",
      "Iteration 54, loss = 0.13649001\n",
      "Iteration 55, loss = 0.13570915\n",
      "Iteration 56, loss = 0.13645045\n",
      "Iteration 57, loss = 0.13664040\n",
      "Iteration 58, loss = 0.14001143\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64680117\n",
      "Iteration 2, loss = 0.46031671\n",
      "Iteration 3, loss = 0.41301873\n",
      "Iteration 4, loss = 0.38679778\n",
      "Iteration 5, loss = 0.36411993\n",
      "Iteration 6, loss = 0.35200870\n",
      "Iteration 7, loss = 0.34083725\n",
      "Iteration 8, loss = 0.32756572\n",
      "Iteration 9, loss = 0.31556966\n",
      "Iteration 10, loss = 0.30633899\n",
      "Iteration 11, loss = 0.30343812\n",
      "Iteration 12, loss = 0.29803546\n",
      "Iteration 13, loss = 0.28407577\n",
      "Iteration 14, loss = 0.28292396\n",
      "Iteration 15, loss = 0.27310043\n",
      "Iteration 16, loss = 0.26852575\n",
      "Iteration 17, loss = 0.26253839\n",
      "Iteration 18, loss = 0.25890721\n",
      "Iteration 19, loss = 0.25839427\n",
      "Iteration 20, loss = 0.24826266\n",
      "Iteration 21, loss = 0.24533265\n",
      "Iteration 22, loss = 0.24245807\n",
      "Iteration 23, loss = 0.23510126\n",
      "Iteration 24, loss = 0.23447459\n",
      "Iteration 25, loss = 0.22908444\n",
      "Iteration 26, loss = 0.22934839\n",
      "Iteration 27, loss = 0.22208568\n",
      "Iteration 28, loss = 0.22379889\n",
      "Iteration 29, loss = 0.21654729\n",
      "Iteration 30, loss = 0.21719670\n",
      "Iteration 31, loss = 0.21183893\n",
      "Iteration 32, loss = 0.20963712\n",
      "Iteration 33, loss = 0.20473598\n",
      "Iteration 34, loss = 0.20576628\n",
      "Iteration 35, loss = 0.19938864\n",
      "Iteration 36, loss = 0.19613539\n",
      "Iteration 37, loss = 0.19310866\n",
      "Iteration 38, loss = 0.18970906\n",
      "Iteration 39, loss = 0.18885031\n",
      "Iteration 40, loss = 0.18609899\n",
      "Iteration 41, loss = 0.18837470\n",
      "Iteration 42, loss = 0.18451747\n",
      "Iteration 43, loss = 0.17841654\n",
      "Iteration 44, loss = 0.17881506\n",
      "Iteration 45, loss = 0.17694782\n",
      "Iteration 46, loss = 0.17471859\n",
      "Iteration 47, loss = 0.17426379\n",
      "Iteration 48, loss = 0.17425983\n",
      "Iteration 49, loss = 0.16944323\n",
      "Iteration 50, loss = 0.16616066\n",
      "Iteration 51, loss = 0.16477915\n",
      "Iteration 52, loss = 0.16715171\n",
      "Iteration 53, loss = 0.15990821\n",
      "Iteration 54, loss = 0.15831130\n",
      "Iteration 55, loss = 0.15973414\n",
      "Iteration 56, loss = 0.16079822\n",
      "Iteration 57, loss = 0.15305839\n",
      "Iteration 58, loss = 0.15346350\n",
      "Iteration 59, loss = 0.15227807\n",
      "Iteration 60, loss = 0.15401374\n",
      "Iteration 61, loss = 0.14910876\n",
      "Iteration 62, loss = 0.14960931\n",
      "Iteration 63, loss = 0.14735581\n",
      "Iteration 64, loss = 0.14500180\n",
      "Iteration 65, loss = 0.14459017\n",
      "Iteration 66, loss = 0.14622965\n",
      "Iteration 67, loss = 0.13852862\n",
      "Iteration 68, loss = 0.13978626\n",
      "Iteration 69, loss = 0.14346024\n",
      "Iteration 70, loss = 0.14175865\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65448699\n",
      "Iteration 2, loss = 0.46189189\n",
      "Iteration 3, loss = 0.42138250\n",
      "Iteration 4, loss = 0.39298396\n",
      "Iteration 5, loss = 0.37644967\n",
      "Iteration 6, loss = 0.35820658\n",
      "Iteration 7, loss = 0.34146496\n",
      "Iteration 8, loss = 0.33359762\n",
      "Iteration 9, loss = 0.32236236\n",
      "Iteration 10, loss = 0.31421838\n",
      "Iteration 11, loss = 0.30470192\n",
      "Iteration 12, loss = 0.29860759\n",
      "Iteration 13, loss = 0.29151733\n",
      "Iteration 14, loss = 0.28633576\n",
      "Iteration 15, loss = 0.27842234\n",
      "Iteration 16, loss = 0.26852045\n",
      "Iteration 17, loss = 0.26580389\n",
      "Iteration 18, loss = 0.25782547\n",
      "Iteration 19, loss = 0.25671874\n",
      "Iteration 20, loss = 0.25212428\n",
      "Iteration 21, loss = 0.24718242\n",
      "Iteration 22, loss = 0.24381678\n",
      "Iteration 23, loss = 0.24176684\n",
      "Iteration 24, loss = 0.23481096\n",
      "Iteration 25, loss = 0.22817054\n",
      "Iteration 26, loss = 0.22890330\n",
      "Iteration 27, loss = 0.22160082\n",
      "Iteration 28, loss = 0.22049044\n",
      "Iteration 29, loss = 0.21912085\n",
      "Iteration 30, loss = 0.21837913\n",
      "Iteration 31, loss = 0.21237098\n",
      "Iteration 32, loss = 0.20627568\n",
      "Iteration 33, loss = 0.20712579\n",
      "Iteration 34, loss = 0.20350635\n",
      "Iteration 35, loss = 0.19706029\n",
      "Iteration 36, loss = 0.19925493\n",
      "Iteration 37, loss = 0.19432061\n",
      "Iteration 38, loss = 0.19043836\n",
      "Iteration 39, loss = 0.18701599\n",
      "Iteration 40, loss = 0.18806337\n",
      "Iteration 41, loss = 0.18449222\n",
      "Iteration 42, loss = 0.18546497\n",
      "Iteration 43, loss = 0.18098493\n",
      "Iteration 44, loss = 0.17700893\n",
      "Iteration 45, loss = 0.17454716\n",
      "Iteration 46, loss = 0.17314540\n",
      "Iteration 47, loss = 0.17178975\n",
      "Iteration 48, loss = 0.17262840\n",
      "Iteration 49, loss = 0.16667684\n",
      "Iteration 50, loss = 0.16484481\n",
      "Iteration 51, loss = 0.16460902\n",
      "Iteration 52, loss = 0.16361228\n",
      "Iteration 53, loss = 0.15970235\n",
      "Iteration 54, loss = 0.15793060\n",
      "Iteration 55, loss = 0.15821975\n",
      "Iteration 56, loss = 0.15867592\n",
      "Iteration 57, loss = 0.15303178\n",
      "Iteration 58, loss = 0.15498745\n",
      "Iteration 59, loss = 0.14856594\n",
      "Iteration 60, loss = 0.14626721\n",
      "Iteration 61, loss = 0.14502134\n",
      "Iteration 62, loss = 0.14618003\n",
      "Iteration 63, loss = 0.14883076\n",
      "Iteration 64, loss = 0.14388382\n",
      "Iteration 65, loss = 0.14115014\n",
      "Iteration 66, loss = 0.14448933\n",
      "Iteration 67, loss = 0.14116894\n",
      "Iteration 68, loss = 0.13923843\n",
      "Iteration 69, loss = 0.13549848\n",
      "Iteration 70, loss = 0.13309270\n",
      "Iteration 71, loss = 0.13530574\n",
      "Iteration 72, loss = 0.13150215\n",
      "Iteration 73, loss = 0.13369684\n",
      "Iteration 74, loss = 0.13015275\n",
      "Iteration 75, loss = 0.12932953\n",
      "Iteration 76, loss = 0.13072684\n",
      "Iteration 77, loss = 0.12683772\n",
      "Iteration 78, loss = 0.12953647\n",
      "Iteration 79, loss = 0.12545724\n",
      "Iteration 80, loss = 0.13056428\n",
      "Iteration 81, loss = 0.12387030\n",
      "Iteration 82, loss = 0.12496060\n",
      "Iteration 83, loss = 0.12519380\n",
      "Iteration 84, loss = 0.11783927\n",
      "Iteration 85, loss = 0.11933246\n",
      "Iteration 86, loss = 0.11819740\n",
      "Iteration 87, loss = 0.12174061\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65196687\n",
      "Iteration 2, loss = 0.45741619\n",
      "Iteration 3, loss = 0.41875727\n",
      "Iteration 4, loss = 0.39130649\n",
      "Iteration 5, loss = 0.36839863\n",
      "Iteration 6, loss = 0.35866907\n",
      "Iteration 7, loss = 0.33927473\n",
      "Iteration 8, loss = 0.32747262\n",
      "Iteration 9, loss = 0.31924089\n",
      "Iteration 10, loss = 0.31097268\n",
      "Iteration 11, loss = 0.30529104\n",
      "Iteration 12, loss = 0.29699868\n",
      "Iteration 13, loss = 0.28603043\n",
      "Iteration 14, loss = 0.28282112\n",
      "Iteration 15, loss = 0.27760432\n",
      "Iteration 16, loss = 0.26902680\n",
      "Iteration 17, loss = 0.26224340\n",
      "Iteration 18, loss = 0.25632282\n",
      "Iteration 19, loss = 0.25640237\n",
      "Iteration 20, loss = 0.25074738\n",
      "Iteration 21, loss = 0.24734315\n",
      "Iteration 22, loss = 0.24015100\n",
      "Iteration 23, loss = 0.24034972\n",
      "Iteration 24, loss = 0.23216208\n",
      "Iteration 25, loss = 0.23115428\n",
      "Iteration 26, loss = 0.22650550\n",
      "Iteration 27, loss = 0.22437568\n",
      "Iteration 28, loss = 0.21809532\n",
      "Iteration 29, loss = 0.21762987\n",
      "Iteration 30, loss = 0.21091251\n",
      "Iteration 31, loss = 0.21036546\n",
      "Iteration 32, loss = 0.20495039\n",
      "Iteration 33, loss = 0.20546346\n",
      "Iteration 34, loss = 0.19960129\n",
      "Iteration 35, loss = 0.19966093\n",
      "Iteration 36, loss = 0.19413830\n",
      "Iteration 37, loss = 0.19108156\n",
      "Iteration 38, loss = 0.18738462\n",
      "Iteration 39, loss = 0.18522482\n",
      "Iteration 40, loss = 0.18995814\n",
      "Iteration 41, loss = 0.18154021\n",
      "Iteration 42, loss = 0.17666961\n",
      "Iteration 43, loss = 0.17735061\n",
      "Iteration 44, loss = 0.17693747\n",
      "Iteration 45, loss = 0.17170928\n",
      "Iteration 46, loss = 0.17129849\n",
      "Iteration 47, loss = 0.16839856\n",
      "Iteration 48, loss = 0.16822160\n",
      "Iteration 49, loss = 0.16749377\n",
      "Iteration 50, loss = 0.16291585\n",
      "Iteration 51, loss = 0.16167209\n",
      "Iteration 52, loss = 0.15990289\n",
      "Iteration 53, loss = 0.15748676\n",
      "Iteration 54, loss = 0.15625389\n",
      "Iteration 55, loss = 0.15423827\n",
      "Iteration 56, loss = 0.15316788\n",
      "Iteration 57, loss = 0.15670523\n",
      "Iteration 58, loss = 0.15010992\n",
      "Iteration 59, loss = 0.14689502\n",
      "Iteration 60, loss = 0.14232876\n",
      "Iteration 61, loss = 0.14507320\n",
      "Iteration 62, loss = 0.14064881\n",
      "Iteration 63, loss = 0.14321692\n",
      "Iteration 64, loss = 0.14557578\n",
      "Iteration 65, loss = 0.14037426\n",
      "Iteration 66, loss = 0.13931658\n",
      "Iteration 67, loss = 0.13606270\n",
      "Iteration 68, loss = 0.13534368\n",
      "Iteration 69, loss = 0.13891583\n",
      "Iteration 70, loss = 0.12865356\n",
      "Iteration 71, loss = 0.13256648\n",
      "Iteration 72, loss = 0.13322700\n",
      "Iteration 73, loss = 0.13083176\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59270555\n",
      "Iteration 2, loss = 0.42025801\n",
      "Iteration 3, loss = 0.37185984\n",
      "Iteration 4, loss = 0.34943095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.32797594\n",
      "Iteration 6, loss = 0.31536210\n",
      "Iteration 7, loss = 0.29749040\n",
      "Iteration 8, loss = 0.28662996\n",
      "Iteration 9, loss = 0.27624369\n",
      "Iteration 10, loss = 0.26409480\n",
      "Iteration 11, loss = 0.26260937\n",
      "Iteration 12, loss = 0.25035344\n",
      "Iteration 13, loss = 0.24228836\n",
      "Iteration 14, loss = 0.24031009\n",
      "Iteration 15, loss = 0.23305632\n",
      "Iteration 16, loss = 0.22774492\n",
      "Iteration 17, loss = 0.22476387\n",
      "Iteration 18, loss = 0.21571768\n",
      "Iteration 19, loss = 0.21434329\n",
      "Iteration 20, loss = 0.20864948\n",
      "Iteration 21, loss = 0.19673556\n",
      "Iteration 22, loss = 0.19233061\n",
      "Iteration 23, loss = 0.19280600\n",
      "Iteration 24, loss = 0.18804754\n",
      "Iteration 25, loss = 0.18225786\n",
      "Iteration 26, loss = 0.17421701\n",
      "Iteration 27, loss = 0.17416361\n",
      "Iteration 28, loss = 0.17231402\n",
      "Iteration 29, loss = 0.16514611\n",
      "Iteration 30, loss = 0.16355682\n",
      "Iteration 31, loss = 0.15789638\n",
      "Iteration 32, loss = 0.16129336\n",
      "Iteration 33, loss = 0.15748134\n",
      "Iteration 34, loss = 0.14947701\n",
      "Iteration 35, loss = 0.14729722\n",
      "Iteration 36, loss = 0.14435027\n",
      "Iteration 37, loss = 0.13920509\n",
      "Iteration 38, loss = 0.14573212\n",
      "Iteration 39, loss = 0.14219623\n",
      "Iteration 40, loss = 0.13461660\n",
      "Iteration 41, loss = 0.13033364\n",
      "Iteration 42, loss = 0.12877711\n",
      "Iteration 43, loss = 0.13397327\n",
      "Iteration 44, loss = 0.12800530\n",
      "Iteration 45, loss = 0.12910304\n",
      "Iteration 46, loss = 0.12074600\n",
      "Iteration 47, loss = 0.12169832\n",
      "Iteration 48, loss = 0.11529924\n",
      "Iteration 49, loss = 0.12441346\n",
      "Iteration 50, loss = 0.11925378\n",
      "Iteration 51, loss = 0.11296416\n",
      "Iteration 52, loss = 0.11335208\n",
      "Iteration 53, loss = 0.11256762\n",
      "Iteration 54, loss = 0.11455305\n",
      "Iteration 55, loss = 0.12050770\n",
      "Iteration 56, loss = 0.10879100\n",
      "Iteration 57, loss = 0.10915716\n",
      "Iteration 58, loss = 0.10899425\n",
      "Iteration 59, loss = 0.10456985\n",
      "Iteration 60, loss = 0.10113922\n",
      "Iteration 61, loss = 0.11027624\n",
      "Iteration 62, loss = 0.10438584\n",
      "Iteration 63, loss = 0.10597190\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59878091\n",
      "Iteration 2, loss = 0.42278613\n",
      "Iteration 3, loss = 0.37675403\n",
      "Iteration 4, loss = 0.34970786\n",
      "Iteration 5, loss = 0.32725731\n",
      "Iteration 6, loss = 0.31391045\n",
      "Iteration 7, loss = 0.30315053\n",
      "Iteration 8, loss = 0.28957129\n",
      "Iteration 9, loss = 0.27887082\n",
      "Iteration 10, loss = 0.27040828\n",
      "Iteration 11, loss = 0.25781731\n",
      "Iteration 12, loss = 0.24891678\n",
      "Iteration 13, loss = 0.24620512\n",
      "Iteration 14, loss = 0.23881501\n",
      "Iteration 15, loss = 0.23278275\n",
      "Iteration 16, loss = 0.22807613\n",
      "Iteration 17, loss = 0.21954003\n",
      "Iteration 18, loss = 0.20938112\n",
      "Iteration 19, loss = 0.21145346\n",
      "Iteration 20, loss = 0.20506092\n",
      "Iteration 21, loss = 0.19934669\n",
      "Iteration 22, loss = 0.19506431\n",
      "Iteration 23, loss = 0.19249742\n",
      "Iteration 24, loss = 0.18346165\n",
      "Iteration 25, loss = 0.18567914\n",
      "Iteration 26, loss = 0.17742382\n",
      "Iteration 27, loss = 0.17468066\n",
      "Iteration 28, loss = 0.17184984\n",
      "Iteration 29, loss = 0.16828529\n",
      "Iteration 30, loss = 0.16722259\n",
      "Iteration 31, loss = 0.15914555\n",
      "Iteration 32, loss = 0.16487040\n",
      "Iteration 33, loss = 0.16136811\n",
      "Iteration 34, loss = 0.14830598\n",
      "Iteration 35, loss = 0.14861356\n",
      "Iteration 36, loss = 0.14223594\n",
      "Iteration 37, loss = 0.14075554\n",
      "Iteration 38, loss = 0.14137452\n",
      "Iteration 39, loss = 0.13636790\n",
      "Iteration 40, loss = 0.13884175\n",
      "Iteration 41, loss = 0.13443140\n",
      "Iteration 42, loss = 0.13341971\n",
      "Iteration 43, loss = 0.12403392\n",
      "Iteration 44, loss = 0.12691688\n",
      "Iteration 45, loss = 0.12876072\n",
      "Iteration 46, loss = 0.12218196\n",
      "Iteration 47, loss = 0.12412042\n",
      "Iteration 48, loss = 0.12122740\n",
      "Iteration 49, loss = 0.12094180\n",
      "Iteration 50, loss = 0.11605272\n",
      "Iteration 51, loss = 0.11650919\n",
      "Iteration 52, loss = 0.12389181\n",
      "Iteration 53, loss = 0.11676467\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59571960\n",
      "Iteration 2, loss = 0.42461361\n",
      "Iteration 3, loss = 0.37625135\n",
      "Iteration 4, loss = 0.35050002\n",
      "Iteration 5, loss = 0.32894832\n",
      "Iteration 6, loss = 0.31317492\n",
      "Iteration 7, loss = 0.29797785\n",
      "Iteration 8, loss = 0.28714965\n",
      "Iteration 9, loss = 0.27685409\n",
      "Iteration 10, loss = 0.26339855\n",
      "Iteration 11, loss = 0.26313169\n",
      "Iteration 12, loss = 0.24942427\n",
      "Iteration 13, loss = 0.24700205\n",
      "Iteration 14, loss = 0.24037848\n",
      "Iteration 15, loss = 0.23126882\n",
      "Iteration 16, loss = 0.22432893\n",
      "Iteration 17, loss = 0.21762139\n",
      "Iteration 18, loss = 0.21063935\n",
      "Iteration 19, loss = 0.21009752\n",
      "Iteration 20, loss = 0.20423030\n",
      "Iteration 21, loss = 0.19960737\n",
      "Iteration 22, loss = 0.19709427\n",
      "Iteration 23, loss = 0.18959399\n",
      "Iteration 24, loss = 0.18567440\n",
      "Iteration 25, loss = 0.18105645\n",
      "Iteration 26, loss = 0.17539113\n",
      "Iteration 27, loss = 0.17696251\n",
      "Iteration 28, loss = 0.16706666\n",
      "Iteration 29, loss = 0.17278530\n",
      "Iteration 30, loss = 0.16464532\n",
      "Iteration 31, loss = 0.16477120\n",
      "Iteration 32, loss = 0.15995680\n",
      "Iteration 33, loss = 0.15248969\n",
      "Iteration 34, loss = 0.14814775\n",
      "Iteration 35, loss = 0.14913190\n",
      "Iteration 36, loss = 0.14393016\n",
      "Iteration 37, loss = 0.14066900\n",
      "Iteration 38, loss = 0.13849900\n",
      "Iteration 39, loss = 0.14010233\n",
      "Iteration 40, loss = 0.13708015\n",
      "Iteration 41, loss = 0.13869757\n",
      "Iteration 42, loss = 0.13437816\n",
      "Iteration 43, loss = 0.12747887\n",
      "Iteration 44, loss = 0.12776424\n",
      "Iteration 45, loss = 0.13499549\n",
      "Iteration 46, loss = 0.12319974\n",
      "Iteration 47, loss = 0.12268222\n",
      "Iteration 48, loss = 0.12558420\n",
      "Iteration 49, loss = 0.12108664\n",
      "Iteration 50, loss = 0.12241548\n",
      "Iteration 51, loss = 0.11649744\n",
      "Iteration 52, loss = 0.11617523\n",
      "Iteration 53, loss = 0.11603166\n",
      "Iteration 54, loss = 0.10442175\n",
      "Iteration 55, loss = 0.11264055\n",
      "Iteration 56, loss = 0.11825936\n",
      "Iteration 57, loss = 0.11326785\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63461476\n",
      "Iteration 2, loss = 0.42822718\n",
      "Iteration 3, loss = 0.38624678\n",
      "Iteration 4, loss = 0.35922604\n",
      "Iteration 5, loss = 0.34152638\n",
      "Iteration 6, loss = 0.32279815\n",
      "Iteration 7, loss = 0.30644690\n",
      "Iteration 8, loss = 0.29728015\n",
      "Iteration 9, loss = 0.28438460\n",
      "Iteration 10, loss = 0.28137437\n",
      "Iteration 11, loss = 0.27007019\n",
      "Iteration 12, loss = 0.26637931\n",
      "Iteration 13, loss = 0.25636093\n",
      "Iteration 14, loss = 0.25231954\n",
      "Iteration 15, loss = 0.23913639\n",
      "Iteration 16, loss = 0.23777890\n",
      "Iteration 17, loss = 0.22965693\n",
      "Iteration 18, loss = 0.22587232\n",
      "Iteration 19, loss = 0.22440294\n",
      "Iteration 20, loss = 0.22372444\n",
      "Iteration 21, loss = 0.21044321\n",
      "Iteration 22, loss = 0.20773678\n",
      "Iteration 23, loss = 0.20401614\n",
      "Iteration 24, loss = 0.19539095\n",
      "Iteration 25, loss = 0.19226271\n",
      "Iteration 26, loss = 0.19061815\n",
      "Iteration 27, loss = 0.18908669\n",
      "Iteration 28, loss = 0.17884495\n",
      "Iteration 29, loss = 0.18010598\n",
      "Iteration 30, loss = 0.17671651\n",
      "Iteration 31, loss = 0.16584356\n",
      "Iteration 32, loss = 0.17024412\n",
      "Iteration 33, loss = 0.16968804\n",
      "Iteration 34, loss = 0.16232330\n",
      "Iteration 35, loss = 0.15862540\n",
      "Iteration 36, loss = 0.15455643\n",
      "Iteration 37, loss = 0.15126507\n",
      "Iteration 38, loss = 0.15335192\n",
      "Iteration 39, loss = 0.14568677\n",
      "Iteration 40, loss = 0.14425672\n",
      "Iteration 41, loss = 0.14640348\n",
      "Iteration 42, loss = 0.13971047\n",
      "Iteration 43, loss = 0.14059129\n",
      "Iteration 44, loss = 0.13302309\n",
      "Iteration 45, loss = 0.13058775\n",
      "Iteration 46, loss = 0.12883182\n",
      "Iteration 47, loss = 0.12562701\n",
      "Iteration 48, loss = 0.12747928\n",
      "Iteration 49, loss = 0.12797766\n",
      "Iteration 50, loss = 0.12762110\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62623119\n",
      "Iteration 2, loss = 0.43236784\n",
      "Iteration 3, loss = 0.39007322\n",
      "Iteration 4, loss = 0.36262341\n",
      "Iteration 5, loss = 0.34322884\n",
      "Iteration 6, loss = 0.32594463\n",
      "Iteration 7, loss = 0.31772205\n",
      "Iteration 8, loss = 0.30482016\n",
      "Iteration 9, loss = 0.28857885\n",
      "Iteration 10, loss = 0.28263995\n",
      "Iteration 11, loss = 0.27476419\n",
      "Iteration 12, loss = 0.26732022\n",
      "Iteration 13, loss = 0.25715446\n",
      "Iteration 14, loss = 0.25351710\n",
      "Iteration 15, loss = 0.24451298\n",
      "Iteration 16, loss = 0.23906922\n",
      "Iteration 17, loss = 0.23702449\n",
      "Iteration 18, loss = 0.23262033\n",
      "Iteration 19, loss = 0.22570014\n",
      "Iteration 20, loss = 0.22212010\n",
      "Iteration 21, loss = 0.21429837\n",
      "Iteration 22, loss = 0.21086268\n",
      "Iteration 23, loss = 0.20451450\n",
      "Iteration 24, loss = 0.20512210\n",
      "Iteration 25, loss = 0.19418112\n",
      "Iteration 26, loss = 0.19058151\n",
      "Iteration 27, loss = 0.18588572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28, loss = 0.17855958\n",
      "Iteration 29, loss = 0.17844429\n",
      "Iteration 30, loss = 0.17863273\n",
      "Iteration 31, loss = 0.17493601\n",
      "Iteration 32, loss = 0.16868421\n",
      "Iteration 33, loss = 0.16636930\n",
      "Iteration 34, loss = 0.16665567\n",
      "Iteration 35, loss = 0.16185548\n",
      "Iteration 36, loss = 0.16069695\n",
      "Iteration 37, loss = 0.15596357\n",
      "Iteration 38, loss = 0.14684162\n",
      "Iteration 39, loss = 0.14358709\n",
      "Iteration 40, loss = 0.14486970\n",
      "Iteration 41, loss = 0.14591205\n",
      "Iteration 42, loss = 0.14592298\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63157799\n",
      "Iteration 2, loss = 0.43339071\n",
      "Iteration 3, loss = 0.38915931\n",
      "Iteration 4, loss = 0.36552551\n",
      "Iteration 5, loss = 0.34414593\n",
      "Iteration 6, loss = 0.32732761\n",
      "Iteration 7, loss = 0.31311217\n",
      "Iteration 8, loss = 0.30068871\n",
      "Iteration 9, loss = 0.28668294\n",
      "Iteration 10, loss = 0.28019191\n",
      "Iteration 11, loss = 0.26964992\n",
      "Iteration 12, loss = 0.26565313\n",
      "Iteration 13, loss = 0.25610053\n",
      "Iteration 14, loss = 0.24731436\n",
      "Iteration 15, loss = 0.24458600\n",
      "Iteration 16, loss = 0.23763013\n",
      "Iteration 17, loss = 0.22937930\n",
      "Iteration 18, loss = 0.22393320\n",
      "Iteration 19, loss = 0.21757308\n",
      "Iteration 20, loss = 0.21424042\n",
      "Iteration 21, loss = 0.20833430\n",
      "Iteration 22, loss = 0.20514705\n",
      "Iteration 23, loss = 0.19945529\n",
      "Iteration 24, loss = 0.19664254\n",
      "Iteration 25, loss = 0.19237725\n",
      "Iteration 26, loss = 0.18765750\n",
      "Iteration 27, loss = 0.18232156\n",
      "Iteration 28, loss = 0.17928880\n",
      "Iteration 29, loss = 0.17902029\n",
      "Iteration 30, loss = 0.17982768\n",
      "Iteration 31, loss = 0.16788790\n",
      "Iteration 32, loss = 0.16464991\n",
      "Iteration 33, loss = 0.16229412\n",
      "Iteration 34, loss = 0.15785990\n",
      "Iteration 35, loss = 0.16341187\n",
      "Iteration 36, loss = 0.15677642\n",
      "Iteration 37, loss = 0.15195791\n",
      "Iteration 38, loss = 0.15152512\n",
      "Iteration 39, loss = 0.14467056\n",
      "Iteration 40, loss = 0.14178404\n",
      "Iteration 41, loss = 0.14434616\n",
      "Iteration 42, loss = 0.14038746\n",
      "Iteration 43, loss = 0.13288985\n",
      "Iteration 44, loss = 0.13456382\n",
      "Iteration 45, loss = 0.13438531\n",
      "Iteration 46, loss = 0.13294480\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60096979\n",
      "Iteration 2, loss = 0.42203180\n",
      "Iteration 3, loss = 0.38073396\n",
      "Iteration 4, loss = 0.35557499\n",
      "Iteration 5, loss = 0.33689979\n",
      "Iteration 6, loss = 0.31479523\n",
      "Iteration 7, loss = 0.30021818\n",
      "Iteration 8, loss = 0.29085924\n",
      "Iteration 9, loss = 0.28100853\n",
      "Iteration 10, loss = 0.26907988\n",
      "Iteration 11, loss = 0.26281936\n",
      "Iteration 12, loss = 0.25317103\n",
      "Iteration 13, loss = 0.24498202\n",
      "Iteration 14, loss = 0.24090925\n",
      "Iteration 15, loss = 0.23443771\n",
      "Iteration 16, loss = 0.22811001\n",
      "Iteration 17, loss = 0.22120656\n",
      "Iteration 18, loss = 0.21928513\n",
      "Iteration 19, loss = 0.21232608\n",
      "Iteration 20, loss = 0.20931083\n",
      "Iteration 21, loss = 0.20499091\n",
      "Iteration 22, loss = 0.19807061\n",
      "Iteration 23, loss = 0.19107712\n",
      "Iteration 24, loss = 0.19284589\n",
      "Iteration 25, loss = 0.18993832\n",
      "Iteration 26, loss = 0.18261129\n",
      "Iteration 27, loss = 0.17793318\n",
      "Iteration 28, loss = 0.17435684\n",
      "Iteration 29, loss = 0.17193114\n",
      "Iteration 30, loss = 0.17737838\n",
      "Iteration 31, loss = 0.16512797\n",
      "Iteration 32, loss = 0.16231150\n",
      "Iteration 33, loss = 0.16039763\n",
      "Iteration 34, loss = 0.16614752\n",
      "Iteration 35, loss = 0.15537324\n",
      "Iteration 36, loss = 0.15299723\n",
      "Iteration 37, loss = 0.15704032\n",
      "Iteration 38, loss = 0.14603465\n",
      "Iteration 39, loss = 0.14286102\n",
      "Iteration 40, loss = 0.14677228\n",
      "Iteration 41, loss = 0.14341367\n",
      "Iteration 42, loss = 0.13787184\n",
      "Iteration 43, loss = 0.13544923\n",
      "Iteration 44, loss = 0.13930069\n",
      "Iteration 45, loss = 0.13496258\n",
      "Iteration 46, loss = 0.13853448\n",
      "Iteration 47, loss = 0.13107389\n",
      "Iteration 48, loss = 0.12810122\n",
      "Iteration 49, loss = 0.12583565\n",
      "Iteration 50, loss = 0.12443545\n",
      "Iteration 51, loss = 0.12684144\n",
      "Iteration 52, loss = 0.12095308\n",
      "Iteration 53, loss = 0.11903364\n",
      "Iteration 54, loss = 0.11608034\n",
      "Iteration 55, loss = 0.11614167\n",
      "Iteration 56, loss = 0.12543803\n",
      "Iteration 57, loss = 0.11537052\n",
      "Iteration 58, loss = 0.11336050\n",
      "Iteration 59, loss = 0.10899549\n",
      "Iteration 60, loss = 0.11614723\n",
      "Iteration 61, loss = 0.10642918\n",
      "Iteration 62, loss = 0.11518089\n",
      "Iteration 63, loss = 0.10927109\n",
      "Iteration 64, loss = 0.10062817\n",
      "Iteration 65, loss = 0.10710480\n",
      "Iteration 66, loss = 0.11019612\n",
      "Iteration 67, loss = 0.10148975\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60302154\n",
      "Iteration 2, loss = 0.42978142\n",
      "Iteration 3, loss = 0.38103487\n",
      "Iteration 4, loss = 0.35425490\n",
      "Iteration 5, loss = 0.33182582\n",
      "Iteration 6, loss = 0.31716461\n",
      "Iteration 7, loss = 0.29857294\n",
      "Iteration 8, loss = 0.29230506\n",
      "Iteration 9, loss = 0.28102423\n",
      "Iteration 10, loss = 0.26825128\n",
      "Iteration 11, loss = 0.26377502\n",
      "Iteration 12, loss = 0.25054967\n",
      "Iteration 13, loss = 0.24772049\n",
      "Iteration 14, loss = 0.23958959\n",
      "Iteration 15, loss = 0.23251669\n",
      "Iteration 16, loss = 0.22656692\n",
      "Iteration 17, loss = 0.22266783\n",
      "Iteration 18, loss = 0.21928485\n",
      "Iteration 19, loss = 0.21411009\n",
      "Iteration 20, loss = 0.20563856\n",
      "Iteration 21, loss = 0.20938044\n",
      "Iteration 22, loss = 0.19677342\n",
      "Iteration 23, loss = 0.19823344\n",
      "Iteration 24, loss = 0.18955965\n",
      "Iteration 25, loss = 0.18301155\n",
      "Iteration 26, loss = 0.18061339\n",
      "Iteration 27, loss = 0.17686466\n",
      "Iteration 28, loss = 0.17028308\n",
      "Iteration 29, loss = 0.17776053\n",
      "Iteration 30, loss = 0.17414107\n",
      "Iteration 31, loss = 0.16824074\n",
      "Iteration 32, loss = 0.16253458\n",
      "Iteration 33, loss = 0.15919750\n",
      "Iteration 34, loss = 0.15719452\n",
      "Iteration 35, loss = 0.15195593\n",
      "Iteration 36, loss = 0.14727498\n",
      "Iteration 37, loss = 0.15222470\n",
      "Iteration 38, loss = 0.14851957\n",
      "Iteration 39, loss = 0.14141269\n",
      "Iteration 40, loss = 0.14965428\n",
      "Iteration 41, loss = 0.13901984\n",
      "Iteration 42, loss = 0.13417351\n",
      "Iteration 43, loss = 0.13103640\n",
      "Iteration 44, loss = 0.13605064\n",
      "Iteration 45, loss = 0.13148061\n",
      "Iteration 46, loss = 0.12689141\n",
      "Iteration 47, loss = 0.12561127\n",
      "Iteration 48, loss = 0.12237124\n",
      "Iteration 49, loss = 0.12332835\n",
      "Iteration 50, loss = 0.12676740\n",
      "Iteration 51, loss = 0.12295467\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60847540\n",
      "Iteration 2, loss = 0.42941524\n",
      "Iteration 3, loss = 0.38166213\n",
      "Iteration 4, loss = 0.35285681\n",
      "Iteration 5, loss = 0.34046068\n",
      "Iteration 6, loss = 0.32183995\n",
      "Iteration 7, loss = 0.30295723\n",
      "Iteration 8, loss = 0.29470963\n",
      "Iteration 9, loss = 0.27844467\n",
      "Iteration 10, loss = 0.26924161\n",
      "Iteration 11, loss = 0.26730524\n",
      "Iteration 12, loss = 0.25260075\n",
      "Iteration 13, loss = 0.24728732\n",
      "Iteration 14, loss = 0.24191610\n",
      "Iteration 15, loss = 0.23258986\n",
      "Iteration 16, loss = 0.22866625\n",
      "Iteration 17, loss = 0.22317219\n",
      "Iteration 18, loss = 0.21744380\n",
      "Iteration 19, loss = 0.21537044\n",
      "Iteration 20, loss = 0.20663088\n",
      "Iteration 21, loss = 0.20713227\n",
      "Iteration 22, loss = 0.19628625\n",
      "Iteration 23, loss = 0.19016209\n",
      "Iteration 24, loss = 0.19034158\n",
      "Iteration 25, loss = 0.18450626\n",
      "Iteration 26, loss = 0.18732490\n",
      "Iteration 27, loss = 0.17799506\n",
      "Iteration 28, loss = 0.17169188\n",
      "Iteration 29, loss = 0.17331600\n",
      "Iteration 30, loss = 0.16930128\n",
      "Iteration 31, loss = 0.16793709\n",
      "Iteration 32, loss = 0.16435962\n",
      "Iteration 33, loss = 0.16248119\n",
      "Iteration 34, loss = 0.15338561\n",
      "Iteration 35, loss = 0.15602753\n",
      "Iteration 36, loss = 0.14939075\n",
      "Iteration 37, loss = 0.14670013\n",
      "Iteration 38, loss = 0.14632676\n",
      "Iteration 39, loss = 0.13815087\n",
      "Iteration 40, loss = 0.14035379\n",
      "Iteration 41, loss = 0.14214247\n",
      "Iteration 42, loss = 0.13501115\n",
      "Iteration 43, loss = 0.13575689\n",
      "Iteration 44, loss = 0.13173272\n",
      "Iteration 45, loss = 0.12667702\n",
      "Iteration 46, loss = 0.13135168\n",
      "Iteration 47, loss = 0.12672113\n",
      "Iteration 48, loss = 0.12310504\n",
      "Iteration 49, loss = 0.12155293\n",
      "Iteration 50, loss = 0.12728518\n",
      "Iteration 51, loss = 0.12155575\n",
      "Iteration 52, loss = 0.12100279\n",
      "Iteration 53, loss = 0.11810669\n",
      "Iteration 54, loss = 0.12292230\n",
      "Iteration 55, loss = 0.11645142\n",
      "Iteration 56, loss = 0.10960771\n",
      "Iteration 57, loss = 0.11326444\n",
      "Iteration 58, loss = 0.10965191\n",
      "Iteration 59, loss = 0.10725645\n",
      "Iteration 60, loss = 0.10874920\n",
      "Iteration 61, loss = 0.11122302\n",
      "Iteration 62, loss = 0.11115299\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63962284\n",
      "Iteration 2, loss = 0.46526645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.42332727\n",
      "Iteration 4, loss = 0.39773159\n",
      "Iteration 5, loss = 0.37511716\n",
      "Iteration 6, loss = 0.35965917\n",
      "Iteration 7, loss = 0.34754283\n",
      "Iteration 8, loss = 0.33581677\n",
      "Iteration 9, loss = 0.33054537\n",
      "Iteration 10, loss = 0.31951088\n",
      "Iteration 11, loss = 0.31442339\n",
      "Iteration 12, loss = 0.31235466\n",
      "Iteration 13, loss = 0.30310071\n",
      "Iteration 14, loss = 0.29920325\n",
      "Iteration 15, loss = 0.29175308\n",
      "Iteration 16, loss = 0.28843194\n",
      "Iteration 17, loss = 0.28021673\n",
      "Iteration 18, loss = 0.27741046\n",
      "Iteration 19, loss = 0.27650674\n",
      "Iteration 20, loss = 0.27484301\n",
      "Iteration 21, loss = 0.26953954\n",
      "Iteration 22, loss = 0.26511916\n",
      "Iteration 23, loss = 0.26297381\n",
      "Iteration 24, loss = 0.25908628\n",
      "Iteration 25, loss = 0.25626343\n",
      "Iteration 26, loss = 0.25157347\n",
      "Iteration 27, loss = 0.24808901\n",
      "Iteration 28, loss = 0.24917873\n",
      "Iteration 29, loss = 0.24278309\n",
      "Iteration 30, loss = 0.24337705\n",
      "Iteration 31, loss = 0.23683512\n",
      "Iteration 32, loss = 0.23982271\n",
      "Iteration 33, loss = 0.23495662\n",
      "Iteration 34, loss = 0.23055922\n",
      "Iteration 35, loss = 0.22913891\n",
      "Iteration 36, loss = 0.22809771\n",
      "Iteration 37, loss = 0.22527045\n",
      "Iteration 38, loss = 0.22933286\n",
      "Iteration 39, loss = 0.22460476\n",
      "Iteration 40, loss = 0.21944672\n",
      "Iteration 41, loss = 0.22538277\n",
      "Iteration 42, loss = 0.21919802\n",
      "Iteration 43, loss = 0.21635451\n",
      "Iteration 44, loss = 0.21520822\n",
      "Iteration 45, loss = 0.21784939\n",
      "Iteration 46, loss = 0.21215828\n",
      "Iteration 47, loss = 0.21073001\n",
      "Iteration 48, loss = 0.20684778\n",
      "Iteration 49, loss = 0.20686029\n",
      "Iteration 50, loss = 0.20605629\n",
      "Iteration 51, loss = 0.20647820\n",
      "Iteration 52, loss = 0.20328396\n",
      "Iteration 53, loss = 0.19971914\n",
      "Iteration 54, loss = 0.20058121\n",
      "Iteration 55, loss = 0.20022256\n",
      "Iteration 56, loss = 0.19764108\n",
      "Iteration 57, loss = 0.19759427\n",
      "Iteration 58, loss = 0.19562773\n",
      "Iteration 59, loss = 0.20099137\n",
      "Iteration 60, loss = 0.19543986\n",
      "Iteration 61, loss = 0.19355798\n",
      "Iteration 62, loss = 0.19203126\n",
      "Iteration 63, loss = 0.18748884\n",
      "Iteration 64, loss = 0.18866960\n",
      "Iteration 65, loss = 0.18671960\n",
      "Iteration 66, loss = 0.18414241\n",
      "Iteration 67, loss = 0.18697371\n",
      "Iteration 68, loss = 0.18598888\n",
      "Iteration 69, loss = 0.18780717\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64030441\n",
      "Iteration 2, loss = 0.46444962\n",
      "Iteration 3, loss = 0.43303669\n",
      "Iteration 4, loss = 0.40132262\n",
      "Iteration 5, loss = 0.37704794\n",
      "Iteration 6, loss = 0.37162422\n",
      "Iteration 7, loss = 0.35656989\n",
      "Iteration 8, loss = 0.34729410\n",
      "Iteration 9, loss = 0.33583308\n",
      "Iteration 10, loss = 0.32975004\n",
      "Iteration 11, loss = 0.31826749\n",
      "Iteration 12, loss = 0.31046734\n",
      "Iteration 13, loss = 0.30757434\n",
      "Iteration 14, loss = 0.30393501\n",
      "Iteration 15, loss = 0.29414221\n",
      "Iteration 16, loss = 0.29358938\n",
      "Iteration 17, loss = 0.28726645\n",
      "Iteration 18, loss = 0.28050999\n",
      "Iteration 19, loss = 0.27936271\n",
      "Iteration 20, loss = 0.27752471\n",
      "Iteration 21, loss = 0.26948589\n",
      "Iteration 22, loss = 0.26281004\n",
      "Iteration 23, loss = 0.25984520\n",
      "Iteration 24, loss = 0.25373448\n",
      "Iteration 25, loss = 0.25307510\n",
      "Iteration 26, loss = 0.25833539\n",
      "Iteration 27, loss = 0.24857231\n",
      "Iteration 28, loss = 0.24715737\n",
      "Iteration 29, loss = 0.24863922\n",
      "Iteration 30, loss = 0.24248990\n",
      "Iteration 31, loss = 0.23994511\n",
      "Iteration 32, loss = 0.23891780\n",
      "Iteration 33, loss = 0.23361234\n",
      "Iteration 34, loss = 0.23035974\n",
      "Iteration 35, loss = 0.23381255\n",
      "Iteration 36, loss = 0.22719179\n",
      "Iteration 37, loss = 0.22494109\n",
      "Iteration 38, loss = 0.22348010\n",
      "Iteration 39, loss = 0.21903957\n",
      "Iteration 40, loss = 0.22331673\n",
      "Iteration 41, loss = 0.21824032\n",
      "Iteration 42, loss = 0.21744332\n",
      "Iteration 43, loss = 0.21884716\n",
      "Iteration 44, loss = 0.21694130\n",
      "Iteration 45, loss = 0.21367732\n",
      "Iteration 46, loss = 0.20749105\n",
      "Iteration 47, loss = 0.20948262\n",
      "Iteration 48, loss = 0.21018554\n",
      "Iteration 49, loss = 0.20806566\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64726337\n",
      "Iteration 2, loss = 0.47431778\n",
      "Iteration 3, loss = 0.42510942\n",
      "Iteration 4, loss = 0.39926489\n",
      "Iteration 5, loss = 0.37499267\n",
      "Iteration 6, loss = 0.36666920\n",
      "Iteration 7, loss = 0.35074238\n",
      "Iteration 8, loss = 0.34103627\n",
      "Iteration 9, loss = 0.33179082\n",
      "Iteration 10, loss = 0.32133784\n",
      "Iteration 11, loss = 0.31440297\n",
      "Iteration 12, loss = 0.30804130\n",
      "Iteration 13, loss = 0.30476337\n",
      "Iteration 14, loss = 0.30133820\n",
      "Iteration 15, loss = 0.29131004\n",
      "Iteration 16, loss = 0.28908404\n",
      "Iteration 17, loss = 0.28237021\n",
      "Iteration 18, loss = 0.27548944\n",
      "Iteration 19, loss = 0.27200362\n",
      "Iteration 20, loss = 0.26881369\n",
      "Iteration 21, loss = 0.26665433\n",
      "Iteration 22, loss = 0.25997209\n",
      "Iteration 23, loss = 0.25994757\n",
      "Iteration 24, loss = 0.25542403\n",
      "Iteration 25, loss = 0.25339439\n",
      "Iteration 26, loss = 0.25267538\n",
      "Iteration 27, loss = 0.24413888\n",
      "Iteration 28, loss = 0.24584503\n",
      "Iteration 29, loss = 0.23985829\n",
      "Iteration 30, loss = 0.23949872\n",
      "Iteration 31, loss = 0.24022488\n",
      "Iteration 32, loss = 0.23436276\n",
      "Iteration 33, loss = 0.23230412\n",
      "Iteration 34, loss = 0.23333081\n",
      "Iteration 35, loss = 0.22599298\n",
      "Iteration 36, loss = 0.22829847\n",
      "Iteration 37, loss = 0.22584248\n",
      "Iteration 38, loss = 0.22275358\n",
      "Iteration 39, loss = 0.22185590\n",
      "Iteration 40, loss = 0.21798882\n",
      "Iteration 41, loss = 0.21640232\n",
      "Iteration 42, loss = 0.21829012\n",
      "Iteration 43, loss = 0.21338503\n",
      "Iteration 44, loss = 0.21326666\n",
      "Iteration 45, loss = 0.21017928\n",
      "Iteration 46, loss = 0.20701260\n",
      "Iteration 47, loss = 0.20751156\n",
      "Iteration 48, loss = 0.20731540\n",
      "Iteration 49, loss = 0.20414631\n",
      "Iteration 50, loss = 0.20908440\n",
      "Iteration 51, loss = 0.20413064\n",
      "Iteration 52, loss = 0.20472245\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66063353\n",
      "Iteration 2, loss = 0.47590978\n",
      "Iteration 3, loss = 0.42956616\n",
      "Iteration 4, loss = 0.40312352\n",
      "Iteration 5, loss = 0.38169774\n",
      "Iteration 6, loss = 0.36868476\n",
      "Iteration 7, loss = 0.35901398\n",
      "Iteration 8, loss = 0.34587408\n",
      "Iteration 9, loss = 0.33571295\n",
      "Iteration 10, loss = 0.32687782\n",
      "Iteration 11, loss = 0.32351026\n",
      "Iteration 12, loss = 0.31973705\n",
      "Iteration 13, loss = 0.30837362\n",
      "Iteration 14, loss = 0.30815948\n",
      "Iteration 15, loss = 0.29919986\n",
      "Iteration 16, loss = 0.29452111\n",
      "Iteration 17, loss = 0.29047555\n",
      "Iteration 18, loss = 0.28696739\n",
      "Iteration 19, loss = 0.28689112\n",
      "Iteration 20, loss = 0.27902114\n",
      "Iteration 21, loss = 0.27634835\n",
      "Iteration 22, loss = 0.27217648\n",
      "Iteration 23, loss = 0.26687451\n",
      "Iteration 24, loss = 0.26679187\n",
      "Iteration 25, loss = 0.26306542\n",
      "Iteration 26, loss = 0.26186311\n",
      "Iteration 27, loss = 0.25674959\n",
      "Iteration 28, loss = 0.26145961\n",
      "Iteration 29, loss = 0.25218992\n",
      "Iteration 30, loss = 0.25458706\n",
      "Iteration 31, loss = 0.24698908\n",
      "Iteration 32, loss = 0.24737861\n",
      "Iteration 33, loss = 0.24140501\n",
      "Iteration 34, loss = 0.24252975\n",
      "Iteration 35, loss = 0.23868732\n",
      "Iteration 36, loss = 0.23703091\n",
      "Iteration 37, loss = 0.23350994\n",
      "Iteration 38, loss = 0.22963296\n",
      "Iteration 39, loss = 0.22933972\n",
      "Iteration 40, loss = 0.22774019\n",
      "Iteration 41, loss = 0.23041849\n",
      "Iteration 42, loss = 0.22761033\n",
      "Iteration 43, loss = 0.22079511\n",
      "Iteration 44, loss = 0.22331886\n",
      "Iteration 45, loss = 0.22209525\n",
      "Iteration 46, loss = 0.21902221\n",
      "Iteration 47, loss = 0.21610761\n",
      "Iteration 48, loss = 0.22066527\n",
      "Iteration 49, loss = 0.21418020\n",
      "Iteration 50, loss = 0.21331698\n",
      "Iteration 51, loss = 0.20998633\n",
      "Iteration 52, loss = 0.21423053\n",
      "Iteration 53, loss = 0.20862650\n",
      "Iteration 54, loss = 0.20737886\n",
      "Iteration 55, loss = 0.20869987\n",
      "Iteration 56, loss = 0.21400472\n",
      "Iteration 57, loss = 0.20361210\n",
      "Iteration 58, loss = 0.20202570\n",
      "Iteration 59, loss = 0.20336762\n",
      "Iteration 60, loss = 0.20407474\n",
      "Iteration 61, loss = 0.19865508\n",
      "Iteration 62, loss = 0.19949808\n",
      "Iteration 63, loss = 0.19780060\n",
      "Iteration 64, loss = 0.19526173\n",
      "Iteration 65, loss = 0.19445040\n",
      "Iteration 66, loss = 0.20091160\n",
      "Iteration 67, loss = 0.19211832\n",
      "Iteration 68, loss = 0.19004048\n",
      "Iteration 69, loss = 0.19409090\n",
      "Iteration 70, loss = 0.19578394\n",
      "Iteration 71, loss = 0.18862580\n",
      "Iteration 72, loss = 0.19283116\n",
      "Iteration 73, loss = 0.18706278\n",
      "Iteration 74, loss = 0.18775489\n",
      "Iteration 75, loss = 0.18887844\n",
      "Iteration 76, loss = 0.18498049\n",
      "Iteration 77, loss = 0.18572159\n",
      "Iteration 78, loss = 0.18482369\n",
      "Iteration 79, loss = 0.18342656\n",
      "Iteration 80, loss = 0.18293480\n",
      "Iteration 81, loss = 0.18376561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 82, loss = 0.18346577\n",
      "Iteration 83, loss = 0.18127029\n",
      "Iteration 84, loss = 0.18119212\n",
      "Iteration 85, loss = 0.18042508\n",
      "Iteration 86, loss = 0.17948996\n",
      "Iteration 87, loss = 0.17927986\n",
      "Iteration 88, loss = 0.17675868\n",
      "Iteration 89, loss = 0.17804520\n",
      "Iteration 90, loss = 0.17735221\n",
      "Iteration 91, loss = 0.17403422\n",
      "Iteration 92, loss = 0.17647276\n",
      "Iteration 93, loss = 0.17573119\n",
      "Iteration 94, loss = 0.17556071\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66767536\n",
      "Iteration 2, loss = 0.47639981\n",
      "Iteration 3, loss = 0.43608678\n",
      "Iteration 4, loss = 0.40825660\n",
      "Iteration 5, loss = 0.39285592\n",
      "Iteration 6, loss = 0.37562391\n",
      "Iteration 7, loss = 0.36010712\n",
      "Iteration 8, loss = 0.35231255\n",
      "Iteration 9, loss = 0.34212190\n",
      "Iteration 10, loss = 0.33495038\n",
      "Iteration 11, loss = 0.32685478\n",
      "Iteration 12, loss = 0.32106479\n",
      "Iteration 13, loss = 0.31522909\n",
      "Iteration 14, loss = 0.31059878\n",
      "Iteration 15, loss = 0.30433203\n",
      "Iteration 16, loss = 0.29457042\n",
      "Iteration 17, loss = 0.29544196\n",
      "Iteration 18, loss = 0.28697765\n",
      "Iteration 19, loss = 0.28656052\n",
      "Iteration 20, loss = 0.28313327\n",
      "Iteration 21, loss = 0.27785342\n",
      "Iteration 22, loss = 0.27655152\n",
      "Iteration 23, loss = 0.27409275\n",
      "Iteration 24, loss = 0.26744481\n",
      "Iteration 25, loss = 0.26371904\n",
      "Iteration 26, loss = 0.26462529\n",
      "Iteration 27, loss = 0.25774008\n",
      "Iteration 28, loss = 0.25791063\n",
      "Iteration 29, loss = 0.25640690\n",
      "Iteration 30, loss = 0.25441363\n",
      "Iteration 31, loss = 0.25292927\n",
      "Iteration 32, loss = 0.24694478\n",
      "Iteration 33, loss = 0.24671705\n",
      "Iteration 34, loss = 0.24369361\n",
      "Iteration 35, loss = 0.24001673\n",
      "Iteration 36, loss = 0.24048500\n",
      "Iteration 37, loss = 0.23732831\n",
      "Iteration 38, loss = 0.23491854\n",
      "Iteration 39, loss = 0.23082382\n",
      "Iteration 40, loss = 0.23163133\n",
      "Iteration 41, loss = 0.22805539\n",
      "Iteration 42, loss = 0.23316932\n",
      "Iteration 43, loss = 0.22793998\n",
      "Iteration 44, loss = 0.22399350\n",
      "Iteration 45, loss = 0.22442392\n",
      "Iteration 46, loss = 0.22314397\n",
      "Iteration 47, loss = 0.22166401\n",
      "Iteration 48, loss = 0.22212301\n",
      "Iteration 49, loss = 0.21787414\n",
      "Iteration 50, loss = 0.21454145\n",
      "Iteration 51, loss = 0.21533220\n",
      "Iteration 52, loss = 0.21494281\n",
      "Iteration 53, loss = 0.21197348\n",
      "Iteration 54, loss = 0.21317811\n",
      "Iteration 55, loss = 0.20870603\n",
      "Iteration 56, loss = 0.21129164\n",
      "Iteration 57, loss = 0.20875575\n",
      "Iteration 58, loss = 0.20895783\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66575278\n",
      "Iteration 2, loss = 0.47232083\n",
      "Iteration 3, loss = 0.43343211\n",
      "Iteration 4, loss = 0.40840344\n",
      "Iteration 5, loss = 0.38692336\n",
      "Iteration 6, loss = 0.37874914\n",
      "Iteration 7, loss = 0.35903004\n",
      "Iteration 8, loss = 0.34817222\n",
      "Iteration 9, loss = 0.34197409\n",
      "Iteration 10, loss = 0.33371478\n",
      "Iteration 11, loss = 0.32841388\n",
      "Iteration 12, loss = 0.32108390\n",
      "Iteration 13, loss = 0.31149720\n",
      "Iteration 14, loss = 0.30851203\n",
      "Iteration 15, loss = 0.30369794\n",
      "Iteration 16, loss = 0.29656633\n",
      "Iteration 17, loss = 0.28938223\n",
      "Iteration 18, loss = 0.28392819\n",
      "Iteration 19, loss = 0.28462145\n",
      "Iteration 20, loss = 0.28031542\n",
      "Iteration 21, loss = 0.27710838\n",
      "Iteration 22, loss = 0.27255942\n",
      "Iteration 23, loss = 0.27301208\n",
      "Iteration 24, loss = 0.26559604\n",
      "Iteration 25, loss = 0.26614777\n",
      "Iteration 26, loss = 0.26195945\n",
      "Iteration 27, loss = 0.25874024\n",
      "Iteration 28, loss = 0.25435349\n",
      "Iteration 29, loss = 0.25420639\n",
      "Iteration 30, loss = 0.25128302\n",
      "Iteration 31, loss = 0.24917221\n",
      "Iteration 32, loss = 0.24443655\n",
      "Iteration 33, loss = 0.24626978\n",
      "Iteration 34, loss = 0.23973867\n",
      "Iteration 35, loss = 0.23977144\n",
      "Iteration 36, loss = 0.23652081\n",
      "Iteration 37, loss = 0.23654528\n",
      "Iteration 38, loss = 0.23130937\n",
      "Iteration 39, loss = 0.22885727\n",
      "Iteration 40, loss = 0.23452266\n",
      "Iteration 41, loss = 0.22650465\n",
      "Iteration 42, loss = 0.22339694\n",
      "Iteration 43, loss = 0.22214187\n",
      "Iteration 44, loss = 0.22254232\n",
      "Iteration 45, loss = 0.21877896\n",
      "Iteration 46, loss = 0.21900979\n",
      "Iteration 47, loss = 0.21718783\n",
      "Iteration 48, loss = 0.21430781\n",
      "Iteration 49, loss = 0.21363264\n",
      "Iteration 50, loss = 0.20978150\n",
      "Iteration 51, loss = 0.21180592\n",
      "Iteration 52, loss = 0.21239853\n",
      "Iteration 53, loss = 0.21009753\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62612663\n",
      "Iteration 2, loss = 0.45226906\n",
      "Iteration 3, loss = 0.40433847\n",
      "Iteration 4, loss = 0.37973799\n",
      "Iteration 5, loss = 0.36256378\n",
      "Iteration 6, loss = 0.35101649\n",
      "Iteration 7, loss = 0.33289486\n",
      "Iteration 8, loss = 0.32050992\n",
      "Iteration 9, loss = 0.31145423\n",
      "Iteration 10, loss = 0.30245805\n",
      "Iteration 11, loss = 0.29977081\n",
      "Iteration 12, loss = 0.29070125\n",
      "Iteration 13, loss = 0.28040101\n",
      "Iteration 14, loss = 0.27951846\n",
      "Iteration 15, loss = 0.27484241\n",
      "Iteration 16, loss = 0.26582179\n",
      "Iteration 17, loss = 0.26660676\n",
      "Iteration 18, loss = 0.25698199\n",
      "Iteration 19, loss = 0.25540968\n",
      "Iteration 20, loss = 0.24939432\n",
      "Iteration 21, loss = 0.24438314\n",
      "Iteration 22, loss = 0.23894128\n",
      "Iteration 23, loss = 0.23630638\n",
      "Iteration 24, loss = 0.23110751\n",
      "Iteration 25, loss = 0.23197504\n",
      "Iteration 26, loss = 0.22027530\n",
      "Iteration 27, loss = 0.22124405\n",
      "Iteration 28, loss = 0.22771010\n",
      "Iteration 29, loss = 0.21536410\n",
      "Iteration 30, loss = 0.21381153\n",
      "Iteration 31, loss = 0.20701700\n",
      "Iteration 32, loss = 0.20699037\n",
      "Iteration 33, loss = 0.21008387\n",
      "Iteration 34, loss = 0.20042681\n",
      "Iteration 35, loss = 0.19914973\n",
      "Iteration 36, loss = 0.19725301\n",
      "Iteration 37, loss = 0.19302705\n",
      "Iteration 38, loss = 0.19125107\n",
      "Iteration 39, loss = 0.19103570\n",
      "Iteration 40, loss = 0.18781562\n",
      "Iteration 41, loss = 0.18970893\n",
      "Iteration 42, loss = 0.18117769\n",
      "Iteration 43, loss = 0.18047240\n",
      "Iteration 44, loss = 0.17918698\n",
      "Iteration 45, loss = 0.18086338\n",
      "Iteration 46, loss = 0.17298407\n",
      "Iteration 47, loss = 0.17329880\n",
      "Iteration 48, loss = 0.16944227\n",
      "Iteration 49, loss = 0.17638089\n",
      "Iteration 50, loss = 0.16888016\n",
      "Iteration 51, loss = 0.16593750\n",
      "Iteration 52, loss = 0.16621698\n",
      "Iteration 53, loss = 0.17090781\n",
      "Iteration 54, loss = 0.16785135\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63389032\n",
      "Iteration 2, loss = 0.45658132\n",
      "Iteration 3, loss = 0.40842325\n",
      "Iteration 4, loss = 0.38234422\n",
      "Iteration 5, loss = 0.35861146\n",
      "Iteration 6, loss = 0.34766365\n",
      "Iteration 7, loss = 0.33646413\n",
      "Iteration 8, loss = 0.32764914\n",
      "Iteration 9, loss = 0.31454632\n",
      "Iteration 10, loss = 0.30633303\n",
      "Iteration 11, loss = 0.29773038\n",
      "Iteration 12, loss = 0.28622893\n",
      "Iteration 13, loss = 0.28440571\n",
      "Iteration 14, loss = 0.27936414\n",
      "Iteration 15, loss = 0.27525886\n",
      "Iteration 16, loss = 0.27084576\n",
      "Iteration 17, loss = 0.26016659\n",
      "Iteration 18, loss = 0.25474109\n",
      "Iteration 19, loss = 0.25567840\n",
      "Iteration 20, loss = 0.24816395\n",
      "Iteration 21, loss = 0.24526715\n",
      "Iteration 22, loss = 0.23787068\n",
      "Iteration 23, loss = 0.23571033\n",
      "Iteration 24, loss = 0.23389067\n",
      "Iteration 25, loss = 0.23079401\n",
      "Iteration 26, loss = 0.23127869\n",
      "Iteration 27, loss = 0.22785815\n",
      "Iteration 28, loss = 0.22793529\n",
      "Iteration 29, loss = 0.21989395\n",
      "Iteration 30, loss = 0.21549074\n",
      "Iteration 31, loss = 0.20730752\n",
      "Iteration 32, loss = 0.20924759\n",
      "Iteration 33, loss = 0.21425992\n",
      "Iteration 34, loss = 0.20142879\n",
      "Iteration 35, loss = 0.19791337\n",
      "Iteration 36, loss = 0.20299559\n",
      "Iteration 37, loss = 0.19848570\n",
      "Iteration 38, loss = 0.19545234\n",
      "Iteration 39, loss = 0.18679419\n",
      "Iteration 40, loss = 0.18788035\n",
      "Iteration 41, loss = 0.18577992\n",
      "Iteration 42, loss = 0.18591087\n",
      "Iteration 43, loss = 0.18237314\n",
      "Iteration 44, loss = 0.17775420\n",
      "Iteration 45, loss = 0.18323944\n",
      "Iteration 46, loss = 0.17642869\n",
      "Iteration 47, loss = 0.17142063\n",
      "Iteration 48, loss = 0.17251579\n",
      "Iteration 49, loss = 0.17526069\n",
      "Iteration 50, loss = 0.17872926\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62814937\n",
      "Iteration 2, loss = 0.45849293\n",
      "Iteration 3, loss = 0.40964141\n",
      "Iteration 4, loss = 0.38452327\n",
      "Iteration 5, loss = 0.36291248\n",
      "Iteration 6, loss = 0.34803914\n",
      "Iteration 7, loss = 0.33270204\n",
      "Iteration 8, loss = 0.32239215\n",
      "Iteration 9, loss = 0.31491624\n",
      "Iteration 10, loss = 0.30308089\n",
      "Iteration 11, loss = 0.29937979\n",
      "Iteration 12, loss = 0.28913073\n",
      "Iteration 13, loss = 0.28888707\n",
      "Iteration 14, loss = 0.28263720\n",
      "Iteration 15, loss = 0.27243003\n",
      "Iteration 16, loss = 0.26790006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.26143083\n",
      "Iteration 18, loss = 0.25451918\n",
      "Iteration 19, loss = 0.25323144\n",
      "Iteration 20, loss = 0.24623224\n",
      "Iteration 21, loss = 0.24288706\n",
      "Iteration 22, loss = 0.24113852\n",
      "Iteration 23, loss = 0.23769774\n",
      "Iteration 24, loss = 0.23114854\n",
      "Iteration 25, loss = 0.22945133\n",
      "Iteration 26, loss = 0.22406863\n",
      "Iteration 27, loss = 0.22364806\n",
      "Iteration 28, loss = 0.21571078\n",
      "Iteration 29, loss = 0.21628195\n",
      "Iteration 30, loss = 0.21275445\n",
      "Iteration 31, loss = 0.21295496\n",
      "Iteration 32, loss = 0.20984775\n",
      "Iteration 33, loss = 0.20521182\n",
      "Iteration 34, loss = 0.20031211\n",
      "Iteration 35, loss = 0.19985423\n",
      "Iteration 36, loss = 0.19257687\n",
      "Iteration 37, loss = 0.19152153\n",
      "Iteration 38, loss = 0.19241108\n",
      "Iteration 39, loss = 0.18966697\n",
      "Iteration 40, loss = 0.19222430\n",
      "Iteration 41, loss = 0.18457898\n",
      "Iteration 42, loss = 0.18571459\n",
      "Iteration 43, loss = 0.18366661\n",
      "Iteration 44, loss = 0.17912177\n",
      "Iteration 45, loss = 0.17653856\n",
      "Iteration 46, loss = 0.17740799\n",
      "Iteration 47, loss = 0.17433171\n",
      "Iteration 48, loss = 0.17128599\n",
      "Iteration 49, loss = 0.16868010\n",
      "Iteration 50, loss = 0.17439402\n",
      "Iteration 51, loss = 0.16810904\n",
      "Iteration 52, loss = 0.16604703\n",
      "Iteration 53, loss = 0.16685181\n",
      "Iteration 54, loss = 0.16006128\n",
      "Iteration 55, loss = 0.16486280\n",
      "Iteration 56, loss = 0.16283076\n",
      "Iteration 57, loss = 0.16364559\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65531788\n",
      "Iteration 2, loss = 0.44864060\n",
      "Iteration 3, loss = 0.40823602\n",
      "Iteration 4, loss = 0.38409646\n",
      "Iteration 5, loss = 0.36651388\n",
      "Iteration 6, loss = 0.34888738\n",
      "Iteration 7, loss = 0.33260114\n",
      "Iteration 8, loss = 0.32555492\n",
      "Iteration 9, loss = 0.31385466\n",
      "Iteration 10, loss = 0.30948255\n",
      "Iteration 11, loss = 0.30131365\n",
      "Iteration 12, loss = 0.29648811\n",
      "Iteration 13, loss = 0.28922143\n",
      "Iteration 14, loss = 0.28473501\n",
      "Iteration 15, loss = 0.27329344\n",
      "Iteration 16, loss = 0.27066905\n",
      "Iteration 17, loss = 0.26472123\n",
      "Iteration 18, loss = 0.26312188\n",
      "Iteration 19, loss = 0.25932049\n",
      "Iteration 20, loss = 0.25638371\n",
      "Iteration 21, loss = 0.24483009\n",
      "Iteration 22, loss = 0.24693135\n",
      "Iteration 23, loss = 0.24267693\n",
      "Iteration 24, loss = 0.23628899\n",
      "Iteration 25, loss = 0.23232914\n",
      "Iteration 26, loss = 0.23111178\n",
      "Iteration 27, loss = 0.22633927\n",
      "Iteration 28, loss = 0.22340609\n",
      "Iteration 29, loss = 0.22333160\n",
      "Iteration 30, loss = 0.22053988\n",
      "Iteration 31, loss = 0.21392937\n",
      "Iteration 32, loss = 0.21380278\n",
      "Iteration 33, loss = 0.21286309\n",
      "Iteration 34, loss = 0.21063387\n",
      "Iteration 35, loss = 0.20458185\n",
      "Iteration 36, loss = 0.20108434\n",
      "Iteration 37, loss = 0.20054204\n",
      "Iteration 38, loss = 0.19578528\n",
      "Iteration 39, loss = 0.19646468\n",
      "Iteration 40, loss = 0.19300122\n",
      "Iteration 41, loss = 0.19701794\n",
      "Iteration 42, loss = 0.18870463\n",
      "Iteration 43, loss = 0.18367288\n",
      "Iteration 44, loss = 0.19150258\n",
      "Iteration 45, loss = 0.18174694\n",
      "Iteration 46, loss = 0.18143082\n",
      "Iteration 47, loss = 0.17905759\n",
      "Iteration 48, loss = 0.17595097\n",
      "Iteration 49, loss = 0.18030551\n",
      "Iteration 50, loss = 0.17907349\n",
      "Iteration 51, loss = 0.17781312\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64672677\n",
      "Iteration 2, loss = 0.45254519\n",
      "Iteration 3, loss = 0.41053265\n",
      "Iteration 4, loss = 0.38470587\n",
      "Iteration 5, loss = 0.36606681\n",
      "Iteration 6, loss = 0.34871322\n",
      "Iteration 7, loss = 0.34061735\n",
      "Iteration 8, loss = 0.33128217\n",
      "Iteration 9, loss = 0.31443592\n",
      "Iteration 10, loss = 0.30993930\n",
      "Iteration 11, loss = 0.30233831\n",
      "Iteration 12, loss = 0.29464553\n",
      "Iteration 13, loss = 0.28703284\n",
      "Iteration 14, loss = 0.28339720\n",
      "Iteration 15, loss = 0.27590133\n",
      "Iteration 16, loss = 0.27143795\n",
      "Iteration 17, loss = 0.26433396\n",
      "Iteration 18, loss = 0.26400095\n",
      "Iteration 19, loss = 0.26014460\n",
      "Iteration 20, loss = 0.25631108\n",
      "Iteration 21, loss = 0.25164012\n",
      "Iteration 22, loss = 0.24634290\n",
      "Iteration 23, loss = 0.24057896\n",
      "Iteration 24, loss = 0.24202777\n",
      "Iteration 25, loss = 0.23440134\n",
      "Iteration 26, loss = 0.22985437\n",
      "Iteration 27, loss = 0.22836993\n",
      "Iteration 28, loss = 0.22300533\n",
      "Iteration 29, loss = 0.22143246\n",
      "Iteration 30, loss = 0.21991656\n",
      "Iteration 31, loss = 0.21670710\n",
      "Iteration 32, loss = 0.21125286\n",
      "Iteration 33, loss = 0.21048481\n",
      "Iteration 34, loss = 0.20986890\n",
      "Iteration 35, loss = 0.20344067\n",
      "Iteration 36, loss = 0.20554071\n",
      "Iteration 37, loss = 0.20094223\n",
      "Iteration 38, loss = 0.19375204\n",
      "Iteration 39, loss = 0.19143699\n",
      "Iteration 40, loss = 0.18897799\n",
      "Iteration 41, loss = 0.19732998\n",
      "Iteration 42, loss = 0.19485060\n",
      "Iteration 43, loss = 0.18800381\n",
      "Iteration 44, loss = 0.18766055\n",
      "Iteration 45, loss = 0.18319244\n",
      "Iteration 46, loss = 0.18560137\n",
      "Iteration 47, loss = 0.18465012\n",
      "Iteration 48, loss = 0.17338037\n",
      "Iteration 49, loss = 0.17580838\n",
      "Iteration 50, loss = 0.17613164\n",
      "Iteration 51, loss = 0.17148328\n",
      "Iteration 52, loss = 0.17104071\n",
      "Iteration 53, loss = 0.17361927\n",
      "Iteration 54, loss = 0.17473014\n",
      "Iteration 55, loss = 0.17065646\n",
      "Iteration 56, loss = 0.16837215\n",
      "Iteration 57, loss = 0.16881700\n",
      "Iteration 58, loss = 0.16883281\n",
      "Iteration 59, loss = 0.16245354\n",
      "Iteration 60, loss = 0.16038353\n",
      "Iteration 61, loss = 0.15879633\n",
      "Iteration 62, loss = 0.16409686\n",
      "Iteration 63, loss = 0.16343671\n",
      "Iteration 64, loss = 0.15795241\n",
      "Iteration 65, loss = 0.15860336\n",
      "Iteration 66, loss = 0.15503848\n",
      "Iteration 67, loss = 0.15321257\n",
      "Iteration 68, loss = 0.15530182\n",
      "Iteration 69, loss = 0.15705810\n",
      "Iteration 70, loss = 0.15414353\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65135417\n",
      "Iteration 2, loss = 0.45219634\n",
      "Iteration 3, loss = 0.40917418\n",
      "Iteration 4, loss = 0.38725205\n",
      "Iteration 5, loss = 0.36620123\n",
      "Iteration 6, loss = 0.35096617\n",
      "Iteration 7, loss = 0.33864895\n",
      "Iteration 8, loss = 0.32654487\n",
      "Iteration 9, loss = 0.31226763\n",
      "Iteration 10, loss = 0.30871541\n",
      "Iteration 11, loss = 0.29826478\n",
      "Iteration 12, loss = 0.29745188\n",
      "Iteration 13, loss = 0.28815102\n",
      "Iteration 14, loss = 0.28025547\n",
      "Iteration 15, loss = 0.27822073\n",
      "Iteration 16, loss = 0.27376444\n",
      "Iteration 17, loss = 0.26189195\n",
      "Iteration 18, loss = 0.25702981\n",
      "Iteration 19, loss = 0.25270481\n",
      "Iteration 20, loss = 0.25058681\n",
      "Iteration 21, loss = 0.24892679\n",
      "Iteration 22, loss = 0.24504030\n",
      "Iteration 23, loss = 0.23997186\n",
      "Iteration 24, loss = 0.23805131\n",
      "Iteration 25, loss = 0.23269915\n",
      "Iteration 26, loss = 0.23003970\n",
      "Iteration 27, loss = 0.22381239\n",
      "Iteration 28, loss = 0.22150105\n",
      "Iteration 29, loss = 0.22230775\n",
      "Iteration 30, loss = 0.22182744\n",
      "Iteration 31, loss = 0.21420922\n",
      "Iteration 32, loss = 0.20741292\n",
      "Iteration 33, loss = 0.20496033\n",
      "Iteration 34, loss = 0.20738803\n",
      "Iteration 35, loss = 0.20440035\n",
      "Iteration 36, loss = 0.20140947\n",
      "Iteration 37, loss = 0.19851980\n",
      "Iteration 38, loss = 0.19344530\n",
      "Iteration 39, loss = 0.19498782\n",
      "Iteration 40, loss = 0.19296233\n",
      "Iteration 41, loss = 0.19002615\n",
      "Iteration 42, loss = 0.18995232\n",
      "Iteration 43, loss = 0.18202542\n",
      "Iteration 44, loss = 0.18476099\n",
      "Iteration 45, loss = 0.18344027\n",
      "Iteration 46, loss = 0.18295506\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63166955\n",
      "Iteration 2, loss = 0.44979908\n",
      "Iteration 3, loss = 0.40868860\n",
      "Iteration 4, loss = 0.38503825\n",
      "Iteration 5, loss = 0.36387431\n",
      "Iteration 6, loss = 0.34612468\n",
      "Iteration 7, loss = 0.33297492\n",
      "Iteration 8, loss = 0.32346672\n",
      "Iteration 9, loss = 0.31591029\n",
      "Iteration 10, loss = 0.30697778\n",
      "Iteration 11, loss = 0.30027894\n",
      "Iteration 12, loss = 0.29164189\n",
      "Iteration 13, loss = 0.28400441\n",
      "Iteration 14, loss = 0.27966950\n",
      "Iteration 15, loss = 0.27819664\n",
      "Iteration 16, loss = 0.27180489\n",
      "Iteration 17, loss = 0.26353669\n",
      "Iteration 18, loss = 0.25970175\n",
      "Iteration 19, loss = 0.25361492\n",
      "Iteration 20, loss = 0.25009302\n",
      "Iteration 21, loss = 0.25031058\n",
      "Iteration 22, loss = 0.24466894\n",
      "Iteration 23, loss = 0.24018675\n",
      "Iteration 24, loss = 0.23663663\n",
      "Iteration 25, loss = 0.23609581\n",
      "Iteration 26, loss = 0.22705095\n",
      "Iteration 27, loss = 0.22756356\n",
      "Iteration 28, loss = 0.22045420\n",
      "Iteration 29, loss = 0.21951721\n",
      "Iteration 30, loss = 0.21929778\n",
      "Iteration 31, loss = 0.21248901\n",
      "Iteration 32, loss = 0.21398709\n",
      "Iteration 33, loss = 0.20687952\n",
      "Iteration 34, loss = 0.21187859\n",
      "Iteration 35, loss = 0.20995620\n",
      "Iteration 36, loss = 0.20164529\n",
      "Iteration 37, loss = 0.20065965\n",
      "Iteration 38, loss = 0.19796435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39, loss = 0.19115705\n",
      "Iteration 40, loss = 0.19290356\n",
      "Iteration 41, loss = 0.18837815\n",
      "Iteration 42, loss = 0.19163467\n",
      "Iteration 43, loss = 0.18565535\n",
      "Iteration 44, loss = 0.18871012\n",
      "Iteration 45, loss = 0.18223685\n",
      "Iteration 46, loss = 0.18710795\n",
      "Iteration 47, loss = 0.18029156\n",
      "Iteration 48, loss = 0.17907438\n",
      "Iteration 49, loss = 0.17394257\n",
      "Iteration 50, loss = 0.17521323\n",
      "Iteration 51, loss = 0.17590668\n",
      "Iteration 52, loss = 0.16679221\n",
      "Iteration 53, loss = 0.16721954\n",
      "Iteration 54, loss = 0.16837546\n",
      "Iteration 55, loss = 0.16976185\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63350307\n",
      "Iteration 2, loss = 0.45947214\n",
      "Iteration 3, loss = 0.41037698\n",
      "Iteration 4, loss = 0.38472857\n",
      "Iteration 5, loss = 0.36501094\n",
      "Iteration 6, loss = 0.34814464\n",
      "Iteration 7, loss = 0.32980032\n",
      "Iteration 8, loss = 0.32488960\n",
      "Iteration 9, loss = 0.31527514\n",
      "Iteration 10, loss = 0.30526272\n",
      "Iteration 11, loss = 0.29975924\n",
      "Iteration 12, loss = 0.29028708\n",
      "Iteration 13, loss = 0.28550911\n",
      "Iteration 14, loss = 0.28026405\n",
      "Iteration 15, loss = 0.27421860\n",
      "Iteration 16, loss = 0.26800432\n",
      "Iteration 17, loss = 0.26288761\n",
      "Iteration 18, loss = 0.25928806\n",
      "Iteration 19, loss = 0.25362160\n",
      "Iteration 20, loss = 0.24847793\n",
      "Iteration 21, loss = 0.25056611\n",
      "Iteration 22, loss = 0.24149388\n",
      "Iteration 23, loss = 0.23910835\n",
      "Iteration 24, loss = 0.23713421\n",
      "Iteration 25, loss = 0.23411531\n",
      "Iteration 26, loss = 0.22645586\n",
      "Iteration 27, loss = 0.22401124\n",
      "Iteration 28, loss = 0.21820780\n",
      "Iteration 29, loss = 0.22328963\n",
      "Iteration 30, loss = 0.22039256\n",
      "Iteration 31, loss = 0.21375390\n",
      "Iteration 32, loss = 0.21299412\n",
      "Iteration 33, loss = 0.21022705\n",
      "Iteration 34, loss = 0.20893815\n",
      "Iteration 35, loss = 0.20353567\n",
      "Iteration 36, loss = 0.20244379\n",
      "Iteration 37, loss = 0.20122439\n",
      "Iteration 38, loss = 0.19627837\n",
      "Iteration 39, loss = 0.19396694\n",
      "Iteration 40, loss = 0.19282519\n",
      "Iteration 41, loss = 0.19629095\n",
      "Iteration 42, loss = 0.18920424\n",
      "Iteration 43, loss = 0.18792811\n",
      "Iteration 44, loss = 0.18578211\n",
      "Iteration 45, loss = 0.18310739\n",
      "Iteration 46, loss = 0.17899411\n",
      "Iteration 47, loss = 0.17741493\n",
      "Iteration 48, loss = 0.17663809\n",
      "Iteration 49, loss = 0.17681885\n",
      "Iteration 50, loss = 0.16980804\n",
      "Iteration 51, loss = 0.17962271\n",
      "Iteration 52, loss = 0.17506892\n",
      "Iteration 53, loss = 0.17445251\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63738363\n",
      "Iteration 2, loss = 0.45897124\n",
      "Iteration 3, loss = 0.41139863\n",
      "Iteration 4, loss = 0.38258429\n",
      "Iteration 5, loss = 0.36974562\n",
      "Iteration 6, loss = 0.35457758\n",
      "Iteration 7, loss = 0.33548273\n",
      "Iteration 8, loss = 0.32726095\n",
      "Iteration 9, loss = 0.31266256\n",
      "Iteration 10, loss = 0.30645156\n",
      "Iteration 11, loss = 0.30144987\n",
      "Iteration 12, loss = 0.29259539\n",
      "Iteration 13, loss = 0.28505190\n",
      "Iteration 14, loss = 0.28466381\n",
      "Iteration 15, loss = 0.27411509\n",
      "Iteration 16, loss = 0.27170119\n",
      "Iteration 17, loss = 0.26539469\n",
      "Iteration 18, loss = 0.25871558\n",
      "Iteration 19, loss = 0.26150971\n",
      "Iteration 20, loss = 0.25456478\n",
      "Iteration 21, loss = 0.24768444\n",
      "Iteration 22, loss = 0.24432981\n",
      "Iteration 23, loss = 0.23666560\n",
      "Iteration 24, loss = 0.23848533\n",
      "Iteration 25, loss = 0.22977975\n",
      "Iteration 26, loss = 0.22898609\n",
      "Iteration 27, loss = 0.23117143\n",
      "Iteration 28, loss = 0.22206268\n",
      "Iteration 29, loss = 0.22664435\n",
      "Iteration 30, loss = 0.21715093\n",
      "Iteration 31, loss = 0.21583394\n",
      "Iteration 32, loss = 0.21550568\n",
      "Iteration 33, loss = 0.21629029\n",
      "Iteration 34, loss = 0.20695370\n",
      "Iteration 35, loss = 0.20845128\n",
      "Iteration 36, loss = 0.20221056\n",
      "Iteration 37, loss = 0.20200280\n",
      "Iteration 38, loss = 0.20172566\n",
      "Iteration 39, loss = 0.19726180\n",
      "Iteration 40, loss = 0.19431160\n",
      "Iteration 41, loss = 0.19762127\n",
      "Iteration 42, loss = 0.18967755\n",
      "Iteration 43, loss = 0.19298824\n",
      "Iteration 44, loss = 0.18805467\n",
      "Iteration 45, loss = 0.18322058\n",
      "Iteration 46, loss = 0.18560988\n",
      "Iteration 47, loss = 0.18257454\n",
      "Iteration 48, loss = 0.18155544\n",
      "Iteration 49, loss = 0.17753432\n",
      "Iteration 50, loss = 0.17463802\n",
      "Iteration 51, loss = 0.17779428\n",
      "Iteration 52, loss = 0.17681659\n",
      "Iteration 53, loss = 0.17338888\n",
      "Iteration 54, loss = 0.17042645\n",
      "Iteration 55, loss = 0.17407127\n",
      "Iteration 56, loss = 0.17073649\n",
      "Iteration 57, loss = 0.16762984\n",
      "Iteration 58, loss = 0.16798663\n",
      "Iteration 59, loss = 0.16364217\n",
      "Iteration 60, loss = 0.16589972\n",
      "Iteration 61, loss = 0.16468798\n",
      "Iteration 62, loss = 0.15631191\n",
      "Iteration 63, loss = 0.16118119\n",
      "Iteration 64, loss = 0.15136980\n",
      "Iteration 65, loss = 0.15543867\n",
      "Iteration 66, loss = 0.16431204\n",
      "Iteration 67, loss = 0.15348498\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70604537\n",
      "Iteration 2, loss = 0.52547613\n",
      "Iteration 3, loss = 0.48310578\n",
      "Iteration 4, loss = 0.45667556\n",
      "Iteration 5, loss = 0.43180372\n",
      "Iteration 6, loss = 0.41649899\n",
      "Iteration 7, loss = 0.40456062\n",
      "Iteration 8, loss = 0.39339886\n",
      "Iteration 9, loss = 0.38801812\n",
      "Iteration 10, loss = 0.37758906\n",
      "Iteration 11, loss = 0.37385261\n",
      "Iteration 12, loss = 0.37210604\n",
      "Iteration 13, loss = 0.36462053\n",
      "Iteration 14, loss = 0.36135161\n",
      "Iteration 15, loss = 0.35316237\n",
      "Iteration 16, loss = 0.35082544\n",
      "Iteration 17, loss = 0.34274206\n",
      "Iteration 18, loss = 0.34220458\n",
      "Iteration 19, loss = 0.34267640\n",
      "Iteration 20, loss = 0.33987078\n",
      "Iteration 21, loss = 0.33833187\n",
      "Iteration 22, loss = 0.33484469\n",
      "Iteration 23, loss = 0.33085860\n",
      "Iteration 24, loss = 0.32989776\n",
      "Iteration 25, loss = 0.32691900\n",
      "Iteration 26, loss = 0.32616407\n",
      "Iteration 27, loss = 0.32251676\n",
      "Iteration 28, loss = 0.32134044\n",
      "Iteration 29, loss = 0.31932844\n",
      "Iteration 30, loss = 0.31855482\n",
      "Iteration 31, loss = 0.31571575\n",
      "Iteration 32, loss = 0.31920728\n",
      "Iteration 33, loss = 0.31379912\n",
      "Iteration 34, loss = 0.30882742\n",
      "Iteration 35, loss = 0.30919950\n",
      "Iteration 36, loss = 0.31382874\n",
      "Iteration 37, loss = 0.30620560\n",
      "Iteration 38, loss = 0.30986957\n",
      "Iteration 39, loss = 0.30864903\n",
      "Iteration 40, loss = 0.30597298\n",
      "Iteration 41, loss = 0.30926839\n",
      "Iteration 42, loss = 0.30250170\n",
      "Iteration 43, loss = 0.30342350\n",
      "Iteration 44, loss = 0.30132671\n",
      "Iteration 45, loss = 0.30509138\n",
      "Iteration 46, loss = 0.30326405\n",
      "Iteration 47, loss = 0.29914979\n",
      "Iteration 48, loss = 0.29835865\n",
      "Iteration 49, loss = 0.29875873\n",
      "Iteration 50, loss = 0.29509758\n",
      "Iteration 51, loss = 0.29730624\n",
      "Iteration 52, loss = 0.29540904\n",
      "Iteration 53, loss = 0.29376024\n",
      "Iteration 54, loss = 0.29411290\n",
      "Iteration 55, loss = 0.29217053\n",
      "Iteration 56, loss = 0.29325894\n",
      "Iteration 57, loss = 0.29045935\n",
      "Iteration 58, loss = 0.28941302\n",
      "Iteration 59, loss = 0.29458761\n",
      "Iteration 60, loss = 0.29286357\n",
      "Iteration 61, loss = 0.28969673\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70589136\n",
      "Iteration 2, loss = 0.52420034\n",
      "Iteration 3, loss = 0.49131728\n",
      "Iteration 4, loss = 0.45788579\n",
      "Iteration 5, loss = 0.43292952\n",
      "Iteration 6, loss = 0.42759706\n",
      "Iteration 7, loss = 0.41190426\n",
      "Iteration 8, loss = 0.40662026\n",
      "Iteration 9, loss = 0.39369749\n",
      "Iteration 10, loss = 0.39094485\n",
      "Iteration 11, loss = 0.37858259\n",
      "Iteration 12, loss = 0.37325761\n",
      "Iteration 13, loss = 0.37256787\n",
      "Iteration 14, loss = 0.36515430\n",
      "Iteration 15, loss = 0.35546875\n",
      "Iteration 16, loss = 0.35801526\n",
      "Iteration 17, loss = 0.35017832\n",
      "Iteration 18, loss = 0.34630243\n",
      "Iteration 19, loss = 0.34288624\n",
      "Iteration 20, loss = 0.34368372\n",
      "Iteration 21, loss = 0.33975756\n",
      "Iteration 22, loss = 0.33325658\n",
      "Iteration 23, loss = 0.33310191\n",
      "Iteration 24, loss = 0.32808322\n",
      "Iteration 25, loss = 0.32562778\n",
      "Iteration 26, loss = 0.33223059\n",
      "Iteration 27, loss = 0.32561309\n",
      "Iteration 28, loss = 0.32216736\n",
      "Iteration 29, loss = 0.32206860\n",
      "Iteration 30, loss = 0.32125210\n",
      "Iteration 31, loss = 0.31968944\n",
      "Iteration 32, loss = 0.32206866\n",
      "Iteration 33, loss = 0.31624111\n",
      "Iteration 34, loss = 0.30935816\n",
      "Iteration 35, loss = 0.31269287\n",
      "Iteration 36, loss = 0.31087013\n",
      "Iteration 37, loss = 0.30863563\n",
      "Iteration 38, loss = 0.30687098\n",
      "Iteration 39, loss = 0.30530471\n",
      "Iteration 40, loss = 0.31029440\n",
      "Iteration 41, loss = 0.30457178\n",
      "Iteration 42, loss = 0.30472057\n",
      "Iteration 43, loss = 0.30724541\n",
      "Iteration 44, loss = 0.30510057\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71396300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.53361404\n",
      "Iteration 3, loss = 0.48239099\n",
      "Iteration 4, loss = 0.45482940\n",
      "Iteration 5, loss = 0.43147444\n",
      "Iteration 6, loss = 0.42214890\n",
      "Iteration 7, loss = 0.40750016\n",
      "Iteration 8, loss = 0.39801030\n",
      "Iteration 9, loss = 0.38921151\n",
      "Iteration 10, loss = 0.37910431\n",
      "Iteration 11, loss = 0.37388988\n",
      "Iteration 12, loss = 0.36839365\n",
      "Iteration 13, loss = 0.36777847\n",
      "Iteration 14, loss = 0.36079164\n",
      "Iteration 15, loss = 0.35278565\n",
      "Iteration 16, loss = 0.35221345\n",
      "Iteration 17, loss = 0.34661387\n",
      "Iteration 18, loss = 0.33965977\n",
      "Iteration 19, loss = 0.34069337\n",
      "Iteration 20, loss = 0.33728993\n",
      "Iteration 21, loss = 0.33744892\n",
      "Iteration 22, loss = 0.33085956\n",
      "Iteration 23, loss = 0.33174003\n",
      "Iteration 24, loss = 0.32730164\n",
      "Iteration 25, loss = 0.32949051\n",
      "Iteration 26, loss = 0.32640854\n",
      "Iteration 27, loss = 0.32084121\n",
      "Iteration 28, loss = 0.32578118\n",
      "Iteration 29, loss = 0.31775382\n",
      "Iteration 30, loss = 0.31966870\n",
      "Iteration 31, loss = 0.32004222\n",
      "Iteration 32, loss = 0.31644716\n",
      "Iteration 33, loss = 0.31210629\n",
      "Iteration 34, loss = 0.31221874\n",
      "Iteration 35, loss = 0.30993221\n",
      "Iteration 36, loss = 0.31123178\n",
      "Iteration 37, loss = 0.30882354\n",
      "Iteration 38, loss = 0.30732156\n",
      "Iteration 39, loss = 0.30913356\n",
      "Iteration 40, loss = 0.30447693\n",
      "Iteration 41, loss = 0.30493751\n",
      "Iteration 42, loss = 0.30680767\n",
      "Iteration 43, loss = 0.30401857\n",
      "Iteration 44, loss = 0.30197096\n",
      "Iteration 45, loss = 0.29961797\n",
      "Iteration 46, loss = 0.30046224\n",
      "Iteration 47, loss = 0.29871483\n",
      "Iteration 48, loss = 0.30006864\n",
      "Iteration 49, loss = 0.29717264\n",
      "Iteration 50, loss = 0.30061042\n",
      "Iteration 51, loss = 0.29606037\n",
      "Iteration 52, loss = 0.29568205\n",
      "Iteration 53, loss = 0.29419701\n",
      "Iteration 54, loss = 0.29249247\n",
      "Iteration 55, loss = 0.29157790\n",
      "Iteration 56, loss = 0.29545348\n",
      "Iteration 57, loss = 0.29209812\n",
      "Iteration 58, loss = 0.29426545\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70315746\n",
      "Iteration 2, loss = 0.51869214\n",
      "Iteration 3, loss = 0.47209276\n",
      "Iteration 4, loss = 0.44786902\n",
      "Iteration 5, loss = 0.42696265\n",
      "Iteration 6, loss = 0.41503845\n",
      "Iteration 7, loss = 0.40674265\n",
      "Iteration 8, loss = 0.39302775\n",
      "Iteration 9, loss = 0.38601678\n",
      "Iteration 10, loss = 0.37587374\n",
      "Iteration 11, loss = 0.37320432\n",
      "Iteration 12, loss = 0.37064278\n",
      "Iteration 13, loss = 0.36077740\n",
      "Iteration 14, loss = 0.36375448\n",
      "Iteration 15, loss = 0.35500853\n",
      "Iteration 16, loss = 0.35166558\n",
      "Iteration 17, loss = 0.34777902\n",
      "Iteration 18, loss = 0.34616695\n",
      "Iteration 19, loss = 0.34542372\n",
      "Iteration 20, loss = 0.33817303\n",
      "Iteration 21, loss = 0.33831454\n",
      "Iteration 22, loss = 0.33285723\n",
      "Iteration 23, loss = 0.33036617\n",
      "Iteration 24, loss = 0.33176782\n",
      "Iteration 25, loss = 0.32796059\n",
      "Iteration 26, loss = 0.32736444\n",
      "Iteration 27, loss = 0.32341641\n",
      "Iteration 28, loss = 0.32574583\n",
      "Iteration 29, loss = 0.32217488\n",
      "Iteration 30, loss = 0.32640926\n",
      "Iteration 31, loss = 0.31759841\n",
      "Iteration 32, loss = 0.31589678\n",
      "Iteration 33, loss = 0.31281363\n",
      "Iteration 34, loss = 0.31393235\n",
      "Iteration 35, loss = 0.31265703\n",
      "Iteration 36, loss = 0.31108381\n",
      "Iteration 37, loss = 0.30895482\n",
      "Iteration 38, loss = 0.30677061\n",
      "Iteration 39, loss = 0.30643364\n",
      "Iteration 40, loss = 0.30398929\n",
      "Iteration 41, loss = 0.30591015\n",
      "Iteration 42, loss = 0.30517162\n",
      "Iteration 43, loss = 0.30203788\n",
      "Iteration 44, loss = 0.30393911\n",
      "Iteration 45, loss = 0.30353001\n",
      "Iteration 46, loss = 0.30137466\n",
      "Iteration 47, loss = 0.29836781\n",
      "Iteration 48, loss = 0.30197673\n",
      "Iteration 49, loss = 0.29610820\n",
      "Iteration 50, loss = 0.29807961\n",
      "Iteration 51, loss = 0.29406914\n",
      "Iteration 52, loss = 0.30107863\n",
      "Iteration 53, loss = 0.29458649\n",
      "Iteration 54, loss = 0.29248549\n",
      "Iteration 55, loss = 0.29713022\n",
      "Iteration 56, loss = 0.29730790\n",
      "Iteration 57, loss = 0.29311240\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71007737\n",
      "Iteration 2, loss = 0.51730240\n",
      "Iteration 3, loss = 0.47740714\n",
      "Iteration 4, loss = 0.45069038\n",
      "Iteration 5, loss = 0.43624891\n",
      "Iteration 6, loss = 0.42148459\n",
      "Iteration 7, loss = 0.40659013\n",
      "Iteration 8, loss = 0.39986950\n",
      "Iteration 9, loss = 0.39217242\n",
      "Iteration 10, loss = 0.38391647\n",
      "Iteration 11, loss = 0.37700553\n",
      "Iteration 12, loss = 0.37429723\n",
      "Iteration 13, loss = 0.36719814\n",
      "Iteration 14, loss = 0.36622527\n",
      "Iteration 15, loss = 0.36008325\n",
      "Iteration 16, loss = 0.35103581\n",
      "Iteration 17, loss = 0.35062399\n",
      "Iteration 18, loss = 0.34583718\n",
      "Iteration 19, loss = 0.34568991\n",
      "Iteration 20, loss = 0.34224561\n",
      "Iteration 21, loss = 0.33835293\n",
      "Iteration 22, loss = 0.33951340\n",
      "Iteration 23, loss = 0.33584484\n",
      "Iteration 24, loss = 0.33014388\n",
      "Iteration 25, loss = 0.32761516\n",
      "Iteration 26, loss = 0.32976035\n",
      "Iteration 27, loss = 0.32350366\n",
      "Iteration 28, loss = 0.32539521\n",
      "Iteration 29, loss = 0.32436045\n",
      "Iteration 30, loss = 0.32155500\n",
      "Iteration 31, loss = 0.32088608\n",
      "Iteration 32, loss = 0.31569733\n",
      "Iteration 33, loss = 0.31556200\n",
      "Iteration 34, loss = 0.31477818\n",
      "Iteration 35, loss = 0.30929791\n",
      "Iteration 36, loss = 0.31385506\n",
      "Iteration 37, loss = 0.31064085\n",
      "Iteration 38, loss = 0.30960463\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70879563\n",
      "Iteration 2, loss = 0.51490751\n",
      "Iteration 3, loss = 0.47692144\n",
      "Iteration 4, loss = 0.45185597\n",
      "Iteration 5, loss = 0.42952309\n",
      "Iteration 6, loss = 0.42361993\n",
      "Iteration 7, loss = 0.40566935\n",
      "Iteration 8, loss = 0.39509585\n",
      "Iteration 9, loss = 0.38904488\n",
      "Iteration 10, loss = 0.38235359\n",
      "Iteration 11, loss = 0.37826892\n",
      "Iteration 12, loss = 0.37268494\n",
      "Iteration 13, loss = 0.36410959\n",
      "Iteration 14, loss = 0.36303152\n",
      "Iteration 15, loss = 0.35898568\n",
      "Iteration 16, loss = 0.35212742\n",
      "Iteration 17, loss = 0.34597355\n",
      "Iteration 18, loss = 0.34208304\n",
      "Iteration 19, loss = 0.34497816\n",
      "Iteration 20, loss = 0.34120740\n",
      "Iteration 21, loss = 0.33731461\n",
      "Iteration 22, loss = 0.33511758\n",
      "Iteration 23, loss = 0.33456936\n",
      "Iteration 24, loss = 0.33128786\n",
      "Iteration 25, loss = 0.33280376\n",
      "Iteration 26, loss = 0.32831201\n",
      "Iteration 27, loss = 0.32812298\n",
      "Iteration 28, loss = 0.32037148\n",
      "Iteration 29, loss = 0.32314311\n",
      "Iteration 30, loss = 0.31990402\n",
      "Iteration 31, loss = 0.32185406\n",
      "Iteration 32, loss = 0.31457993\n",
      "Iteration 33, loss = 0.31851471\n",
      "Iteration 34, loss = 0.31220724\n",
      "Iteration 35, loss = 0.31353211\n",
      "Iteration 36, loss = 0.31176093\n",
      "Iteration 37, loss = 0.30938711\n",
      "Iteration 38, loss = 0.30811343\n",
      "Iteration 39, loss = 0.30306116\n",
      "Iteration 40, loss = 0.31119290\n",
      "Iteration 41, loss = 0.30356824\n",
      "Iteration 42, loss = 0.30248580\n",
      "Iteration 43, loss = 0.30329344\n",
      "Iteration 44, loss = 0.30150407\n",
      "Iteration 45, loss = 0.29949588\n",
      "Iteration 46, loss = 0.30151191\n",
      "Iteration 47, loss = 0.29963546\n",
      "Iteration 48, loss = 0.29779613\n",
      "Iteration 49, loss = 0.29796204\n",
      "Iteration 50, loss = 0.29433861\n",
      "Iteration 51, loss = 0.29446452\n",
      "Iteration 52, loss = 0.29779726\n",
      "Iteration 53, loss = 0.29819592\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73226625\n",
      "Iteration 2, loss = 0.54347902\n",
      "Iteration 3, loss = 0.48800464\n",
      "Iteration 4, loss = 0.45827501\n",
      "Iteration 5, loss = 0.43684248\n",
      "Iteration 6, loss = 0.42412332\n",
      "Iteration 7, loss = 0.40391929\n",
      "Iteration 8, loss = 0.39309718\n",
      "Iteration 9, loss = 0.38164460\n",
      "Iteration 10, loss = 0.37095793\n",
      "Iteration 11, loss = 0.36779961\n",
      "Iteration 12, loss = 0.35817328\n",
      "Iteration 13, loss = 0.35098664\n",
      "Iteration 14, loss = 0.34724751\n",
      "Iteration 15, loss = 0.34005608\n",
      "Iteration 16, loss = 0.33542097\n",
      "Iteration 17, loss = 0.33676656\n",
      "Iteration 18, loss = 0.33016895\n",
      "Iteration 19, loss = 0.33034672\n",
      "Iteration 20, loss = 0.31961629\n",
      "Iteration 21, loss = 0.31633552\n",
      "Iteration 22, loss = 0.31342183\n",
      "Iteration 23, loss = 0.31280134\n",
      "Iteration 24, loss = 0.30444868\n",
      "Iteration 25, loss = 0.30347319\n",
      "Iteration 26, loss = 0.29892575\n",
      "Iteration 27, loss = 0.29536173\n",
      "Iteration 28, loss = 0.29774579\n",
      "Iteration 29, loss = 0.29242152\n",
      "Iteration 30, loss = 0.28706709\n",
      "Iteration 31, loss = 0.28661836\n",
      "Iteration 32, loss = 0.28568298\n",
      "Iteration 33, loss = 0.28509076\n",
      "Iteration 34, loss = 0.28225245\n",
      "Iteration 35, loss = 0.28224863\n",
      "Iteration 36, loss = 0.27884390\n",
      "Iteration 37, loss = 0.27516234\n",
      "Iteration 38, loss = 0.27185845\n",
      "Iteration 39, loss = 0.27821285\n",
      "Iteration 40, loss = 0.27393107\n",
      "Iteration 41, loss = 0.26769696\n",
      "Iteration 42, loss = 0.27002003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43, loss = 0.26922549\n",
      "Iteration 44, loss = 0.26275672\n",
      "Iteration 45, loss = 0.26537713\n",
      "Iteration 46, loss = 0.26147948\n",
      "Iteration 47, loss = 0.26033036\n",
      "Iteration 48, loss = 0.26166224\n",
      "Iteration 49, loss = 0.26421000\n",
      "Iteration 50, loss = 0.25895058\n",
      "Iteration 51, loss = 0.25256158\n",
      "Iteration 52, loss = 0.25670573\n",
      "Iteration 53, loss = 0.25317347\n",
      "Iteration 54, loss = 0.25528699\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73865040\n",
      "Iteration 2, loss = 0.54583473\n",
      "Iteration 3, loss = 0.49281381\n",
      "Iteration 4, loss = 0.46169453\n",
      "Iteration 5, loss = 0.43473658\n",
      "Iteration 6, loss = 0.42215387\n",
      "Iteration 7, loss = 0.40916420\n",
      "Iteration 8, loss = 0.39479230\n",
      "Iteration 9, loss = 0.38422318\n",
      "Iteration 10, loss = 0.37141392\n",
      "Iteration 11, loss = 0.36415976\n",
      "Iteration 12, loss = 0.35503369\n",
      "Iteration 13, loss = 0.35003392\n",
      "Iteration 14, loss = 0.34562707\n",
      "Iteration 15, loss = 0.33805900\n",
      "Iteration 16, loss = 0.33409458\n",
      "Iteration 17, loss = 0.32904224\n",
      "Iteration 18, loss = 0.32105680\n",
      "Iteration 19, loss = 0.32525030\n",
      "Iteration 20, loss = 0.31579401\n",
      "Iteration 21, loss = 0.31538271\n",
      "Iteration 22, loss = 0.31155390\n",
      "Iteration 23, loss = 0.30402898\n",
      "Iteration 24, loss = 0.30224964\n",
      "Iteration 25, loss = 0.30026784\n",
      "Iteration 26, loss = 0.29719037\n",
      "Iteration 27, loss = 0.29877526\n",
      "Iteration 28, loss = 0.29397552\n",
      "Iteration 29, loss = 0.28720668\n",
      "Iteration 30, loss = 0.28997887\n",
      "Iteration 31, loss = 0.28310000\n",
      "Iteration 32, loss = 0.28881146\n",
      "Iteration 33, loss = 0.28413904\n",
      "Iteration 34, loss = 0.27462738\n",
      "Iteration 35, loss = 0.27625412\n",
      "Iteration 36, loss = 0.27539401\n",
      "Iteration 37, loss = 0.27375376\n",
      "Iteration 38, loss = 0.27411608\n",
      "Iteration 39, loss = 0.27059659\n",
      "Iteration 40, loss = 0.26496974\n",
      "Iteration 41, loss = 0.26778680\n",
      "Iteration 42, loss = 0.26404897\n",
      "Iteration 43, loss = 0.26151425\n",
      "Iteration 44, loss = 0.26389284\n",
      "Iteration 45, loss = 0.26540832\n",
      "Iteration 46, loss = 0.25829512\n",
      "Iteration 47, loss = 0.25681549\n",
      "Iteration 48, loss = 0.25669681\n",
      "Iteration 49, loss = 0.26090921\n",
      "Iteration 50, loss = 0.25784545\n",
      "Iteration 51, loss = 0.25596019\n",
      "Iteration 52, loss = 0.25502099\n",
      "Iteration 53, loss = 0.25393325\n",
      "Iteration 54, loss = 0.25237045\n",
      "Iteration 55, loss = 0.25467740\n",
      "Iteration 56, loss = 0.24546212\n",
      "Iteration 57, loss = 0.24645725\n",
      "Iteration 58, loss = 0.24361421\n",
      "Iteration 59, loss = 0.24375058\n",
      "Iteration 60, loss = 0.25016194\n",
      "Iteration 61, loss = 0.24481000\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73293408\n",
      "Iteration 2, loss = 0.54809852\n",
      "Iteration 3, loss = 0.49053336\n",
      "Iteration 4, loss = 0.46270546\n",
      "Iteration 5, loss = 0.43674695\n",
      "Iteration 6, loss = 0.41864824\n",
      "Iteration 7, loss = 0.40066866\n",
      "Iteration 8, loss = 0.39016860\n",
      "Iteration 9, loss = 0.38441768\n",
      "Iteration 10, loss = 0.36812417\n",
      "Iteration 11, loss = 0.36295162\n",
      "Iteration 12, loss = 0.35290999\n",
      "Iteration 13, loss = 0.35408993\n",
      "Iteration 14, loss = 0.34764415\n",
      "Iteration 15, loss = 0.33667287\n",
      "Iteration 16, loss = 0.33247498\n",
      "Iteration 17, loss = 0.32853774\n",
      "Iteration 18, loss = 0.31982468\n",
      "Iteration 19, loss = 0.32202744\n",
      "Iteration 20, loss = 0.31736052\n",
      "Iteration 21, loss = 0.31453262\n",
      "Iteration 22, loss = 0.30827621\n",
      "Iteration 23, loss = 0.30771232\n",
      "Iteration 24, loss = 0.30193267\n",
      "Iteration 25, loss = 0.30017347\n",
      "Iteration 26, loss = 0.29304906\n",
      "Iteration 27, loss = 0.29526830\n",
      "Iteration 28, loss = 0.28926306\n",
      "Iteration 29, loss = 0.29122247\n",
      "Iteration 30, loss = 0.28806297\n",
      "Iteration 31, loss = 0.28293811\n",
      "Iteration 32, loss = 0.28370317\n",
      "Iteration 33, loss = 0.27721914\n",
      "Iteration 34, loss = 0.27545443\n",
      "Iteration 35, loss = 0.27409558\n",
      "Iteration 36, loss = 0.27320338\n",
      "Iteration 37, loss = 0.26920352\n",
      "Iteration 38, loss = 0.27041314\n",
      "Iteration 39, loss = 0.26931023\n",
      "Iteration 40, loss = 0.26925596\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71795166\n",
      "Iteration 2, loss = 0.50852551\n",
      "Iteration 3, loss = 0.46788601\n",
      "Iteration 4, loss = 0.44345417\n",
      "Iteration 5, loss = 0.42558570\n",
      "Iteration 6, loss = 0.40864203\n",
      "Iteration 7, loss = 0.39456748\n",
      "Iteration 8, loss = 0.38638195\n",
      "Iteration 9, loss = 0.37236628\n",
      "Iteration 10, loss = 0.37130697\n",
      "Iteration 11, loss = 0.36388096\n",
      "Iteration 12, loss = 0.35837069\n",
      "Iteration 13, loss = 0.35077889\n",
      "Iteration 14, loss = 0.34607407\n",
      "Iteration 15, loss = 0.33449312\n",
      "Iteration 16, loss = 0.33285482\n",
      "Iteration 17, loss = 0.32925056\n",
      "Iteration 18, loss = 0.32696305\n",
      "Iteration 19, loss = 0.32625567\n",
      "Iteration 20, loss = 0.32027034\n",
      "Iteration 21, loss = 0.31338824\n",
      "Iteration 22, loss = 0.31154081\n",
      "Iteration 23, loss = 0.30883028\n",
      "Iteration 24, loss = 0.30711962\n",
      "Iteration 25, loss = 0.30005964\n",
      "Iteration 26, loss = 0.30245993\n",
      "Iteration 27, loss = 0.29964725\n",
      "Iteration 28, loss = 0.29724196\n",
      "Iteration 29, loss = 0.29347111\n",
      "Iteration 30, loss = 0.29292354\n",
      "Iteration 31, loss = 0.28746195\n",
      "Iteration 32, loss = 0.29058298\n",
      "Iteration 33, loss = 0.28700180\n",
      "Iteration 34, loss = 0.28354695\n",
      "Iteration 35, loss = 0.28385992\n",
      "Iteration 36, loss = 0.27796370\n",
      "Iteration 37, loss = 0.27966548\n",
      "Iteration 38, loss = 0.27711131\n",
      "Iteration 39, loss = 0.27664452\n",
      "Iteration 40, loss = 0.27014432\n",
      "Iteration 41, loss = 0.27264670\n",
      "Iteration 42, loss = 0.27395031\n",
      "Iteration 43, loss = 0.26855001\n",
      "Iteration 44, loss = 0.26890711\n",
      "Iteration 45, loss = 0.26724789\n",
      "Iteration 46, loss = 0.26895277\n",
      "Iteration 47, loss = 0.26703893\n",
      "Iteration 48, loss = 0.26302297\n",
      "Iteration 49, loss = 0.26513803\n",
      "Iteration 50, loss = 0.26058168\n",
      "Iteration 51, loss = 0.26221698\n",
      "Iteration 52, loss = 0.25928662\n",
      "Iteration 53, loss = 0.25936574\n",
      "Iteration 54, loss = 0.26119562\n",
      "Iteration 55, loss = 0.25363706\n",
      "Iteration 56, loss = 0.25407237\n",
      "Iteration 57, loss = 0.25502239\n",
      "Iteration 58, loss = 0.25438518\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71133580\n",
      "Iteration 2, loss = 0.51386075\n",
      "Iteration 3, loss = 0.47191525\n",
      "Iteration 4, loss = 0.44540033\n",
      "Iteration 5, loss = 0.42597954\n",
      "Iteration 6, loss = 0.40805911\n",
      "Iteration 7, loss = 0.40305587\n",
      "Iteration 8, loss = 0.39277580\n",
      "Iteration 9, loss = 0.37374950\n",
      "Iteration 10, loss = 0.37061503\n",
      "Iteration 11, loss = 0.36798393\n",
      "Iteration 12, loss = 0.35626932\n",
      "Iteration 13, loss = 0.35360009\n",
      "Iteration 14, loss = 0.34756181\n",
      "Iteration 15, loss = 0.33970547\n",
      "Iteration 16, loss = 0.33574559\n",
      "Iteration 17, loss = 0.32842923\n",
      "Iteration 18, loss = 0.32882606\n",
      "Iteration 19, loss = 0.32726784\n",
      "Iteration 20, loss = 0.32516824\n",
      "Iteration 21, loss = 0.32133045\n",
      "Iteration 22, loss = 0.31429830\n",
      "Iteration 23, loss = 0.31165680\n",
      "Iteration 24, loss = 0.31296390\n",
      "Iteration 25, loss = 0.30845773\n",
      "Iteration 26, loss = 0.30126200\n",
      "Iteration 27, loss = 0.30079940\n",
      "Iteration 28, loss = 0.29761485\n",
      "Iteration 29, loss = 0.29592994\n",
      "Iteration 30, loss = 0.29336782\n",
      "Iteration 31, loss = 0.29067904\n",
      "Iteration 32, loss = 0.28870319\n",
      "Iteration 33, loss = 0.28851376\n",
      "Iteration 34, loss = 0.28541218\n",
      "Iteration 35, loss = 0.27931098\n",
      "Iteration 36, loss = 0.28144362\n",
      "Iteration 37, loss = 0.27944268\n",
      "Iteration 38, loss = 0.27744807\n",
      "Iteration 39, loss = 0.27524095\n",
      "Iteration 40, loss = 0.27160431\n",
      "Iteration 41, loss = 0.27513635\n",
      "Iteration 42, loss = 0.27458175\n",
      "Iteration 43, loss = 0.27215288\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71312212\n",
      "Iteration 2, loss = 0.51449591\n",
      "Iteration 3, loss = 0.46914827\n",
      "Iteration 4, loss = 0.44839907\n",
      "Iteration 5, loss = 0.42808757\n",
      "Iteration 6, loss = 0.41281967\n",
      "Iteration 7, loss = 0.39884898\n",
      "Iteration 8, loss = 0.38812802\n",
      "Iteration 9, loss = 0.37701192\n",
      "Iteration 10, loss = 0.37296790\n",
      "Iteration 11, loss = 0.36151452\n",
      "Iteration 12, loss = 0.36010318\n",
      "Iteration 13, loss = 0.35377838\n",
      "Iteration 14, loss = 0.34616015\n",
      "Iteration 15, loss = 0.34361473\n",
      "Iteration 16, loss = 0.33639756\n",
      "Iteration 17, loss = 0.32873653\n",
      "Iteration 18, loss = 0.32735456\n",
      "Iteration 19, loss = 0.32414577\n",
      "Iteration 20, loss = 0.32154136\n",
      "Iteration 21, loss = 0.32140825\n",
      "Iteration 22, loss = 0.31566845\n",
      "Iteration 23, loss = 0.31154072\n",
      "Iteration 24, loss = 0.30860030\n",
      "Iteration 25, loss = 0.30455536\n",
      "Iteration 26, loss = 0.30607309\n",
      "Iteration 27, loss = 0.29749292\n",
      "Iteration 28, loss = 0.30007634\n",
      "Iteration 29, loss = 0.29584549\n",
      "Iteration 30, loss = 0.29713907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31, loss = 0.29414530\n",
      "Iteration 32, loss = 0.28991522\n",
      "Iteration 33, loss = 0.28538629\n",
      "Iteration 34, loss = 0.28835111\n",
      "Iteration 35, loss = 0.28327459\n",
      "Iteration 36, loss = 0.27897185\n",
      "Iteration 37, loss = 0.28169703\n",
      "Iteration 38, loss = 0.27870197\n",
      "Iteration 39, loss = 0.27890042\n",
      "Iteration 40, loss = 0.27444814\n",
      "Iteration 41, loss = 0.27206354\n",
      "Iteration 42, loss = 0.27077309\n",
      "Iteration 43, loss = 0.26882412\n",
      "Iteration 44, loss = 0.26925432\n",
      "Iteration 45, loss = 0.27029931\n",
      "Iteration 46, loss = 0.27044082\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72385901\n",
      "Iteration 2, loss = 0.53101361\n",
      "Iteration 3, loss = 0.48532484\n",
      "Iteration 4, loss = 0.45596726\n",
      "Iteration 5, loss = 0.43451661\n",
      "Iteration 6, loss = 0.41304647\n",
      "Iteration 7, loss = 0.39900034\n",
      "Iteration 8, loss = 0.38837617\n",
      "Iteration 9, loss = 0.38250731\n",
      "Iteration 10, loss = 0.37207399\n",
      "Iteration 11, loss = 0.36528659\n",
      "Iteration 12, loss = 0.35423180\n",
      "Iteration 13, loss = 0.34629056\n",
      "Iteration 14, loss = 0.34298910\n",
      "Iteration 15, loss = 0.33958343\n",
      "Iteration 16, loss = 0.33738088\n",
      "Iteration 17, loss = 0.32992587\n",
      "Iteration 18, loss = 0.32434119\n",
      "Iteration 19, loss = 0.31732235\n",
      "Iteration 20, loss = 0.31896453\n",
      "Iteration 21, loss = 0.31731750\n",
      "Iteration 22, loss = 0.31206053\n",
      "Iteration 23, loss = 0.30946612\n",
      "Iteration 24, loss = 0.30266907\n",
      "Iteration 25, loss = 0.30355288\n",
      "Iteration 26, loss = 0.29713384\n",
      "Iteration 27, loss = 0.29757710\n",
      "Iteration 28, loss = 0.29700681\n",
      "Iteration 29, loss = 0.29149616\n",
      "Iteration 30, loss = 0.29166680\n",
      "Iteration 31, loss = 0.28595560\n",
      "Iteration 32, loss = 0.28358129\n",
      "Iteration 33, loss = 0.28160230\n",
      "Iteration 34, loss = 0.28878587\n",
      "Iteration 35, loss = 0.28116639\n",
      "Iteration 36, loss = 0.27709185\n",
      "Iteration 37, loss = 0.27656333\n",
      "Iteration 38, loss = 0.27410251\n",
      "Iteration 39, loss = 0.27330499\n",
      "Iteration 40, loss = 0.26775462\n",
      "Iteration 41, loss = 0.26729072\n",
      "Iteration 42, loss = 0.27209980\n",
      "Iteration 43, loss = 0.26786823\n",
      "Iteration 44, loss = 0.26429711\n",
      "Iteration 45, loss = 0.26360947\n",
      "Iteration 46, loss = 0.26280235\n",
      "Iteration 47, loss = 0.25967512\n",
      "Iteration 48, loss = 0.26469707\n",
      "Iteration 49, loss = 0.25948975\n",
      "Iteration 50, loss = 0.25495822\n",
      "Iteration 51, loss = 0.25823467\n",
      "Iteration 52, loss = 0.25466890\n",
      "Iteration 53, loss = 0.25147800\n",
      "Iteration 54, loss = 0.25503286\n",
      "Iteration 55, loss = 0.25209639\n",
      "Iteration 56, loss = 0.24864371\n",
      "Iteration 57, loss = 0.24945720\n",
      "Iteration 58, loss = 0.24718770\n",
      "Iteration 59, loss = 0.24642984\n",
      "Iteration 60, loss = 0.25210630\n",
      "Iteration 61, loss = 0.24882460\n",
      "Iteration 62, loss = 0.25449399\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72740720\n",
      "Iteration 2, loss = 0.54182091\n",
      "Iteration 3, loss = 0.48720761\n",
      "Iteration 4, loss = 0.45804576\n",
      "Iteration 5, loss = 0.43873508\n",
      "Iteration 6, loss = 0.42057590\n",
      "Iteration 7, loss = 0.40189533\n",
      "Iteration 8, loss = 0.39651573\n",
      "Iteration 9, loss = 0.38439595\n",
      "Iteration 10, loss = 0.37654020\n",
      "Iteration 11, loss = 0.36758977\n",
      "Iteration 12, loss = 0.35797226\n",
      "Iteration 13, loss = 0.35745295\n",
      "Iteration 14, loss = 0.34801643\n",
      "Iteration 15, loss = 0.34278258\n",
      "Iteration 16, loss = 0.33731562\n",
      "Iteration 17, loss = 0.33710011\n",
      "Iteration 18, loss = 0.32890754\n",
      "Iteration 19, loss = 0.32554930\n",
      "Iteration 20, loss = 0.32004180\n",
      "Iteration 21, loss = 0.31970269\n",
      "Iteration 22, loss = 0.31529301\n",
      "Iteration 23, loss = 0.31222090\n",
      "Iteration 24, loss = 0.31070501\n",
      "Iteration 25, loss = 0.30642701\n",
      "Iteration 26, loss = 0.30281795\n",
      "Iteration 27, loss = 0.29815041\n",
      "Iteration 28, loss = 0.29432308\n",
      "Iteration 29, loss = 0.30057794\n",
      "Iteration 30, loss = 0.29543391\n",
      "Iteration 31, loss = 0.29214630\n",
      "Iteration 32, loss = 0.28929292\n",
      "Iteration 33, loss = 0.28732390\n",
      "Iteration 34, loss = 0.28173356\n",
      "Iteration 35, loss = 0.28315927\n",
      "Iteration 36, loss = 0.27971573\n",
      "Iteration 37, loss = 0.27858507\n",
      "Iteration 38, loss = 0.27533254\n",
      "Iteration 39, loss = 0.27717461\n",
      "Iteration 40, loss = 0.27491094\n",
      "Iteration 41, loss = 0.27793747\n",
      "Iteration 42, loss = 0.26903045\n",
      "Iteration 43, loss = 0.26274066\n",
      "Iteration 44, loss = 0.26975388\n",
      "Iteration 45, loss = 0.26537507\n",
      "Iteration 46, loss = 0.26737801\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73117863\n",
      "Iteration 2, loss = 0.54089245\n",
      "Iteration 3, loss = 0.48745524\n",
      "Iteration 4, loss = 0.45588159\n",
      "Iteration 5, loss = 0.43921592\n",
      "Iteration 6, loss = 0.42205951\n",
      "Iteration 7, loss = 0.40256616\n",
      "Iteration 8, loss = 0.39645351\n",
      "Iteration 9, loss = 0.38017292\n",
      "Iteration 10, loss = 0.37483271\n",
      "Iteration 11, loss = 0.36740047\n",
      "Iteration 12, loss = 0.35559509\n",
      "Iteration 13, loss = 0.35444171\n",
      "Iteration 14, loss = 0.35057673\n",
      "Iteration 15, loss = 0.34176365\n",
      "Iteration 16, loss = 0.33716965\n",
      "Iteration 17, loss = 0.33323214\n",
      "Iteration 18, loss = 0.32795514\n",
      "Iteration 19, loss = 0.32776564\n",
      "Iteration 20, loss = 0.32079856\n",
      "Iteration 21, loss = 0.31692033\n",
      "Iteration 22, loss = 0.31462428\n",
      "Iteration 23, loss = 0.30899251\n",
      "Iteration 24, loss = 0.30725642\n",
      "Iteration 25, loss = 0.30409037\n",
      "Iteration 26, loss = 0.29737229\n",
      "Iteration 27, loss = 0.30003505\n",
      "Iteration 28, loss = 0.29257827\n",
      "Iteration 29, loss = 0.29504630\n",
      "Iteration 30, loss = 0.28997705\n",
      "Iteration 31, loss = 0.29220574\n",
      "Iteration 32, loss = 0.28655684\n",
      "Iteration 33, loss = 0.28871116\n",
      "Iteration 34, loss = 0.28306755\n",
      "Iteration 35, loss = 0.27873300\n",
      "Iteration 36, loss = 0.27697783\n",
      "Iteration 37, loss = 0.27603733\n",
      "Iteration 38, loss = 0.27827935\n",
      "Iteration 39, loss = 0.27198339\n",
      "Iteration 40, loss = 0.26807871\n",
      "Iteration 41, loss = 0.26887387\n",
      "Iteration 42, loss = 0.26495912\n",
      "Iteration 43, loss = 0.26495973\n",
      "Iteration 44, loss = 0.26254427\n",
      "Iteration 45, loss = 0.26164056\n",
      "Iteration 46, loss = 0.27079682\n",
      "Iteration 47, loss = 0.25893247\n",
      "Iteration 48, loss = 0.25905208\n",
      "Iteration 49, loss = 0.25775831\n",
      "Iteration 50, loss = 0.26018352\n",
      "Iteration 51, loss = 0.25706102\n",
      "Iteration 52, loss = 0.25616962\n",
      "Iteration 53, loss = 0.25474342\n",
      "Iteration 54, loss = 0.25553289\n",
      "Iteration 55, loss = 0.25210761\n",
      "Iteration 56, loss = 0.25219235\n",
      "Iteration 57, loss = 0.24948111\n",
      "Iteration 58, loss = 0.24706657\n",
      "Iteration 59, loss = 0.24899832\n",
      "Iteration 60, loss = 0.24403313\n",
      "Iteration 61, loss = 0.24916567\n",
      "Iteration 62, loss = 0.24276936\n",
      "Iteration 63, loss = 0.24374079\n",
      "Iteration 64, loss = 0.24120909\n",
      "Iteration 65, loss = 0.24077907\n",
      "Iteration 66, loss = 0.23737424\n",
      "Iteration 67, loss = 0.24253875\n",
      "Iteration 68, loss = 0.24163784\n",
      "Iteration 69, loss = 0.24051653\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86360588\n",
      "Iteration 2, loss = 0.63776489\n",
      "Iteration 3, loss = 0.57477445\n",
      "Iteration 4, loss = 0.53695880\n",
      "Iteration 5, loss = 0.50803581\n",
      "Iteration 6, loss = 0.48628405\n",
      "Iteration 7, loss = 0.47355248\n",
      "Iteration 8, loss = 0.46412619\n",
      "Iteration 9, loss = 0.45420178\n",
      "Iteration 10, loss = 0.44893930\n",
      "Iteration 11, loss = 0.44712004\n",
      "Iteration 12, loss = 0.44465711\n",
      "Iteration 13, loss = 0.44064552\n",
      "Iteration 14, loss = 0.43696495\n",
      "Iteration 15, loss = 0.42874461\n",
      "Iteration 16, loss = 0.42906109\n",
      "Iteration 17, loss = 0.42152104\n",
      "Iteration 18, loss = 0.42446775\n",
      "Iteration 19, loss = 0.42507608\n",
      "Iteration 20, loss = 0.42304103\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86320521\n",
      "Iteration 2, loss = 0.63324595\n",
      "Iteration 3, loss = 0.58208440\n",
      "Iteration 4, loss = 0.53539954\n",
      "Iteration 5, loss = 0.50370241\n",
      "Iteration 6, loss = 0.49730733\n",
      "Iteration 7, loss = 0.47994680\n",
      "Iteration 8, loss = 0.47716103\n",
      "Iteration 9, loss = 0.46055703\n",
      "Iteration 10, loss = 0.45977946\n",
      "Iteration 11, loss = 0.44874859\n",
      "Iteration 12, loss = 0.44484041\n",
      "Iteration 13, loss = 0.44393741\n",
      "Iteration 14, loss = 0.43728262\n",
      "Iteration 15, loss = 0.43197338\n",
      "Iteration 16, loss = 0.43417357\n",
      "Iteration 17, loss = 0.42854062\n",
      "Iteration 18, loss = 0.42825181\n",
      "Iteration 19, loss = 0.42449682\n",
      "Iteration 20, loss = 0.42362682\n",
      "Iteration 21, loss = 0.42558862\n",
      "Iteration 22, loss = 0.42143309\n",
      "Iteration 23, loss = 0.42066011\n",
      "Iteration 24, loss = 0.41625635\n",
      "Iteration 25, loss = 0.41335558\n",
      "Iteration 26, loss = 0.42103513\n",
      "Iteration 27, loss = 0.41569615\n",
      "Iteration 28, loss = 0.41332198\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.87260316\n",
      "Iteration 2, loss = 0.64449189\n",
      "Iteration 3, loss = 0.57354709\n",
      "Iteration 4, loss = 0.53286766\n",
      "Iteration 5, loss = 0.50451495\n",
      "Iteration 6, loss = 0.49159954\n",
      "Iteration 7, loss = 0.47740688\n",
      "Iteration 8, loss = 0.46799120\n",
      "Iteration 9, loss = 0.45992661\n",
      "Iteration 10, loss = 0.44982637\n",
      "Iteration 11, loss = 0.44417326\n",
      "Iteration 12, loss = 0.44024248\n",
      "Iteration 13, loss = 0.44210091\n",
      "Iteration 14, loss = 0.43532003\n",
      "Iteration 15, loss = 0.42999860\n",
      "Iteration 16, loss = 0.43080768\n",
      "Iteration 17, loss = 0.42600872\n",
      "Iteration 18, loss = 0.42195258\n",
      "Iteration 19, loss = 0.42330761\n",
      "Iteration 20, loss = 0.42080751\n",
      "Iteration 21, loss = 0.42431403\n",
      "Iteration 22, loss = 0.41862056\n",
      "Iteration 23, loss = 0.42120821\n",
      "Iteration 24, loss = 0.41620473\n",
      "Iteration 25, loss = 0.41659802\n",
      "Iteration 26, loss = 0.41674640\n",
      "Iteration 27, loss = 0.41230263\n",
      "Iteration 28, loss = 0.41577240\n",
      "Iteration 29, loss = 0.41004644\n",
      "Iteration 30, loss = 0.41342097\n",
      "Iteration 31, loss = 0.41438441\n",
      "Iteration 32, loss = 0.40943214\n",
      "Iteration 33, loss = 0.40825213\n",
      "Iteration 34, loss = 0.40787318\n",
      "Iteration 35, loss = 0.40849051\n",
      "Iteration 36, loss = 0.40984280\n",
      "Iteration 37, loss = 0.40660073\n",
      "Iteration 38, loss = 0.40819065\n",
      "Iteration 39, loss = 0.40925545\n",
      "Iteration 40, loss = 0.40396963\n",
      "Iteration 41, loss = 0.40460819\n",
      "Iteration 42, loss = 0.40631838\n",
      "Iteration 43, loss = 0.40426837\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80763214\n",
      "Iteration 2, loss = 0.60174578\n",
      "Iteration 3, loss = 0.54662872\n",
      "Iteration 4, loss = 0.51703717\n",
      "Iteration 5, loss = 0.49445652\n",
      "Iteration 6, loss = 0.48087575\n",
      "Iteration 7, loss = 0.47170107\n",
      "Iteration 8, loss = 0.45943798\n",
      "Iteration 9, loss = 0.45334606\n",
      "Iteration 10, loss = 0.44248422\n",
      "Iteration 11, loss = 0.44187412\n",
      "Iteration 12, loss = 0.44016874\n",
      "Iteration 13, loss = 0.43220469\n",
      "Iteration 14, loss = 0.43382963\n",
      "Iteration 15, loss = 0.42879935\n",
      "Iteration 16, loss = 0.42716408\n",
      "Iteration 17, loss = 0.42576093\n",
      "Iteration 18, loss = 0.42379376\n",
      "Iteration 19, loss = 0.42278805\n",
      "Iteration 20, loss = 0.41570633\n",
      "Iteration 21, loss = 0.41808261\n",
      "Iteration 22, loss = 0.41463006\n",
      "Iteration 23, loss = 0.41363498\n",
      "Iteration 24, loss = 0.41805793\n",
      "Iteration 25, loss = 0.41248366\n",
      "Iteration 26, loss = 0.41285103\n",
      "Iteration 27, loss = 0.41151870\n",
      "Iteration 28, loss = 0.41328066\n",
      "Iteration 29, loss = 0.40946173\n",
      "Iteration 30, loss = 0.41699253\n",
      "Iteration 31, loss = 0.40689184\n",
      "Iteration 32, loss = 0.40660401\n",
      "Iteration 33, loss = 0.40529285\n",
      "Iteration 34, loss = 0.40644247\n",
      "Iteration 35, loss = 0.40509187\n",
      "Iteration 36, loss = 0.40542662\n",
      "Iteration 37, loss = 0.40534695\n",
      "Iteration 38, loss = 0.40096324\n",
      "Iteration 39, loss = 0.40301305\n",
      "Iteration 40, loss = 0.40023456\n",
      "Iteration 41, loss = 0.40175326\n",
      "Iteration 42, loss = 0.40309475\n",
      "Iteration 43, loss = 0.40035202\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.81473130\n",
      "Iteration 2, loss = 0.60204299\n",
      "Iteration 3, loss = 0.55254786\n",
      "Iteration 4, loss = 0.52140613\n",
      "Iteration 5, loss = 0.50502361\n",
      "Iteration 6, loss = 0.48980312\n",
      "Iteration 7, loss = 0.47365269\n",
      "Iteration 8, loss = 0.46587839\n",
      "Iteration 9, loss = 0.46049622\n",
      "Iteration 10, loss = 0.45269768\n",
      "Iteration 11, loss = 0.44714190\n",
      "Iteration 12, loss = 0.44396186\n",
      "Iteration 13, loss = 0.43715882\n",
      "Iteration 14, loss = 0.43752785\n",
      "Iteration 15, loss = 0.43464901\n",
      "Iteration 16, loss = 0.42698768\n",
      "Iteration 17, loss = 0.42676607\n",
      "Iteration 18, loss = 0.42396202\n",
      "Iteration 19, loss = 0.42333776\n",
      "Iteration 20, loss = 0.42122131\n",
      "Iteration 21, loss = 0.41718874\n",
      "Iteration 22, loss = 0.42396540\n",
      "Iteration 23, loss = 0.41766162\n",
      "Iteration 24, loss = 0.41573641\n",
      "Iteration 25, loss = 0.41452053\n",
      "Iteration 26, loss = 0.41444828\n",
      "Iteration 27, loss = 0.41271897\n",
      "Iteration 28, loss = 0.41330038\n",
      "Iteration 29, loss = 0.41103528\n",
      "Iteration 30, loss = 0.41190921\n",
      "Iteration 31, loss = 0.41099662\n",
      "Iteration 32, loss = 0.40657853\n",
      "Iteration 33, loss = 0.40859620\n",
      "Iteration 34, loss = 0.40643791\n",
      "Iteration 35, loss = 0.40410001\n",
      "Iteration 36, loss = 0.40685948\n",
      "Iteration 37, loss = 0.40526035\n",
      "Iteration 38, loss = 0.40458447\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.81342991\n",
      "Iteration 2, loss = 0.59801215\n",
      "Iteration 3, loss = 0.55038554\n",
      "Iteration 4, loss = 0.51887400\n",
      "Iteration 5, loss = 0.49614837\n",
      "Iteration 6, loss = 0.48938778\n",
      "Iteration 7, loss = 0.47167194\n",
      "Iteration 8, loss = 0.46123255\n",
      "Iteration 9, loss = 0.45497024\n",
      "Iteration 10, loss = 0.45043299\n",
      "Iteration 11, loss = 0.44762230\n",
      "Iteration 12, loss = 0.44038900\n",
      "Iteration 13, loss = 0.43402484\n",
      "Iteration 14, loss = 0.43621208\n",
      "Iteration 15, loss = 0.43181110\n",
      "Iteration 16, loss = 0.42789259\n",
      "Iteration 17, loss = 0.42458699\n",
      "Iteration 18, loss = 0.41924196\n",
      "Iteration 19, loss = 0.42413403\n",
      "Iteration 20, loss = 0.42007500\n",
      "Iteration 21, loss = 0.41801851\n",
      "Iteration 22, loss = 0.41626549\n",
      "Iteration 23, loss = 0.41920995\n",
      "Iteration 24, loss = 0.41839392\n",
      "Iteration 25, loss = 0.41683407\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.97698065\n",
      "Iteration 2, loss = 0.70020829\n",
      "Iteration 3, loss = 0.61088944\n",
      "Iteration 4, loss = 0.55999312\n",
      "Iteration 5, loss = 0.52574112\n",
      "Iteration 6, loss = 0.50454079\n",
      "Iteration 7, loss = 0.48181458\n",
      "Iteration 8, loss = 0.46525001\n",
      "Iteration 9, loss = 0.45461127\n",
      "Iteration 10, loss = 0.44320859\n",
      "Iteration 11, loss = 0.43941160\n",
      "Iteration 12, loss = 0.43135913\n",
      "Iteration 13, loss = 0.42613994\n",
      "Iteration 14, loss = 0.42171094\n",
      "Iteration 15, loss = 0.41392703\n",
      "Iteration 16, loss = 0.41083298\n",
      "Iteration 17, loss = 0.41203915\n",
      "Iteration 18, loss = 0.40792545\n",
      "Iteration 19, loss = 0.40628861\n",
      "Iteration 20, loss = 0.40042834\n",
      "Iteration 21, loss = 0.39668015\n",
      "Iteration 22, loss = 0.39236153\n",
      "Iteration 23, loss = 0.39400266\n",
      "Iteration 24, loss = 0.38969905\n",
      "Iteration 25, loss = 0.39282299\n",
      "Iteration 26, loss = 0.38633854\n",
      "Iteration 27, loss = 0.38484164\n",
      "Iteration 28, loss = 0.38750555\n",
      "Iteration 29, loss = 0.38292866\n",
      "Iteration 30, loss = 0.38247615\n",
      "Iteration 31, loss = 0.38460389\n",
      "Iteration 32, loss = 0.37915019\n",
      "Iteration 33, loss = 0.37989138\n",
      "Iteration 34, loss = 0.37505970\n",
      "Iteration 35, loss = 0.37887823\n",
      "Iteration 36, loss = 0.37473140\n",
      "Iteration 37, loss = 0.37438568\n",
      "Iteration 38, loss = 0.37407139\n",
      "Iteration 39, loss = 0.37553505\n",
      "Iteration 40, loss = 0.37610652\n",
      "Iteration 41, loss = 0.36985747\n",
      "Iteration 42, loss = 0.36999853\n",
      "Iteration 43, loss = 0.36899043\n",
      "Iteration 44, loss = 0.36664466\n",
      "Iteration 45, loss = 0.37177438\n",
      "Iteration 46, loss = 0.36632269\n",
      "Iteration 47, loss = 0.37110888\n",
      "Iteration 48, loss = 0.36686035\n",
      "Iteration 49, loss = 0.36680739\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.98364700\n",
      "Iteration 2, loss = 0.70722111\n",
      "Iteration 3, loss = 0.61822339\n",
      "Iteration 4, loss = 0.56542810\n",
      "Iteration 5, loss = 0.52680162\n",
      "Iteration 6, loss = 0.50210550\n",
      "Iteration 7, loss = 0.48378329\n",
      "Iteration 8, loss = 0.47196199\n",
      "Iteration 9, loss = 0.46264848\n",
      "Iteration 10, loss = 0.44859032\n",
      "Iteration 11, loss = 0.43628431\n",
      "Iteration 12, loss = 0.42956580\n",
      "Iteration 13, loss = 0.42398441\n",
      "Iteration 14, loss = 0.42298465\n",
      "Iteration 15, loss = 0.41664558\n",
      "Iteration 16, loss = 0.41766671\n",
      "Iteration 17, loss = 0.40609526\n",
      "Iteration 18, loss = 0.40599018\n",
      "Iteration 19, loss = 0.40851631\n",
      "Iteration 20, loss = 0.39953621\n",
      "Iteration 21, loss = 0.40020025\n",
      "Iteration 22, loss = 0.39503325\n",
      "Iteration 23, loss = 0.39205570\n",
      "Iteration 24, loss = 0.39363288\n",
      "Iteration 25, loss = 0.39271811\n",
      "Iteration 26, loss = 0.38880976\n",
      "Iteration 27, loss = 0.38873166\n",
      "Iteration 28, loss = 0.38729284\n",
      "Iteration 29, loss = 0.38625343\n",
      "Iteration 30, loss = 0.38424592\n",
      "Iteration 31, loss = 0.37747311\n",
      "Iteration 32, loss = 0.38352374\n",
      "Iteration 33, loss = 0.38129926\n",
      "Iteration 34, loss = 0.37438960\n",
      "Iteration 35, loss = 0.37496005\n",
      "Iteration 36, loss = 0.37396180\n",
      "Iteration 37, loss = 0.37205846\n",
      "Iteration 38, loss = 0.37664960\n",
      "Iteration 39, loss = 0.37301135\n",
      "Iteration 40, loss = 0.37142052\n",
      "Iteration 41, loss = 0.37200713\n",
      "Iteration 42, loss = 0.37166375\n",
      "Iteration 43, loss = 0.37023060\n",
      "Iteration 44, loss = 0.36739751\n",
      "Iteration 45, loss = 0.36902733\n",
      "Iteration 46, loss = 0.36895272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47, loss = 0.36795425\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.97442375\n",
      "Iteration 2, loss = 0.70719019\n",
      "Iteration 3, loss = 0.61526184\n",
      "Iteration 4, loss = 0.56479308\n",
      "Iteration 5, loss = 0.53020356\n",
      "Iteration 6, loss = 0.50207871\n",
      "Iteration 7, loss = 0.48080414\n",
      "Iteration 8, loss = 0.46775711\n",
      "Iteration 9, loss = 0.45892540\n",
      "Iteration 10, loss = 0.44302947\n",
      "Iteration 11, loss = 0.43449445\n",
      "Iteration 12, loss = 0.43100562\n",
      "Iteration 13, loss = 0.42557096\n",
      "Iteration 14, loss = 0.42180523\n",
      "Iteration 15, loss = 0.41360027\n",
      "Iteration 16, loss = 0.41118995\n",
      "Iteration 17, loss = 0.40720909\n",
      "Iteration 18, loss = 0.40124730\n",
      "Iteration 19, loss = 0.40096813\n",
      "Iteration 20, loss = 0.39849556\n",
      "Iteration 21, loss = 0.39402921\n",
      "Iteration 22, loss = 0.39394725\n",
      "Iteration 23, loss = 0.39586362\n",
      "Iteration 24, loss = 0.38711617\n",
      "Iteration 25, loss = 0.38944511\n",
      "Iteration 26, loss = 0.38457007\n",
      "Iteration 27, loss = 0.38790988\n",
      "Iteration 28, loss = 0.38052623\n",
      "Iteration 29, loss = 0.38302860\n",
      "Iteration 30, loss = 0.37958265\n",
      "Iteration 31, loss = 0.38056558\n",
      "Iteration 32, loss = 0.38131192\n",
      "Iteration 33, loss = 0.37424434\n",
      "Iteration 34, loss = 0.37430470\n",
      "Iteration 35, loss = 0.37648295\n",
      "Iteration 36, loss = 0.37435801\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.87435616\n",
      "Iteration 2, loss = 0.63454091\n",
      "Iteration 3, loss = 0.57912739\n",
      "Iteration 4, loss = 0.54534062\n",
      "Iteration 5, loss = 0.51933896\n",
      "Iteration 6, loss = 0.49650584\n",
      "Iteration 7, loss = 0.48095044\n",
      "Iteration 8, loss = 0.46917073\n",
      "Iteration 9, loss = 0.45431148\n",
      "Iteration 10, loss = 0.45216064\n",
      "Iteration 11, loss = 0.44294307\n",
      "Iteration 12, loss = 0.43773330\n",
      "Iteration 13, loss = 0.43150113\n",
      "Iteration 14, loss = 0.42710140\n",
      "Iteration 15, loss = 0.41637342\n",
      "Iteration 16, loss = 0.41654751\n",
      "Iteration 17, loss = 0.41216404\n",
      "Iteration 18, loss = 0.40846708\n",
      "Iteration 19, loss = 0.41203186\n",
      "Iteration 20, loss = 0.40572981\n",
      "Iteration 21, loss = 0.39720280\n",
      "Iteration 22, loss = 0.39680739\n",
      "Iteration 23, loss = 0.39746367\n",
      "Iteration 24, loss = 0.39624430\n",
      "Iteration 25, loss = 0.38924176\n",
      "Iteration 26, loss = 0.39059740\n",
      "Iteration 27, loss = 0.38842196\n",
      "Iteration 28, loss = 0.38724409\n",
      "Iteration 29, loss = 0.38390410\n",
      "Iteration 30, loss = 0.38828733\n",
      "Iteration 31, loss = 0.37862318\n",
      "Iteration 32, loss = 0.38108928\n",
      "Iteration 33, loss = 0.37960542\n",
      "Iteration 34, loss = 0.37949870\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86681285\n",
      "Iteration 2, loss = 0.63765268\n",
      "Iteration 3, loss = 0.58336934\n",
      "Iteration 4, loss = 0.54589118\n",
      "Iteration 5, loss = 0.52056182\n",
      "Iteration 6, loss = 0.49526099\n",
      "Iteration 7, loss = 0.48907654\n",
      "Iteration 8, loss = 0.47399596\n",
      "Iteration 9, loss = 0.45480383\n",
      "Iteration 10, loss = 0.44835432\n",
      "Iteration 11, loss = 0.44483463\n",
      "Iteration 12, loss = 0.43765053\n",
      "Iteration 13, loss = 0.43156445\n",
      "Iteration 14, loss = 0.42657716\n",
      "Iteration 15, loss = 0.42053859\n",
      "Iteration 16, loss = 0.41785381\n",
      "Iteration 17, loss = 0.41042920\n",
      "Iteration 18, loss = 0.41061139\n",
      "Iteration 19, loss = 0.40862159\n",
      "Iteration 20, loss = 0.40423889\n",
      "Iteration 21, loss = 0.40208412\n",
      "Iteration 22, loss = 0.40081714\n",
      "Iteration 23, loss = 0.39856806\n",
      "Iteration 24, loss = 0.39738568\n",
      "Iteration 25, loss = 0.39495988\n",
      "Iteration 26, loss = 0.39113889\n",
      "Iteration 27, loss = 0.38947049\n",
      "Iteration 28, loss = 0.39031341\n",
      "Iteration 29, loss = 0.38744686\n",
      "Iteration 30, loss = 0.38640215\n",
      "Iteration 31, loss = 0.38717484\n",
      "Iteration 32, loss = 0.38366752\n",
      "Iteration 33, loss = 0.38341107\n",
      "Iteration 34, loss = 0.38231016\n",
      "Iteration 35, loss = 0.37532803\n",
      "Iteration 36, loss = 0.38081891\n",
      "Iteration 37, loss = 0.37628979\n",
      "Iteration 38, loss = 0.37442936\n",
      "Iteration 39, loss = 0.37302570\n",
      "Iteration 40, loss = 0.37326887\n",
      "Iteration 41, loss = 0.37539038\n",
      "Iteration 42, loss = 0.37277659\n",
      "Iteration 43, loss = 0.37324146\n",
      "Iteration 44, loss = 0.37404893\n",
      "Iteration 45, loss = 0.37030968\n",
      "Iteration 46, loss = 0.37417294\n",
      "Iteration 47, loss = 0.36977520\n",
      "Iteration 48, loss = 0.36897694\n",
      "Iteration 49, loss = 0.36762752\n",
      "Iteration 50, loss = 0.36763970\n",
      "Iteration 51, loss = 0.36469703\n",
      "Iteration 52, loss = 0.36746214\n",
      "Iteration 53, loss = 0.36726572\n",
      "Iteration 54, loss = 0.36851892\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86919467\n",
      "Iteration 2, loss = 0.63763326\n",
      "Iteration 3, loss = 0.57782345\n",
      "Iteration 4, loss = 0.54922178\n",
      "Iteration 5, loss = 0.52301595\n",
      "Iteration 6, loss = 0.50191505\n",
      "Iteration 7, loss = 0.48511362\n",
      "Iteration 8, loss = 0.47052546\n",
      "Iteration 9, loss = 0.45687645\n",
      "Iteration 10, loss = 0.45107066\n",
      "Iteration 11, loss = 0.43946154\n",
      "Iteration 12, loss = 0.43786966\n",
      "Iteration 13, loss = 0.43227760\n",
      "Iteration 14, loss = 0.42548604\n",
      "Iteration 15, loss = 0.42300588\n",
      "Iteration 16, loss = 0.41654035\n",
      "Iteration 17, loss = 0.41162634\n",
      "Iteration 18, loss = 0.41146674\n",
      "Iteration 19, loss = 0.40592007\n",
      "Iteration 20, loss = 0.40459768\n",
      "Iteration 21, loss = 0.40651259\n",
      "Iteration 22, loss = 0.40277433\n",
      "Iteration 23, loss = 0.39689317\n",
      "Iteration 24, loss = 0.39541465\n",
      "Iteration 25, loss = 0.39321956\n",
      "Iteration 26, loss = 0.39159634\n",
      "Iteration 27, loss = 0.38624725\n",
      "Iteration 28, loss = 0.39016617\n",
      "Iteration 29, loss = 0.39143943\n",
      "Iteration 30, loss = 0.38585157\n",
      "Iteration 31, loss = 0.38924852\n",
      "Iteration 32, loss = 0.38336896\n",
      "Iteration 33, loss = 0.38257121\n",
      "Iteration 34, loss = 0.38290908\n",
      "Iteration 35, loss = 0.38172259\n",
      "Iteration 36, loss = 0.37752401\n",
      "Iteration 37, loss = 0.37823130\n",
      "Iteration 38, loss = 0.37612173\n",
      "Iteration 39, loss = 0.37547408\n",
      "Iteration 40, loss = 0.37656332\n",
      "Iteration 41, loss = 0.37267072\n",
      "Iteration 42, loss = 0.37279187\n",
      "Iteration 43, loss = 0.37135179\n",
      "Iteration 44, loss = 0.37194025\n",
      "Iteration 45, loss = 0.37148116\n",
      "Iteration 46, loss = 0.37132640\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93986621\n",
      "Iteration 2, loss = 0.67520460\n",
      "Iteration 3, loss = 0.60188983\n",
      "Iteration 4, loss = 0.55543381\n",
      "Iteration 5, loss = 0.52104033\n",
      "Iteration 6, loss = 0.49883236\n",
      "Iteration 7, loss = 0.47899884\n",
      "Iteration 8, loss = 0.46665922\n",
      "Iteration 9, loss = 0.46040944\n",
      "Iteration 10, loss = 0.44960454\n",
      "Iteration 11, loss = 0.44212809\n",
      "Iteration 12, loss = 0.43290781\n",
      "Iteration 13, loss = 0.42679879\n",
      "Iteration 14, loss = 0.42635504\n",
      "Iteration 15, loss = 0.42077778\n",
      "Iteration 16, loss = 0.41909093\n",
      "Iteration 17, loss = 0.41033380\n",
      "Iteration 18, loss = 0.40747895\n",
      "Iteration 19, loss = 0.40472043\n",
      "Iteration 20, loss = 0.40543632\n",
      "Iteration 21, loss = 0.40285106\n",
      "Iteration 22, loss = 0.39689402\n",
      "Iteration 23, loss = 0.39773691\n",
      "Iteration 24, loss = 0.39250366\n",
      "Iteration 25, loss = 0.39336636\n",
      "Iteration 26, loss = 0.38957349\n",
      "Iteration 27, loss = 0.38747912\n",
      "Iteration 28, loss = 0.38710949\n",
      "Iteration 29, loss = 0.38477747\n",
      "Iteration 30, loss = 0.38520705\n",
      "Iteration 31, loss = 0.38033264\n",
      "Iteration 32, loss = 0.38255168\n",
      "Iteration 33, loss = 0.37756648\n",
      "Iteration 34, loss = 0.37990341\n",
      "Iteration 35, loss = 0.37503260\n",
      "Iteration 36, loss = 0.37567851\n",
      "Iteration 37, loss = 0.37701680\n",
      "Iteration 38, loss = 0.37353471\n",
      "Iteration 39, loss = 0.36890087\n",
      "Iteration 40, loss = 0.37253856\n",
      "Iteration 41, loss = 0.37034893\n",
      "Iteration 42, loss = 0.37337150\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94699285\n",
      "Iteration 2, loss = 0.69024525\n",
      "Iteration 3, loss = 0.60723237\n",
      "Iteration 4, loss = 0.56165257\n",
      "Iteration 5, loss = 0.53181066\n",
      "Iteration 6, loss = 0.50312519\n",
      "Iteration 7, loss = 0.48630627\n",
      "Iteration 8, loss = 0.47595151\n",
      "Iteration 9, loss = 0.46376005\n",
      "Iteration 10, loss = 0.45524477\n",
      "Iteration 11, loss = 0.44483000\n",
      "Iteration 12, loss = 0.43774682\n",
      "Iteration 13, loss = 0.43607000\n",
      "Iteration 14, loss = 0.42849593\n",
      "Iteration 15, loss = 0.42008968\n",
      "Iteration 16, loss = 0.41638863\n",
      "Iteration 17, loss = 0.41521895\n",
      "Iteration 18, loss = 0.41151918\n",
      "Iteration 19, loss = 0.40785593\n",
      "Iteration 20, loss = 0.40464063\n",
      "Iteration 21, loss = 0.39943808\n",
      "Iteration 22, loss = 0.40066657\n",
      "Iteration 23, loss = 0.40033560\n",
      "Iteration 24, loss = 0.39998388\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.95207806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.69014468\n",
      "Iteration 3, loss = 0.61006746\n",
      "Iteration 4, loss = 0.56036037\n",
      "Iteration 5, loss = 0.53305067\n",
      "Iteration 6, loss = 0.50798008\n",
      "Iteration 7, loss = 0.48662691\n",
      "Iteration 8, loss = 0.47285053\n",
      "Iteration 9, loss = 0.46049729\n",
      "Iteration 10, loss = 0.45281331\n",
      "Iteration 11, loss = 0.44433424\n",
      "Iteration 12, loss = 0.43260166\n",
      "Iteration 13, loss = 0.42929479\n",
      "Iteration 14, loss = 0.42825829\n",
      "Iteration 15, loss = 0.41955275\n",
      "Iteration 16, loss = 0.41658888\n",
      "Iteration 17, loss = 0.41392501\n",
      "Iteration 18, loss = 0.40860196\n",
      "Iteration 19, loss = 0.40841033\n",
      "Iteration 20, loss = 0.40313220\n",
      "Iteration 21, loss = 0.40306415\n",
      "Iteration 22, loss = 0.40055108\n",
      "Iteration 23, loss = 0.39746434\n",
      "Iteration 24, loss = 0.39628596\n",
      "Iteration 25, loss = 0.39246084\n",
      "Iteration 26, loss = 0.38798061\n",
      "Iteration 27, loss = 0.39014689\n",
      "Iteration 28, loss = 0.38685077\n",
      "Iteration 29, loss = 0.38808276\n",
      "Iteration 30, loss = 0.38532218\n",
      "Iteration 31, loss = 0.38480618\n",
      "Iteration 32, loss = 0.38273162\n",
      "Iteration 33, loss = 0.38492455\n",
      "Iteration 34, loss = 0.37901372\n",
      "Iteration 35, loss = 0.37684555\n",
      "Iteration 36, loss = 0.37693941\n",
      "Iteration 37, loss = 0.37483371\n",
      "Iteration 38, loss = 0.38342334\n",
      "Iteration 39, loss = 0.37767934\n",
      "Iteration 40, loss = 0.37324923\n",
      "Iteration 41, loss = 0.37729108\n",
      "Iteration 42, loss = 0.37029119\n",
      "Iteration 43, loss = 0.37000824\n",
      "Iteration 44, loss = 0.37069049\n",
      "Iteration 45, loss = 0.36800157\n",
      "Iteration 46, loss = 0.37095110\n",
      "Iteration 47, loss = 0.36797243\n",
      "Iteration 48, loss = 0.36710868\n",
      "Iteration 49, loss = 0.36571256\n",
      "Iteration 50, loss = 0.37049019\n",
      "Iteration 51, loss = 0.36658149\n",
      "Iteration 52, loss = 0.36672714\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.04937524\n",
      "Iteration 2, loss = 0.72909692\n",
      "Iteration 3, loss = 0.63702114\n",
      "Iteration 4, loss = 0.58709805\n",
      "Iteration 5, loss = 0.55568496\n",
      "Iteration 6, loss = 0.53394326\n",
      "Iteration 7, loss = 0.52552519\n",
      "Iteration 8, loss = 0.51941594\n",
      "Iteration 9, loss = 0.51016053\n",
      "Iteration 10, loss = 0.50817190\n",
      "Iteration 11, loss = 0.50975088\n",
      "Iteration 12, loss = 0.50578432\n",
      "Iteration 13, loss = 0.50589883\n",
      "Iteration 14, loss = 0.50215226\n",
      "Iteration 15, loss = 0.49577842\n",
      "Iteration 16, loss = 0.49404600\n",
      "Iteration 17, loss = 0.49137672\n",
      "Iteration 18, loss = 0.49269121\n",
      "Iteration 19, loss = 0.49504726\n",
      "Iteration 20, loss = 0.49213416\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.04840199\n",
      "Iteration 2, loss = 0.72208712\n",
      "Iteration 3, loss = 0.64253407\n",
      "Iteration 4, loss = 0.58427253\n",
      "Iteration 5, loss = 0.55161599\n",
      "Iteration 6, loss = 0.54454699\n",
      "Iteration 7, loss = 0.53059229\n",
      "Iteration 8, loss = 0.53423340\n",
      "Iteration 9, loss = 0.51533709\n",
      "Iteration 10, loss = 0.51972664\n",
      "Iteration 11, loss = 0.50811514\n",
      "Iteration 12, loss = 0.50870994\n",
      "Iteration 13, loss = 0.50806140\n",
      "Iteration 14, loss = 0.50310490\n",
      "Iteration 15, loss = 0.49863767\n",
      "Iteration 16, loss = 0.50142913\n",
      "Iteration 17, loss = 0.49852101\n",
      "Iteration 18, loss = 0.49916469\n",
      "Iteration 19, loss = 0.49447613\n",
      "Iteration 20, loss = 0.49464874\n",
      "Iteration 21, loss = 0.49858195\n",
      "Iteration 22, loss = 0.49349869\n",
      "Iteration 23, loss = 0.49398047\n",
      "Iteration 24, loss = 0.48720161\n",
      "Iteration 25, loss = 0.48552116\n",
      "Iteration 26, loss = 0.49317988\n",
      "Iteration 27, loss = 0.48890738\n",
      "Iteration 28, loss = 0.48681249\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.06049809\n",
      "Iteration 2, loss = 0.73239490\n",
      "Iteration 3, loss = 0.63297998\n",
      "Iteration 4, loss = 0.58189882\n",
      "Iteration 5, loss = 0.55173735\n",
      "Iteration 6, loss = 0.54060873\n",
      "Iteration 7, loss = 0.52831426\n",
      "Iteration 8, loss = 0.52321398\n",
      "Iteration 9, loss = 0.51647955\n",
      "Iteration 10, loss = 0.50953074\n",
      "Iteration 11, loss = 0.50585863\n",
      "Iteration 12, loss = 0.50451518\n",
      "Iteration 13, loss = 0.50696807\n",
      "Iteration 14, loss = 0.50060913\n",
      "Iteration 15, loss = 0.49764712\n",
      "Iteration 16, loss = 0.50028886\n",
      "Iteration 17, loss = 0.49649438\n",
      "Iteration 18, loss = 0.49325863\n",
      "Iteration 19, loss = 0.49425603\n",
      "Iteration 20, loss = 0.49104694\n",
      "Iteration 21, loss = 0.49711327\n",
      "Iteration 22, loss = 0.49070632\n",
      "Iteration 23, loss = 0.49320247\n",
      "Iteration 24, loss = 0.48919523\n",
      "Iteration 25, loss = 0.48772195\n",
      "Iteration 26, loss = 0.48941056\n",
      "Iteration 27, loss = 0.48377301\n",
      "Iteration 28, loss = 0.48803665\n",
      "Iteration 29, loss = 0.48510961\n",
      "Iteration 30, loss = 0.48646389\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93685137\n",
      "Iteration 2, loss = 0.68134690\n",
      "Iteration 3, loss = 0.60995681\n",
      "Iteration 4, loss = 0.57157504\n",
      "Iteration 5, loss = 0.54703006\n",
      "Iteration 6, loss = 0.53273881\n",
      "Iteration 7, loss = 0.52372602\n",
      "Iteration 8, loss = 0.51311688\n",
      "Iteration 9, loss = 0.50905829\n",
      "Iteration 10, loss = 0.49879705\n",
      "Iteration 11, loss = 0.50014652\n",
      "Iteration 12, loss = 0.50123086\n",
      "Iteration 13, loss = 0.49366723\n",
      "Iteration 14, loss = 0.49443782\n",
      "Iteration 15, loss = 0.49156505\n",
      "Iteration 16, loss = 0.49200513\n",
      "Iteration 17, loss = 0.49139792\n",
      "Iteration 18, loss = 0.48953377\n",
      "Iteration 19, loss = 0.48878457\n",
      "Iteration 20, loss = 0.48449499\n",
      "Iteration 21, loss = 0.48625756\n",
      "Iteration 22, loss = 0.48484934\n",
      "Iteration 23, loss = 0.48375824\n",
      "Iteration 24, loss = 0.48960256\n",
      "Iteration 25, loss = 0.48182879\n",
      "Iteration 26, loss = 0.48310352\n",
      "Iteration 27, loss = 0.48195601\n",
      "Iteration 28, loss = 0.48457636\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94617044\n",
      "Iteration 2, loss = 0.68506730\n",
      "Iteration 3, loss = 0.61698416\n",
      "Iteration 4, loss = 0.57744962\n",
      "Iteration 5, loss = 0.55764258\n",
      "Iteration 6, loss = 0.54316982\n",
      "Iteration 7, loss = 0.52760077\n",
      "Iteration 8, loss = 0.51855061\n",
      "Iteration 9, loss = 0.51627126\n",
      "Iteration 10, loss = 0.50951796\n",
      "Iteration 11, loss = 0.50557090\n",
      "Iteration 12, loss = 0.50447349\n",
      "Iteration 13, loss = 0.49860308\n",
      "Iteration 14, loss = 0.49916973\n",
      "Iteration 15, loss = 0.49800130\n",
      "Iteration 16, loss = 0.49246268\n",
      "Iteration 17, loss = 0.49223104\n",
      "Iteration 18, loss = 0.49247442\n",
      "Iteration 19, loss = 0.48931886\n",
      "Iteration 20, loss = 0.48956504\n",
      "Iteration 21, loss = 0.48548708\n",
      "Iteration 22, loss = 0.49458367\n",
      "Iteration 23, loss = 0.48583420\n",
      "Iteration 24, loss = 0.48786359\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94268753\n",
      "Iteration 2, loss = 0.67731437\n",
      "Iteration 3, loss = 0.61209967\n",
      "Iteration 4, loss = 0.57179472\n",
      "Iteration 5, loss = 0.54759034\n",
      "Iteration 6, loss = 0.54049301\n",
      "Iteration 7, loss = 0.52423703\n",
      "Iteration 8, loss = 0.51584905\n",
      "Iteration 9, loss = 0.51051215\n",
      "Iteration 10, loss = 0.50680345\n",
      "Iteration 11, loss = 0.50850344\n",
      "Iteration 12, loss = 0.50089696\n",
      "Iteration 13, loss = 0.49618575\n",
      "Iteration 14, loss = 0.50128657\n",
      "Iteration 15, loss = 0.49633324\n",
      "Iteration 16, loss = 0.49268801\n",
      "Iteration 17, loss = 0.49125509\n",
      "Iteration 18, loss = 0.48738919\n",
      "Iteration 19, loss = 0.49407191\n",
      "Iteration 20, loss = 0.48947938\n",
      "Iteration 21, loss = 0.48726322\n",
      "Iteration 22, loss = 0.48602762\n",
      "Iteration 23, loss = 0.49018289\n",
      "Iteration 24, loss = 0.49031245\n",
      "Iteration 25, loss = 0.48741706\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.25679929\n",
      "Iteration 2, loss = 0.82259994\n",
      "Iteration 3, loss = 0.69046101\n",
      "Iteration 4, loss = 0.62085537\n",
      "Iteration 5, loss = 0.58116854\n",
      "Iteration 6, loss = 0.55690002\n",
      "Iteration 7, loss = 0.53925927\n",
      "Iteration 8, loss = 0.52111144\n",
      "Iteration 9, loss = 0.51094989\n",
      "Iteration 10, loss = 0.50440635\n",
      "Iteration 11, loss = 0.50156654\n",
      "Iteration 12, loss = 0.50023686\n",
      "Iteration 13, loss = 0.49480014\n",
      "Iteration 14, loss = 0.49017807\n",
      "Iteration 15, loss = 0.48585860\n",
      "Iteration 16, loss = 0.48337110\n",
      "Iteration 17, loss = 0.48532011\n",
      "Iteration 18, loss = 0.48102956\n",
      "Iteration 19, loss = 0.48020451\n",
      "Iteration 20, loss = 0.47319567\n",
      "Iteration 21, loss = 0.47260077\n",
      "Iteration 22, loss = 0.46916743\n",
      "Iteration 23, loss = 0.46760539\n",
      "Iteration 24, loss = 0.46704375\n",
      "Iteration 25, loss = 0.47028217\n",
      "Iteration 26, loss = 0.46268905\n",
      "Iteration 27, loss = 0.46425885\n",
      "Iteration 28, loss = 0.46541808\n",
      "Iteration 29, loss = 0.46415455\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.26216444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.83162675\n",
      "Iteration 3, loss = 0.69962280\n",
      "Iteration 4, loss = 0.62856022\n",
      "Iteration 5, loss = 0.58339898\n",
      "Iteration 6, loss = 0.55551078\n",
      "Iteration 7, loss = 0.54034154\n",
      "Iteration 8, loss = 0.52918466\n",
      "Iteration 9, loss = 0.51999958\n",
      "Iteration 10, loss = 0.51108063\n",
      "Iteration 11, loss = 0.49990323\n",
      "Iteration 12, loss = 0.49691676\n",
      "Iteration 13, loss = 0.49206907\n",
      "Iteration 14, loss = 0.49442121\n",
      "Iteration 15, loss = 0.48688416\n",
      "Iteration 16, loss = 0.48660715\n",
      "Iteration 17, loss = 0.48017100\n",
      "Iteration 18, loss = 0.48117614\n",
      "Iteration 19, loss = 0.48237892\n",
      "Iteration 20, loss = 0.47566393\n",
      "Iteration 21, loss = 0.47609082\n",
      "Iteration 22, loss = 0.47235959\n",
      "Iteration 23, loss = 0.47092024\n",
      "Iteration 24, loss = 0.47121809\n",
      "Iteration 25, loss = 0.47220384\n",
      "Iteration 26, loss = 0.46848822\n",
      "Iteration 27, loss = 0.46800960\n",
      "Iteration 28, loss = 0.46991791\n",
      "Iteration 29, loss = 0.46735683\n",
      "Iteration 30, loss = 0.46550236\n",
      "Iteration 31, loss = 0.46164406\n",
      "Iteration 32, loss = 0.46491473\n",
      "Iteration 33, loss = 0.46347517\n",
      "Iteration 34, loss = 0.45825430\n",
      "Iteration 35, loss = 0.46094837\n",
      "Iteration 36, loss = 0.45918814\n",
      "Iteration 37, loss = 0.45589963\n",
      "Iteration 38, loss = 0.45935699\n",
      "Iteration 39, loss = 0.45972033\n",
      "Iteration 40, loss = 0.45628288\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.25552173\n",
      "Iteration 2, loss = 0.83272365\n",
      "Iteration 3, loss = 0.69627853\n",
      "Iteration 4, loss = 0.62496447\n",
      "Iteration 5, loss = 0.58758807\n",
      "Iteration 6, loss = 0.55832265\n",
      "Iteration 7, loss = 0.53699915\n",
      "Iteration 8, loss = 0.52534509\n",
      "Iteration 9, loss = 0.51978916\n",
      "Iteration 10, loss = 0.50703871\n",
      "Iteration 11, loss = 0.50008641\n",
      "Iteration 12, loss = 0.49858967\n",
      "Iteration 13, loss = 0.49550843\n",
      "Iteration 14, loss = 0.49197430\n",
      "Iteration 15, loss = 0.48338534\n",
      "Iteration 16, loss = 0.48605728\n",
      "Iteration 17, loss = 0.48022947\n",
      "Iteration 18, loss = 0.47618014\n",
      "Iteration 19, loss = 0.47750997\n",
      "Iteration 20, loss = 0.47399340\n",
      "Iteration 21, loss = 0.47426764\n",
      "Iteration 22, loss = 0.47149741\n",
      "Iteration 23, loss = 0.47474011\n",
      "Iteration 24, loss = 0.46787833\n",
      "Iteration 25, loss = 0.47080492\n",
      "Iteration 26, loss = 0.46291479\n",
      "Iteration 27, loss = 0.46724208\n",
      "Iteration 28, loss = 0.46259732\n",
      "Iteration 29, loss = 0.46555595\n",
      "Iteration 30, loss = 0.46058543\n",
      "Iteration 31, loss = 0.46055681\n",
      "Iteration 32, loss = 0.46162231\n",
      "Iteration 33, loss = 0.45793222\n",
      "Iteration 34, loss = 0.45911315\n",
      "Iteration 35, loss = 0.46023095\n",
      "Iteration 36, loss = 0.45852342\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.06837212\n",
      "Iteration 2, loss = 0.75645402\n",
      "Iteration 3, loss = 0.67391028\n",
      "Iteration 4, loss = 0.62558097\n",
      "Iteration 5, loss = 0.59212277\n",
      "Iteration 6, loss = 0.56342798\n",
      "Iteration 7, loss = 0.54590643\n",
      "Iteration 8, loss = 0.53647124\n",
      "Iteration 9, loss = 0.51862327\n",
      "Iteration 10, loss = 0.51907240\n",
      "Iteration 11, loss = 0.50881613\n",
      "Iteration 12, loss = 0.50586791\n",
      "Iteration 13, loss = 0.50029981\n",
      "Iteration 14, loss = 0.49590369\n",
      "Iteration 15, loss = 0.48652621\n",
      "Iteration 16, loss = 0.48880884\n",
      "Iteration 17, loss = 0.48493105\n",
      "Iteration 18, loss = 0.48048812\n",
      "Iteration 19, loss = 0.48708172\n",
      "Iteration 20, loss = 0.48244607\n",
      "Iteration 21, loss = 0.47368208\n",
      "Iteration 22, loss = 0.47240672\n",
      "Iteration 23, loss = 0.47186787\n",
      "Iteration 24, loss = 0.47302233\n",
      "Iteration 25, loss = 0.47013641\n",
      "Iteration 26, loss = 0.46755846\n",
      "Iteration 27, loss = 0.46592752\n",
      "Iteration 28, loss = 0.46832481\n",
      "Iteration 29, loss = 0.46380308\n",
      "Iteration 30, loss = 0.46861613\n",
      "Iteration 31, loss = 0.45981031\n",
      "Iteration 32, loss = 0.46813313\n",
      "Iteration 33, loss = 0.46217954\n",
      "Iteration 34, loss = 0.46118667\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.06144535\n",
      "Iteration 2, loss = 0.75695051\n",
      "Iteration 3, loss = 0.67347073\n",
      "Iteration 4, loss = 0.62277718\n",
      "Iteration 5, loss = 0.58963120\n",
      "Iteration 6, loss = 0.56072313\n",
      "Iteration 7, loss = 0.55281856\n",
      "Iteration 8, loss = 0.53819273\n",
      "Iteration 9, loss = 0.51801194\n",
      "Iteration 10, loss = 0.51190886\n",
      "Iteration 11, loss = 0.51259486\n",
      "Iteration 12, loss = 0.50393003\n",
      "Iteration 13, loss = 0.49936103\n",
      "Iteration 14, loss = 0.49695076\n",
      "Iteration 15, loss = 0.49201352\n",
      "Iteration 16, loss = 0.48854524\n",
      "Iteration 17, loss = 0.48196929\n",
      "Iteration 18, loss = 0.48456819\n",
      "Iteration 19, loss = 0.48064571\n",
      "Iteration 20, loss = 0.47817059\n",
      "Iteration 21, loss = 0.47893997\n",
      "Iteration 22, loss = 0.47666332\n",
      "Iteration 23, loss = 0.47515590\n",
      "Iteration 24, loss = 0.47428010\n",
      "Iteration 25, loss = 0.47286299\n",
      "Iteration 26, loss = 0.47124065\n",
      "Iteration 27, loss = 0.46920878\n",
      "Iteration 28, loss = 0.46960905\n",
      "Iteration 29, loss = 0.46946609\n",
      "Iteration 30, loss = 0.46759518\n",
      "Iteration 31, loss = 0.46900687\n",
      "Iteration 32, loss = 0.46455934\n",
      "Iteration 33, loss = 0.46430001\n",
      "Iteration 34, loss = 0.46284732\n",
      "Iteration 35, loss = 0.46021936\n",
      "Iteration 36, loss = 0.46344648\n",
      "Iteration 37, loss = 0.45983579\n",
      "Iteration 38, loss = 0.45877906\n",
      "Iteration 39, loss = 0.45904851\n",
      "Iteration 40, loss = 0.45920549\n",
      "Iteration 41, loss = 0.45843968\n",
      "Iteration 42, loss = 0.45555306\n",
      "Iteration 43, loss = 0.45843362\n",
      "Iteration 44, loss = 0.45745335\n",
      "Iteration 45, loss = 0.45831712\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.06217845\n",
      "Iteration 2, loss = 0.75883866\n",
      "Iteration 3, loss = 0.67126911\n",
      "Iteration 4, loss = 0.62731814\n",
      "Iteration 5, loss = 0.59580748\n",
      "Iteration 6, loss = 0.56778774\n",
      "Iteration 7, loss = 0.54963203\n",
      "Iteration 8, loss = 0.53533847\n",
      "Iteration 9, loss = 0.52106955\n",
      "Iteration 10, loss = 0.51544572\n",
      "Iteration 11, loss = 0.50637409\n",
      "Iteration 12, loss = 0.50302733\n",
      "Iteration 13, loss = 0.49927128\n",
      "Iteration 14, loss = 0.49570542\n",
      "Iteration 15, loss = 0.49131528\n",
      "Iteration 16, loss = 0.48656911\n",
      "Iteration 17, loss = 0.48293311\n",
      "Iteration 18, loss = 0.48208004\n",
      "Iteration 19, loss = 0.47814916\n",
      "Iteration 20, loss = 0.47776948\n",
      "Iteration 21, loss = 0.48207489\n",
      "Iteration 22, loss = 0.47789667\n",
      "Iteration 23, loss = 0.47505550\n",
      "Iteration 24, loss = 0.47201922\n",
      "Iteration 25, loss = 0.47183319\n",
      "Iteration 26, loss = 0.46916683\n",
      "Iteration 27, loss = 0.46602488\n",
      "Iteration 28, loss = 0.46906515\n",
      "Iteration 29, loss = 0.46968184\n",
      "Iteration 30, loss = 0.46723699\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.19475044\n",
      "Iteration 2, loss = 0.79800079\n",
      "Iteration 3, loss = 0.68772657\n",
      "Iteration 4, loss = 0.62377794\n",
      "Iteration 5, loss = 0.58576440\n",
      "Iteration 6, loss = 0.56052511\n",
      "Iteration 7, loss = 0.53925542\n",
      "Iteration 8, loss = 0.52928325\n",
      "Iteration 9, loss = 0.52080570\n",
      "Iteration 10, loss = 0.51294894\n",
      "Iteration 11, loss = 0.50786257\n",
      "Iteration 12, loss = 0.50189199\n",
      "Iteration 13, loss = 0.49701030\n",
      "Iteration 14, loss = 0.49579973\n",
      "Iteration 15, loss = 0.49239387\n",
      "Iteration 16, loss = 0.48877255\n",
      "Iteration 17, loss = 0.48514108\n",
      "Iteration 18, loss = 0.48030459\n",
      "Iteration 19, loss = 0.47874304\n",
      "Iteration 20, loss = 0.48104103\n",
      "Iteration 21, loss = 0.47680288\n",
      "Iteration 22, loss = 0.47368965\n",
      "Iteration 23, loss = 0.47419574\n",
      "Iteration 24, loss = 0.47326933\n",
      "Iteration 25, loss = 0.47201637\n",
      "Iteration 26, loss = 0.47033779\n",
      "Iteration 27, loss = 0.46691243\n",
      "Iteration 28, loss = 0.46975976\n",
      "Iteration 29, loss = 0.46520506\n",
      "Iteration 30, loss = 0.46627127\n",
      "Iteration 31, loss = 0.46452180\n",
      "Iteration 32, loss = 0.46671917\n",
      "Iteration 33, loss = 0.45990107\n",
      "Iteration 34, loss = 0.46403032\n",
      "Iteration 35, loss = 0.46199047\n",
      "Iteration 36, loss = 0.45977749\n",
      "Iteration 37, loss = 0.46145664\n",
      "Iteration 38, loss = 0.45678904\n",
      "Iteration 39, loss = 0.45584126\n",
      "Iteration 40, loss = 0.45782832\n",
      "Iteration 41, loss = 0.45692900\n",
      "Iteration 42, loss = 0.45836080\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.19901730\n",
      "Iteration 2, loss = 0.80830390\n",
      "Iteration 3, loss = 0.68924156\n",
      "Iteration 4, loss = 0.62679707\n",
      "Iteration 5, loss = 0.59183606\n",
      "Iteration 6, loss = 0.56026157\n",
      "Iteration 7, loss = 0.54511446\n",
      "Iteration 8, loss = 0.53690313\n",
      "Iteration 9, loss = 0.52468145\n",
      "Iteration 10, loss = 0.51745158\n",
      "Iteration 11, loss = 0.51080808\n",
      "Iteration 12, loss = 0.50495575\n",
      "Iteration 13, loss = 0.50332074\n",
      "Iteration 14, loss = 0.49908275\n",
      "Iteration 15, loss = 0.49224101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 0.48936323\n",
      "Iteration 17, loss = 0.48870001\n",
      "Iteration 18, loss = 0.48421271\n",
      "Iteration 19, loss = 0.48376409\n",
      "Iteration 20, loss = 0.48281384\n",
      "Iteration 21, loss = 0.47744161\n",
      "Iteration 22, loss = 0.47879900\n",
      "Iteration 23, loss = 0.48124919\n",
      "Iteration 24, loss = 0.47668319\n",
      "Iteration 25, loss = 0.47699344\n",
      "Iteration 26, loss = 0.47195435\n",
      "Iteration 27, loss = 0.47059986\n",
      "Iteration 28, loss = 0.46901402\n",
      "Iteration 29, loss = 0.47451568\n",
      "Iteration 30, loss = 0.46979768\n",
      "Iteration 31, loss = 0.46839661\n",
      "Iteration 32, loss = 0.46691680\n",
      "Iteration 33, loss = 0.46779579\n",
      "Iteration 34, loss = 0.46374708\n",
      "Iteration 35, loss = 0.46697495\n",
      "Iteration 36, loss = 0.46561505\n",
      "Iteration 37, loss = 0.45986686\n",
      "Iteration 38, loss = 0.46305220\n",
      "Iteration 39, loss = 0.46047104\n",
      "Iteration 40, loss = 0.46036081\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.20544510\n",
      "Iteration 2, loss = 0.81108399\n",
      "Iteration 3, loss = 0.69348573\n",
      "Iteration 4, loss = 0.62920908\n",
      "Iteration 5, loss = 0.59552618\n",
      "Iteration 6, loss = 0.56385939\n",
      "Iteration 7, loss = 0.54351930\n",
      "Iteration 8, loss = 0.53221587\n",
      "Iteration 9, loss = 0.52331696\n",
      "Iteration 10, loss = 0.51453672\n",
      "Iteration 11, loss = 0.50778235\n",
      "Iteration 12, loss = 0.49931867\n",
      "Iteration 13, loss = 0.49636960\n",
      "Iteration 14, loss = 0.49778363\n",
      "Iteration 15, loss = 0.48906800\n",
      "Iteration 16, loss = 0.48929873\n",
      "Iteration 17, loss = 0.48577082\n",
      "Iteration 18, loss = 0.48479684\n",
      "Iteration 19, loss = 0.48253843\n",
      "Iteration 20, loss = 0.47724703\n",
      "Iteration 21, loss = 0.47655648\n",
      "Iteration 22, loss = 0.47380654\n",
      "Iteration 23, loss = 0.47379582\n",
      "Iteration 24, loss = 0.47257831\n",
      "Iteration 25, loss = 0.47033286\n",
      "Iteration 26, loss = 0.46603651\n",
      "Iteration 27, loss = 0.46975164\n",
      "Iteration 28, loss = 0.46625087\n",
      "Iteration 29, loss = 0.46804050\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.24344711\n",
      "Iteration 2, loss = 0.78769765\n",
      "Iteration 3, loss = 0.67491202\n",
      "Iteration 4, loss = 0.62187490\n",
      "Iteration 5, loss = 0.59646104\n",
      "Iteration 6, loss = 0.57893871\n",
      "Iteration 7, loss = 0.57691591\n",
      "Iteration 8, loss = 0.57348438\n",
      "Iteration 9, loss = 0.56523799\n",
      "Iteration 10, loss = 0.56650374\n",
      "Iteration 11, loss = 0.56787601\n",
      "Iteration 12, loss = 0.56376167\n",
      "Iteration 13, loss = 0.56643594\n",
      "Iteration 14, loss = 0.56395498\n",
      "Iteration 15, loss = 0.55769334\n",
      "Iteration 16, loss = 0.55458285\n",
      "Iteration 17, loss = 0.55289854\n",
      "Iteration 18, loss = 0.55538101\n",
      "Iteration 19, loss = 0.55731693\n",
      "Iteration 20, loss = 0.55673958\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.23903289\n",
      "Iteration 2, loss = 0.77589543\n",
      "Iteration 3, loss = 0.67714468\n",
      "Iteration 4, loss = 0.61941932\n",
      "Iteration 5, loss = 0.59206551\n",
      "Iteration 6, loss = 0.58999609\n",
      "Iteration 7, loss = 0.57938314\n",
      "Iteration 8, loss = 0.58674420\n",
      "Iteration 9, loss = 0.56985090\n",
      "Iteration 10, loss = 0.57571492\n",
      "Iteration 11, loss = 0.56495582\n",
      "Iteration 12, loss = 0.56637364\n",
      "Iteration 13, loss = 0.56716629\n",
      "Iteration 14, loss = 0.56272785\n",
      "Iteration 15, loss = 0.55958038\n",
      "Iteration 16, loss = 0.56214700\n",
      "Iteration 17, loss = 0.56012524\n",
      "Iteration 18, loss = 0.56063958\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.25850611\n",
      "Iteration 2, loss = 0.79049367\n",
      "Iteration 3, loss = 0.66696987\n",
      "Iteration 4, loss = 0.61785976\n",
      "Iteration 5, loss = 0.59199325\n",
      "Iteration 6, loss = 0.58649660\n",
      "Iteration 7, loss = 0.57692612\n",
      "Iteration 8, loss = 0.57738394\n",
      "Iteration 9, loss = 0.57164146\n",
      "Iteration 10, loss = 0.56556226\n",
      "Iteration 11, loss = 0.56323926\n",
      "Iteration 12, loss = 0.56301682\n",
      "Iteration 13, loss = 0.56592869\n",
      "Iteration 14, loss = 0.56093022\n",
      "Iteration 15, loss = 0.55842661\n",
      "Iteration 16, loss = 0.56008346\n",
      "Iteration 17, loss = 0.55770737\n",
      "Iteration 18, loss = 0.55609772\n",
      "Iteration 19, loss = 0.55627201\n",
      "Iteration 20, loss = 0.55340759\n",
      "Iteration 21, loss = 0.56029232\n",
      "Iteration 22, loss = 0.55370104\n",
      "Iteration 23, loss = 0.55581166\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.07917712\n",
      "Iteration 2, loss = 0.74795468\n",
      "Iteration 3, loss = 0.65921224\n",
      "Iteration 4, loss = 0.61516535\n",
      "Iteration 5, loss = 0.59135532\n",
      "Iteration 6, loss = 0.57909992\n",
      "Iteration 7, loss = 0.57299778\n",
      "Iteration 8, loss = 0.56559057\n",
      "Iteration 9, loss = 0.56276323\n",
      "Iteration 10, loss = 0.55422584\n",
      "Iteration 11, loss = 0.55728897\n",
      "Iteration 12, loss = 0.55933890\n",
      "Iteration 13, loss = 0.55256963\n",
      "Iteration 14, loss = 0.55430462\n",
      "Iteration 15, loss = 0.55238081\n",
      "Iteration 16, loss = 0.55245220\n",
      "Iteration 17, loss = 0.55356226\n",
      "Iteration 18, loss = 0.55024960\n",
      "Iteration 19, loss = 0.55046479\n",
      "Iteration 20, loss = 0.54652242\n",
      "Iteration 21, loss = 0.54862815\n",
      "Iteration 22, loss = 0.54880989\n",
      "Iteration 23, loss = 0.54621397\n",
      "Iteration 24, loss = 0.55190357\n",
      "Iteration 25, loss = 0.54517994\n",
      "Iteration 26, loss = 0.54574857\n",
      "Iteration 27, loss = 0.54552478\n",
      "Iteration 28, loss = 0.54816705\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08902863\n",
      "Iteration 2, loss = 0.75175451\n",
      "Iteration 3, loss = 0.66476577\n",
      "Iteration 4, loss = 0.62076506\n",
      "Iteration 5, loss = 0.60139627\n",
      "Iteration 6, loss = 0.59105026\n",
      "Iteration 7, loss = 0.57693266\n",
      "Iteration 8, loss = 0.56919596\n",
      "Iteration 9, loss = 0.56987277\n",
      "Iteration 10, loss = 0.56551031\n",
      "Iteration 11, loss = 0.56390409\n",
      "Iteration 12, loss = 0.56171945\n",
      "Iteration 13, loss = 0.55870628\n",
      "Iteration 14, loss = 0.55720220\n",
      "Iteration 15, loss = 0.55896433\n",
      "Iteration 16, loss = 0.55442775\n",
      "Iteration 17, loss = 0.55393903\n",
      "Iteration 18, loss = 0.55467727\n",
      "Iteration 19, loss = 0.55148520\n",
      "Iteration 20, loss = 0.55231937\n",
      "Iteration 21, loss = 0.54804018\n",
      "Iteration 22, loss = 0.55726310\n",
      "Iteration 23, loss = 0.54909739\n",
      "Iteration 24, loss = 0.55236842\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08607953\n",
      "Iteration 2, loss = 0.74268148\n",
      "Iteration 3, loss = 0.65726133\n",
      "Iteration 4, loss = 0.61463420\n",
      "Iteration 5, loss = 0.59097978\n",
      "Iteration 6, loss = 0.58563321\n",
      "Iteration 7, loss = 0.57253301\n",
      "Iteration 8, loss = 0.56652885\n",
      "Iteration 9, loss = 0.56315257\n",
      "Iteration 10, loss = 0.56150138\n",
      "Iteration 11, loss = 0.56670642\n",
      "Iteration 12, loss = 0.55699342\n",
      "Iteration 13, loss = 0.55463085\n",
      "Iteration 14, loss = 0.56110210\n",
      "Iteration 15, loss = 0.55456034\n",
      "Iteration 16, loss = 0.55310142\n",
      "Iteration 17, loss = 0.55204755\n",
      "Iteration 18, loss = 0.54921630\n",
      "Iteration 19, loss = 0.55589949\n",
      "Iteration 20, loss = 0.55064121\n",
      "Iteration 21, loss = 0.54886772\n",
      "Iteration 22, loss = 0.54879589\n",
      "Iteration 23, loss = 0.55088081\n",
      "Iteration 24, loss = 0.55184905\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.54052461\n",
      "Iteration 2, loss = 0.89658226\n",
      "Iteration 3, loss = 0.73609991\n",
      "Iteration 4, loss = 0.66517490\n",
      "Iteration 5, loss = 0.62587046\n",
      "Iteration 6, loss = 0.60742340\n",
      "Iteration 7, loss = 0.59331074\n",
      "Iteration 8, loss = 0.58305764\n",
      "Iteration 9, loss = 0.57294573\n",
      "Iteration 10, loss = 0.56962899\n",
      "Iteration 11, loss = 0.56718522\n",
      "Iteration 12, loss = 0.57222329\n",
      "Iteration 13, loss = 0.56285732\n",
      "Iteration 14, loss = 0.56003017\n",
      "Iteration 15, loss = 0.55590825\n",
      "Iteration 16, loss = 0.55360011\n",
      "Iteration 17, loss = 0.55743313\n",
      "Iteration 18, loss = 0.55221444\n",
      "Iteration 19, loss = 0.55102950\n",
      "Iteration 20, loss = 0.54551677\n",
      "Iteration 21, loss = 0.54802431\n",
      "Iteration 22, loss = 0.54303449\n",
      "Iteration 23, loss = 0.54426922\n",
      "Iteration 24, loss = 0.53959631\n",
      "Iteration 25, loss = 0.54183690\n",
      "Iteration 26, loss = 0.53772421\n",
      "Iteration 27, loss = 0.53807879\n",
      "Iteration 28, loss = 0.53930977\n",
      "Iteration 29, loss = 0.53861269\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.54277721\n",
      "Iteration 2, loss = 0.90813729\n",
      "Iteration 3, loss = 0.74508127\n",
      "Iteration 4, loss = 0.67179643\n",
      "Iteration 5, loss = 0.62857413\n",
      "Iteration 6, loss = 0.60962348\n",
      "Iteration 7, loss = 0.59695824\n",
      "Iteration 8, loss = 0.59026754\n",
      "Iteration 9, loss = 0.58264476\n",
      "Iteration 10, loss = 0.57680019\n",
      "Iteration 11, loss = 0.56824264\n",
      "Iteration 12, loss = 0.56809747\n",
      "Iteration 13, loss = 0.56421700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = 0.56519877\n",
      "Iteration 15, loss = 0.55900062\n",
      "Iteration 16, loss = 0.55869247\n",
      "Iteration 17, loss = 0.55455351\n",
      "Iteration 18, loss = 0.55228550\n",
      "Iteration 19, loss = 0.55522834\n",
      "Iteration 20, loss = 0.54939511\n",
      "Iteration 21, loss = 0.55042704\n",
      "Iteration 22, loss = 0.54695808\n",
      "Iteration 23, loss = 0.54526526\n",
      "Iteration 24, loss = 0.54567984\n",
      "Iteration 25, loss = 0.54775523\n",
      "Iteration 26, loss = 0.54410765\n",
      "Iteration 27, loss = 0.54199213\n",
      "Iteration 28, loss = 0.54130219\n",
      "Iteration 29, loss = 0.54296581\n",
      "Iteration 30, loss = 0.54103121\n",
      "Iteration 31, loss = 0.53628630\n",
      "Iteration 32, loss = 0.53971176\n",
      "Iteration 33, loss = 0.53871604\n",
      "Iteration 34, loss = 0.53474733\n",
      "Iteration 35, loss = 0.53448349\n",
      "Iteration 36, loss = 0.53476891\n",
      "Iteration 37, loss = 0.53252939\n",
      "Iteration 38, loss = 0.53425425\n",
      "Iteration 39, loss = 0.53670155\n",
      "Iteration 40, loss = 0.53247500\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.54392523\n",
      "Iteration 2, loss = 0.91499803\n",
      "Iteration 3, loss = 0.74373165\n",
      "Iteration 4, loss = 0.66901539\n",
      "Iteration 5, loss = 0.62793777\n",
      "Iteration 6, loss = 0.61006519\n",
      "Iteration 7, loss = 0.59072348\n",
      "Iteration 8, loss = 0.58517892\n",
      "Iteration 9, loss = 0.57998735\n",
      "Iteration 10, loss = 0.57385954\n",
      "Iteration 11, loss = 0.56517804\n",
      "Iteration 12, loss = 0.56728952\n",
      "Iteration 13, loss = 0.56397174\n",
      "Iteration 14, loss = 0.56293303\n",
      "Iteration 15, loss = 0.55267712\n",
      "Iteration 16, loss = 0.55675207\n",
      "Iteration 17, loss = 0.55362850\n",
      "Iteration 18, loss = 0.54691376\n",
      "Iteration 19, loss = 0.54864420\n",
      "Iteration 20, loss = 0.54676734\n",
      "Iteration 21, loss = 0.54822366\n",
      "Iteration 22, loss = 0.54396677\n",
      "Iteration 23, loss = 0.54821766\n",
      "Iteration 24, loss = 0.54212521\n",
      "Iteration 25, loss = 0.54548567\n",
      "Iteration 26, loss = 0.53647065\n",
      "Iteration 27, loss = 0.54073086\n",
      "Iteration 28, loss = 0.53681263\n",
      "Iteration 29, loss = 0.53936406\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.27951577\n",
      "Iteration 2, loss = 0.85279182\n",
      "Iteration 3, loss = 0.74188399\n",
      "Iteration 4, loss = 0.68232420\n",
      "Iteration 5, loss = 0.64670610\n",
      "Iteration 6, loss = 0.62015350\n",
      "Iteration 7, loss = 0.60059852\n",
      "Iteration 8, loss = 0.59627115\n",
      "Iteration 9, loss = 0.57982044\n",
      "Iteration 10, loss = 0.58172421\n",
      "Iteration 11, loss = 0.57466666\n",
      "Iteration 12, loss = 0.57245340\n",
      "Iteration 13, loss = 0.56853242\n",
      "Iteration 14, loss = 0.56501701\n",
      "Iteration 15, loss = 0.55548792\n",
      "Iteration 16, loss = 0.55865028\n",
      "Iteration 17, loss = 0.55578408\n",
      "Iteration 18, loss = 0.54985038\n",
      "Iteration 19, loss = 0.55940206\n",
      "Iteration 20, loss = 0.55196465\n",
      "Iteration 21, loss = 0.54552740\n",
      "Iteration 22, loss = 0.54467825\n",
      "Iteration 23, loss = 0.54617753\n",
      "Iteration 24, loss = 0.54740623\n",
      "Iteration 25, loss = 0.54435371\n",
      "Iteration 26, loss = 0.54248125\n",
      "Iteration 27, loss = 0.54146237\n",
      "Iteration 28, loss = 0.54368238\n",
      "Iteration 29, loss = 0.53942955\n",
      "Iteration 30, loss = 0.54353350\n",
      "Iteration 31, loss = 0.53549419\n",
      "Iteration 32, loss = 0.54244826\n",
      "Iteration 33, loss = 0.54039356\n",
      "Iteration 34, loss = 0.53800796\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.27351368\n",
      "Iteration 2, loss = 0.85668671\n",
      "Iteration 3, loss = 0.74328572\n",
      "Iteration 4, loss = 0.68181893\n",
      "Iteration 5, loss = 0.64418478\n",
      "Iteration 6, loss = 0.61544213\n",
      "Iteration 7, loss = 0.60948156\n",
      "Iteration 8, loss = 0.59663760\n",
      "Iteration 9, loss = 0.58010608\n",
      "Iteration 10, loss = 0.57401662\n",
      "Iteration 11, loss = 0.57548825\n",
      "Iteration 12, loss = 0.56926403\n",
      "Iteration 13, loss = 0.56849032\n",
      "Iteration 14, loss = 0.56655614\n",
      "Iteration 15, loss = 0.56257921\n",
      "Iteration 16, loss = 0.56121206\n",
      "Iteration 17, loss = 0.55490336\n",
      "Iteration 18, loss = 0.55597691\n",
      "Iteration 19, loss = 0.55433499\n",
      "Iteration 20, loss = 0.55162121\n",
      "Iteration 21, loss = 0.55388468\n",
      "Iteration 22, loss = 0.55156284\n",
      "Iteration 23, loss = 0.54940719\n",
      "Iteration 24, loss = 0.54996856\n",
      "Iteration 25, loss = 0.54776461\n",
      "Iteration 26, loss = 0.54824772\n",
      "Iteration 27, loss = 0.54544423\n",
      "Iteration 28, loss = 0.54519207\n",
      "Iteration 29, loss = 0.54682737\n",
      "Iteration 30, loss = 0.54707592\n",
      "Iteration 31, loss = 0.54536375\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.27346149\n",
      "Iteration 2, loss = 0.85531451\n",
      "Iteration 3, loss = 0.73583954\n",
      "Iteration 4, loss = 0.68393189\n",
      "Iteration 5, loss = 0.64870537\n",
      "Iteration 6, loss = 0.62332545\n",
      "Iteration 7, loss = 0.60591654\n",
      "Iteration 8, loss = 0.59483409\n",
      "Iteration 9, loss = 0.58173304\n",
      "Iteration 10, loss = 0.57720663\n",
      "Iteration 11, loss = 0.57112077\n",
      "Iteration 12, loss = 0.56839286\n",
      "Iteration 13, loss = 0.56544188\n",
      "Iteration 14, loss = 0.56280904\n",
      "Iteration 15, loss = 0.56082094\n",
      "Iteration 16, loss = 0.55618065\n",
      "Iteration 17, loss = 0.55337587\n",
      "Iteration 18, loss = 0.55249652\n",
      "Iteration 19, loss = 0.54921081\n",
      "Iteration 20, loss = 0.55151786\n",
      "Iteration 21, loss = 0.55475555\n",
      "Iteration 22, loss = 0.55087792\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.45276784\n",
      "Iteration 2, loss = 0.88195146\n",
      "Iteration 3, loss = 0.74167864\n",
      "Iteration 4, loss = 0.67210537\n",
      "Iteration 5, loss = 0.63887846\n",
      "Iteration 6, loss = 0.61423904\n",
      "Iteration 7, loss = 0.59722257\n",
      "Iteration 8, loss = 0.58781568\n",
      "Iteration 9, loss = 0.58270954\n",
      "Iteration 10, loss = 0.57709640\n",
      "Iteration 11, loss = 0.57371018\n",
      "Iteration 12, loss = 0.56985851\n",
      "Iteration 13, loss = 0.56733313\n",
      "Iteration 14, loss = 0.56430310\n",
      "Iteration 15, loss = 0.56182720\n",
      "Iteration 16, loss = 0.55917650\n",
      "Iteration 17, loss = 0.55675390\n",
      "Iteration 18, loss = 0.55392791\n",
      "Iteration 19, loss = 0.55357631\n",
      "Iteration 20, loss = 0.55389467\n",
      "Iteration 21, loss = 0.54871835\n",
      "Iteration 22, loss = 0.54578827\n",
      "Iteration 23, loss = 0.54966263\n",
      "Iteration 24, loss = 0.54830135\n",
      "Iteration 25, loss = 0.54676322\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.46149543\n",
      "Iteration 2, loss = 0.89335224\n",
      "Iteration 3, loss = 0.74443321\n",
      "Iteration 4, loss = 0.67750638\n",
      "Iteration 5, loss = 0.64386340\n",
      "Iteration 6, loss = 0.61560982\n",
      "Iteration 7, loss = 0.60353992\n",
      "Iteration 8, loss = 0.59768999\n",
      "Iteration 9, loss = 0.58635774\n",
      "Iteration 10, loss = 0.58156987\n",
      "Iteration 11, loss = 0.57772955\n",
      "Iteration 12, loss = 0.57235544\n",
      "Iteration 13, loss = 0.57216826\n",
      "Iteration 14, loss = 0.56884143\n",
      "Iteration 15, loss = 0.56496233\n",
      "Iteration 16, loss = 0.56036202\n",
      "Iteration 17, loss = 0.55988925\n",
      "Iteration 18, loss = 0.55468367\n",
      "Iteration 19, loss = 0.55532295\n",
      "Iteration 20, loss = 0.55658108\n",
      "Iteration 21, loss = 0.54868394\n",
      "Iteration 22, loss = 0.54912978\n",
      "Iteration 23, loss = 0.55061509\n",
      "Iteration 24, loss = 0.54983965\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.47183811\n",
      "Iteration 2, loss = 0.89985654\n",
      "Iteration 3, loss = 0.75207695\n",
      "Iteration 4, loss = 0.67952823\n",
      "Iteration 5, loss = 0.64804651\n",
      "Iteration 6, loss = 0.61974287\n",
      "Iteration 7, loss = 0.60096788\n",
      "Iteration 8, loss = 0.59310168\n",
      "Iteration 9, loss = 0.58533986\n",
      "Iteration 10, loss = 0.57783417\n",
      "Iteration 11, loss = 0.57418336\n",
      "Iteration 12, loss = 0.56777098\n",
      "Iteration 13, loss = 0.56448029\n",
      "Iteration 14, loss = 0.56802899\n",
      "Iteration 15, loss = 0.56109819\n",
      "Iteration 16, loss = 0.56125585\n",
      "Iteration 17, loss = 0.55748209\n",
      "Iteration 18, loss = 0.55432639\n",
      "Iteration 19, loss = 0.55585925\n",
      "Iteration 20, loss = 0.55206809\n",
      "Iteration 21, loss = 0.54991119\n",
      "Iteration 22, loss = 0.54890386\n",
      "Iteration 23, loss = 0.54975211\n",
      "Iteration 24, loss = 0.54650289\n",
      "Iteration 25, loss = 0.54711543\n",
      "Iteration 26, loss = 0.54190468\n",
      "Iteration 27, loss = 0.54436434\n",
      "Iteration 28, loss = 0.54298492\n",
      "Iteration 29, loss = 0.54373507\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.82455188\n",
      "Iteration 2, loss = 0.86645477\n",
      "Iteration 3, loss = 0.78303067\n",
      "Iteration 4, loss = 0.76037563\n",
      "Iteration 5, loss = 0.75576303\n",
      "Iteration 6, loss = 0.74614817\n",
      "Iteration 7, loss = 0.75074755\n",
      "Iteration 8, loss = 0.74986161\n",
      "Iteration 9, loss = 0.74400453\n",
      "Iteration 10, loss = 0.74754654\n",
      "Iteration 11, loss = 0.74647867\n",
      "Iteration 12, loss = 0.74389108\n",
      "Iteration 13, loss = 0.74491624\n",
      "Iteration 14, loss = 0.74442004\n",
      "Iteration 15, loss = 0.73872831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 0.73502484\n",
      "Iteration 17, loss = 0.73465496\n",
      "Iteration 18, loss = 0.73800128\n",
      "Iteration 19, loss = 0.73986605\n",
      "Iteration 20, loss = 0.74094815\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.80527981\n",
      "Iteration 2, loss = 0.84888507\n",
      "Iteration 3, loss = 0.78655915\n",
      "Iteration 4, loss = 0.76311336\n",
      "Iteration 5, loss = 0.75209611\n",
      "Iteration 6, loss = 0.75860018\n",
      "Iteration 7, loss = 0.75230696\n",
      "Iteration 8, loss = 0.75870936\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.86384050\n",
      "Iteration 2, loss = 0.87138283\n",
      "Iteration 3, loss = 0.77576447\n",
      "Iteration 4, loss = 0.75933773\n",
      "Iteration 5, loss = 0.75097209\n",
      "Iteration 6, loss = 0.75469170\n",
      "Iteration 7, loss = 0.74697409\n",
      "Iteration 8, loss = 0.75424846\n",
      "Iteration 9, loss = 0.74650306\n",
      "Iteration 10, loss = 0.74340432\n",
      "Iteration 11, loss = 0.74171071\n",
      "Iteration 12, loss = 0.74303827\n",
      "Iteration 13, loss = 0.74516279\n",
      "Iteration 14, loss = 0.74298243\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.54074113\n",
      "Iteration 2, loss = 0.87153290\n",
      "Iteration 3, loss = 0.78330861\n",
      "Iteration 4, loss = 0.75735014\n",
      "Iteration 5, loss = 0.75035663\n",
      "Iteration 6, loss = 0.74612341\n",
      "Iteration 7, loss = 0.74635550\n",
      "Iteration 8, loss = 0.74300060\n",
      "Iteration 9, loss = 0.74193796\n",
      "Iteration 10, loss = 0.73690549\n",
      "Iteration 11, loss = 0.73875897\n",
      "Iteration 12, loss = 0.74160824\n",
      "Iteration 13, loss = 0.73544032\n",
      "Iteration 14, loss = 0.73698182\n",
      "Iteration 15, loss = 0.73568162\n",
      "Iteration 16, loss = 0.73795131\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.55856209\n",
      "Iteration 2, loss = 0.87767601\n",
      "Iteration 3, loss = 0.78499214\n",
      "Iteration 4, loss = 0.76043815\n",
      "Iteration 5, loss = 0.75571726\n",
      "Iteration 6, loss = 0.75414439\n",
      "Iteration 7, loss = 0.75065612\n",
      "Iteration 8, loss = 0.74240518\n",
      "Iteration 9, loss = 0.74653285\n",
      "Iteration 10, loss = 0.74678916\n",
      "Iteration 11, loss = 0.74210978\n",
      "Iteration 12, loss = 0.74026500\n",
      "Iteration 13, loss = 0.74079034\n",
      "Iteration 14, loss = 0.74007461\n",
      "Iteration 15, loss = 0.74211483\n",
      "Iteration 16, loss = 0.73659752\n",
      "Iteration 17, loss = 0.73831905\n",
      "Iteration 18, loss = 0.73819869\n",
      "Iteration 19, loss = 0.73538173\n",
      "Iteration 20, loss = 0.73717585\n",
      "Iteration 21, loss = 0.73400091\n",
      "Iteration 22, loss = 0.73976832\n",
      "Iteration 23, loss = 0.73250530\n",
      "Iteration 24, loss = 0.73535660\n",
      "Iteration 25, loss = 0.73510133\n",
      "Iteration 26, loss = 0.73615460\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.56236802\n",
      "Iteration 2, loss = 0.86882442\n",
      "Iteration 3, loss = 0.78122581\n",
      "Iteration 4, loss = 0.75791888\n",
      "Iteration 5, loss = 0.74813148\n",
      "Iteration 6, loss = 0.74969835\n",
      "Iteration 7, loss = 0.74440050\n",
      "Iteration 8, loss = 0.73912338\n",
      "Iteration 9, loss = 0.73894905\n",
      "Iteration 10, loss = 0.73899981\n",
      "Iteration 11, loss = 0.74347752\n",
      "Iteration 12, loss = 0.73548567\n",
      "Iteration 13, loss = 0.73669096\n",
      "Iteration 14, loss = 0.74038362\n",
      "Iteration 15, loss = 0.73396011\n",
      "Iteration 16, loss = 0.73411882\n",
      "Iteration 17, loss = 0.73292950\n",
      "Iteration 18, loss = 0.73027745\n",
      "Iteration 19, loss = 0.73615383\n",
      "Iteration 20, loss = 0.73371214\n",
      "Iteration 21, loss = 0.72971582\n",
      "Iteration 22, loss = 0.73400191\n",
      "Iteration 23, loss = 0.73282700\n",
      "Iteration 24, loss = 0.73303002\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38822421\n",
      "Iteration 2, loss = 1.00284999\n",
      "Iteration 3, loss = 0.87017977\n",
      "Iteration 4, loss = 0.82937581\n",
      "Iteration 5, loss = 0.80912441\n",
      "Iteration 6, loss = 0.80760177\n",
      "Iteration 7, loss = 0.80240790\n",
      "Iteration 8, loss = 0.79446531\n",
      "Iteration 9, loss = 0.78605138\n",
      "Iteration 10, loss = 0.78672679\n",
      "Iteration 11, loss = 0.78067607\n",
      "Iteration 12, loss = 0.78590885\n",
      "Iteration 13, loss = 0.78137438\n",
      "Iteration 14, loss = 0.78206134\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.40042676\n",
      "Iteration 2, loss = 1.00943959\n",
      "Iteration 3, loss = 0.87524055\n",
      "Iteration 4, loss = 0.83404816\n",
      "Iteration 5, loss = 0.81626102\n",
      "Iteration 6, loss = 0.81113647\n",
      "Iteration 7, loss = 0.80352856\n",
      "Iteration 8, loss = 0.80163611\n",
      "Iteration 9, loss = 0.79364399\n",
      "Iteration 10, loss = 0.79352798\n",
      "Iteration 11, loss = 0.78664641\n",
      "Iteration 12, loss = 0.79084841\n",
      "Iteration 13, loss = 0.78400165\n",
      "Iteration 14, loss = 0.78487155\n",
      "Iteration 15, loss = 0.78101599\n",
      "Iteration 16, loss = 0.78115107\n",
      "Iteration 17, loss = 0.77894087\n",
      "Iteration 18, loss = 0.77585132\n",
      "Iteration 19, loss = 0.77631604\n",
      "Iteration 20, loss = 0.77050658\n",
      "Iteration 21, loss = 0.77311261\n",
      "Iteration 22, loss = 0.76963283\n",
      "Iteration 23, loss = 0.77184274\n",
      "Iteration 24, loss = 0.76725148\n",
      "Iteration 25, loss = 0.77238663\n",
      "Iteration 26, loss = 0.76845592\n",
      "Iteration 27, loss = 0.76620178\n",
      "Iteration 28, loss = 0.76857472\n",
      "Iteration 29, loss = 0.76681503\n",
      "Iteration 30, loss = 0.76975232\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.40651881\n",
      "Iteration 2, loss = 1.01578493\n",
      "Iteration 3, loss = 0.86881785\n",
      "Iteration 4, loss = 0.83041794\n",
      "Iteration 5, loss = 0.81131423\n",
      "Iteration 6, loss = 0.80720096\n",
      "Iteration 7, loss = 0.79682874\n",
      "Iteration 8, loss = 0.79760249\n",
      "Iteration 9, loss = 0.79373153\n",
      "Iteration 10, loss = 0.78976575\n",
      "Iteration 11, loss = 0.78178030\n",
      "Iteration 12, loss = 0.78753922\n",
      "Iteration 13, loss = 0.78355378\n",
      "Iteration 14, loss = 0.78317278\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.00154311\n",
      "Iteration 2, loss = 1.04916076\n",
      "Iteration 3, loss = 0.90472841\n",
      "Iteration 4, loss = 0.85263909\n",
      "Iteration 5, loss = 0.83116232\n",
      "Iteration 6, loss = 0.81490668\n",
      "Iteration 7, loss = 0.80605942\n",
      "Iteration 8, loss = 0.80632836\n",
      "Iteration 9, loss = 0.79484705\n",
      "Iteration 10, loss = 0.79681784\n",
      "Iteration 11, loss = 0.79165158\n",
      "Iteration 12, loss = 0.79203880\n",
      "Iteration 13, loss = 0.79069160\n",
      "Iteration 14, loss = 0.78810223\n",
      "Iteration 15, loss = 0.77720974\n",
      "Iteration 16, loss = 0.78038124\n",
      "Iteration 17, loss = 0.78020382\n",
      "Iteration 18, loss = 0.77322971\n",
      "Iteration 19, loss = 0.77877361\n",
      "Iteration 20, loss = 0.77556827\n",
      "Iteration 21, loss = 0.77138398\n",
      "Iteration 22, loss = 0.76993202\n",
      "Iteration 23, loss = 0.77048492\n",
      "Iteration 24, loss = 0.77099034\n",
      "Iteration 25, loss = 0.76765484\n",
      "Iteration 26, loss = 0.76524548\n",
      "Iteration 27, loss = 0.76403925\n",
      "Iteration 28, loss = 0.76678061\n",
      "Iteration 29, loss = 0.76537563\n",
      "Iteration 30, loss = 0.76910503\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.98029526\n",
      "Iteration 2, loss = 1.04036219\n",
      "Iteration 3, loss = 0.90120195\n",
      "Iteration 4, loss = 0.85116144\n",
      "Iteration 5, loss = 0.82764731\n",
      "Iteration 6, loss = 0.81090283\n",
      "Iteration 7, loss = 0.81141847\n",
      "Iteration 8, loss = 0.80866548\n",
      "Iteration 9, loss = 0.79441380\n",
      "Iteration 10, loss = 0.79208775\n",
      "Iteration 11, loss = 0.79307136\n",
      "Iteration 12, loss = 0.78964072\n",
      "Iteration 13, loss = 0.79158127\n",
      "Iteration 14, loss = 0.78788169\n",
      "Iteration 15, loss = 0.78432455\n",
      "Iteration 16, loss = 0.78760805\n",
      "Iteration 17, loss = 0.78095248\n",
      "Iteration 18, loss = 0.77889190\n",
      "Iteration 19, loss = 0.78039637\n",
      "Iteration 20, loss = 0.77908831\n",
      "Iteration 21, loss = 0.77876433\n",
      "Iteration 22, loss = 0.77526291\n",
      "Iteration 23, loss = 0.77568018\n",
      "Iteration 24, loss = 0.77408611\n",
      "Iteration 25, loss = 0.77217244\n",
      "Iteration 26, loss = 0.77380534\n",
      "Iteration 27, loss = 0.77225616\n",
      "Iteration 28, loss = 0.77140037\n",
      "Iteration 29, loss = 0.77157792\n",
      "Iteration 30, loss = 0.77083719\n",
      "Iteration 31, loss = 0.77134598\n",
      "Iteration 32, loss = 0.76931072\n",
      "Iteration 33, loss = 0.76969869\n",
      "Iteration 34, loss = 0.76707461\n",
      "Iteration 35, loss = 0.76479910\n",
      "Iteration 36, loss = 0.77013822\n",
      "Iteration 37, loss = 0.76599406\n",
      "Iteration 38, loss = 0.76523722\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.98139716\n",
      "Iteration 2, loss = 1.04590947\n",
      "Iteration 3, loss = 0.89587339\n",
      "Iteration 4, loss = 0.85128773\n",
      "Iteration 5, loss = 0.82722812\n",
      "Iteration 6, loss = 0.81487994\n",
      "Iteration 7, loss = 0.80616765\n",
      "Iteration 8, loss = 0.80017936\n",
      "Iteration 9, loss = 0.79264726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.79325390\n",
      "Iteration 11, loss = 0.78661966\n",
      "Iteration 12, loss = 0.78545050\n",
      "Iteration 13, loss = 0.78207080\n",
      "Iteration 14, loss = 0.77863749\n",
      "Iteration 15, loss = 0.78219109\n",
      "Iteration 16, loss = 0.77667339\n",
      "Iteration 17, loss = 0.77672711\n",
      "Iteration 18, loss = 0.77509619\n",
      "Iteration 19, loss = 0.77563250\n",
      "Iteration 20, loss = 0.77462014\n",
      "Iteration 21, loss = 0.77422976\n",
      "Iteration 22, loss = 0.77667881\n",
      "Iteration 23, loss = 0.77186298\n",
      "Iteration 24, loss = 0.76871420\n",
      "Iteration 25, loss = 0.76925677\n",
      "Iteration 26, loss = 0.76463734\n",
      "Iteration 27, loss = 0.76669923\n",
      "Iteration 28, loss = 0.76790801\n",
      "Iteration 29, loss = 0.76664570\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.26335773\n",
      "Iteration 2, loss = 1.02003702\n",
      "Iteration 3, loss = 0.88523356\n",
      "Iteration 4, loss = 0.84105793\n",
      "Iteration 5, loss = 0.82954213\n",
      "Iteration 6, loss = 0.81455965\n",
      "Iteration 7, loss = 0.80491128\n",
      "Iteration 8, loss = 0.80010402\n",
      "Iteration 9, loss = 0.79570378\n",
      "Iteration 10, loss = 0.79131078\n",
      "Iteration 11, loss = 0.79392286\n",
      "Iteration 12, loss = 0.78745908\n",
      "Iteration 13, loss = 0.78431633\n",
      "Iteration 14, loss = 0.78338259\n",
      "Iteration 15, loss = 0.78061226\n",
      "Iteration 16, loss = 0.77818197\n",
      "Iteration 17, loss = 0.77656603\n",
      "Iteration 18, loss = 0.77514084\n",
      "Iteration 19, loss = 0.77616116\n",
      "Iteration 20, loss = 0.77257657\n",
      "Iteration 21, loss = 0.77368623\n",
      "Iteration 22, loss = 0.76902380\n",
      "Iteration 23, loss = 0.77090224\n",
      "Iteration 24, loss = 0.76856906\n",
      "Iteration 25, loss = 0.76901450\n",
      "Iteration 26, loss = 0.77203959\n",
      "Iteration 27, loss = 0.76960342\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.26487299\n",
      "Iteration 2, loss = 1.02496602\n",
      "Iteration 3, loss = 0.88527036\n",
      "Iteration 4, loss = 0.84704038\n",
      "Iteration 5, loss = 0.83307551\n",
      "Iteration 6, loss = 0.81847188\n",
      "Iteration 7, loss = 0.81239162\n",
      "Iteration 8, loss = 0.81166886\n",
      "Iteration 9, loss = 0.80568856\n",
      "Iteration 10, loss = 0.79688577\n",
      "Iteration 11, loss = 0.79748167\n",
      "Iteration 12, loss = 0.79140866\n",
      "Iteration 13, loss = 0.78872839\n",
      "Iteration 14, loss = 0.78973590\n",
      "Iteration 15, loss = 0.78872840\n",
      "Iteration 16, loss = 0.78195612\n",
      "Iteration 17, loss = 0.78115311\n",
      "Iteration 18, loss = 0.78003223\n",
      "Iteration 19, loss = 0.77775570\n",
      "Iteration 20, loss = 0.77804111\n",
      "Iteration 21, loss = 0.77370542\n",
      "Iteration 22, loss = 0.77244280\n",
      "Iteration 23, loss = 0.77541175\n",
      "Iteration 24, loss = 0.77252670\n",
      "Iteration 25, loss = 0.77608311\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.27514122\n",
      "Iteration 2, loss = 1.02951032\n",
      "Iteration 3, loss = 0.89042595\n",
      "Iteration 4, loss = 0.84384671\n",
      "Iteration 5, loss = 0.83011828\n",
      "Iteration 6, loss = 0.81674394\n",
      "Iteration 7, loss = 0.80099425\n",
      "Iteration 8, loss = 0.80366168\n",
      "Iteration 9, loss = 0.79843702\n",
      "Iteration 10, loss = 0.79006334\n",
      "Iteration 11, loss = 0.78792715\n",
      "Iteration 12, loss = 0.78527911\n",
      "Iteration 13, loss = 0.78200431\n",
      "Iteration 14, loss = 0.78126366\n",
      "Iteration 15, loss = 0.78043717\n",
      "Iteration 16, loss = 0.77841494\n",
      "Iteration 17, loss = 0.77699626\n",
      "Iteration 18, loss = 0.77541436\n",
      "Iteration 19, loss = 0.78080231\n",
      "Iteration 20, loss = 0.77240434\n",
      "Iteration 21, loss = 0.76889727\n",
      "Iteration 22, loss = 0.76868066\n",
      "Iteration 23, loss = 0.77128096\n",
      "Iteration 24, loss = 0.76885243\n",
      "Iteration 25, loss = 0.76787545\n",
      "Iteration 26, loss = 0.76559356\n",
      "Iteration 27, loss = 0.76665185\n",
      "Iteration 28, loss = 0.76555863\n",
      "Iteration 29, loss = 0.76485732\n",
      "Iteration 30, loss = 0.76724661\n",
      "Iteration 31, loss = 0.76359323\n",
      "Iteration 32, loss = 0.76364655\n",
      "Iteration 33, loss = 0.76576473\n",
      "Iteration 34, loss = 0.76252036\n",
      "Iteration 35, loss = 0.76235868\n",
      "Iteration 36, loss = 0.76034387\n",
      "Iteration 37, loss = 0.75980231\n",
      "Iteration 38, loss = 0.76411944\n",
      "Iteration 39, loss = 0.76211573\n",
      "Iteration 40, loss = 0.75685492\n",
      "Iteration 41, loss = 0.75853583\n",
      "Iteration 42, loss = 0.76165592\n",
      "Iteration 43, loss = 0.75735007\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.86760691\n",
      "Iteration 2, loss = 1.12073672\n",
      "Iteration 3, loss = 1.09906863\n",
      "Iteration 4, loss = 1.08593860\n",
      "Iteration 5, loss = 1.08272167\n",
      "Iteration 6, loss = 1.07683079\n",
      "Iteration 7, loss = 1.07846730\n",
      "Iteration 8, loss = 1.07729203\n",
      "Iteration 9, loss = 1.07475524\n",
      "Iteration 10, loss = 1.07499387\n",
      "Iteration 11, loss = 1.07188817\n",
      "Iteration 12, loss = 1.07029361\n",
      "Iteration 13, loss = 1.07064120\n",
      "Iteration 14, loss = 1.07238170\n",
      "Iteration 15, loss = 1.07062020\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.84801356\n",
      "Iteration 2, loss = 1.12159872\n",
      "Iteration 3, loss = 1.09918360\n",
      "Iteration 4, loss = 1.08721748\n",
      "Iteration 5, loss = 1.07924901\n",
      "Iteration 6, loss = 1.08481177\n",
      "Iteration 7, loss = 1.07647817\n",
      "Iteration 8, loss = 1.08061130\n",
      "Iteration 9, loss = 1.07567370\n",
      "Iteration 10, loss = 1.07738523\n",
      "Iteration 11, loss = 1.07207695\n",
      "Iteration 12, loss = 1.07213082\n",
      "Iteration 13, loss = 1.06812313\n",
      "Iteration 14, loss = 1.07157346\n",
      "Iteration 15, loss = 1.07184377\n",
      "Iteration 16, loss = 1.06772885\n",
      "Iteration 17, loss = 1.06922840\n",
      "Iteration 18, loss = 1.07007702\n",
      "Iteration 19, loss = 1.06818297\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.95607735\n",
      "Iteration 2, loss = 1.12210153\n",
      "Iteration 3, loss = 1.09577571\n",
      "Iteration 4, loss = 1.08547019\n",
      "Iteration 5, loss = 1.08012405\n",
      "Iteration 6, loss = 1.07979606\n",
      "Iteration 7, loss = 1.07635969\n",
      "Iteration 8, loss = 1.07689132\n",
      "Iteration 9, loss = 1.07186046\n",
      "Iteration 10, loss = 1.07243213\n",
      "Iteration 11, loss = 1.07120142\n",
      "Iteration 12, loss = 1.07244190\n",
      "Iteration 13, loss = 1.06905857\n",
      "Iteration 14, loss = 1.06977660\n",
      "Iteration 15, loss = 1.07026581\n",
      "Iteration 16, loss = 1.06820134\n",
      "Iteration 17, loss = 1.07053112\n",
      "Iteration 18, loss = 1.06729514\n",
      "Iteration 19, loss = 1.06531623\n",
      "Iteration 20, loss = 1.06558303\n",
      "Iteration 21, loss = 1.06840730\n",
      "Iteration 22, loss = 1.06894300\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.40022752\n",
      "Iteration 2, loss = 1.13166187\n",
      "Iteration 3, loss = 1.10140503\n",
      "Iteration 4, loss = 1.09026245\n",
      "Iteration 5, loss = 1.08339361\n",
      "Iteration 6, loss = 1.07893307\n",
      "Iteration 7, loss = 1.07684333\n",
      "Iteration 8, loss = 1.07392406\n",
      "Iteration 9, loss = 1.07258051\n",
      "Iteration 10, loss = 1.06982131\n",
      "Iteration 11, loss = 1.06884461\n",
      "Iteration 12, loss = 1.07068603\n",
      "Iteration 13, loss = 1.06670676\n",
      "Iteration 14, loss = 1.06920873\n",
      "Iteration 15, loss = 1.06710369\n",
      "Iteration 16, loss = 1.06756126\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.40671896\n",
      "Iteration 2, loss = 1.13084882\n",
      "Iteration 3, loss = 1.10078612\n",
      "Iteration 4, loss = 1.09030292\n",
      "Iteration 5, loss = 1.08465426\n",
      "Iteration 6, loss = 1.08172545\n",
      "Iteration 7, loss = 1.07864967\n",
      "Iteration 8, loss = 1.07314053\n",
      "Iteration 9, loss = 1.07625759\n",
      "Iteration 10, loss = 1.07520849\n",
      "Iteration 11, loss = 1.07043626\n",
      "Iteration 12, loss = 1.06842494\n",
      "Iteration 13, loss = 1.06930833\n",
      "Iteration 14, loss = 1.07059995\n",
      "Iteration 15, loss = 1.06892542\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.42618371\n",
      "Iteration 2, loss = 1.12906893\n",
      "Iteration 3, loss = 1.09881484\n",
      "Iteration 4, loss = 1.08929479\n",
      "Iteration 5, loss = 1.08086274\n",
      "Iteration 6, loss = 1.08123007\n",
      "Iteration 7, loss = 1.07790169\n",
      "Iteration 8, loss = 1.07238250\n",
      "Iteration 9, loss = 1.07102659\n",
      "Iteration 10, loss = 1.07246641\n",
      "Iteration 11, loss = 1.07013487\n",
      "Iteration 12, loss = 1.06817286\n",
      "Iteration 13, loss = 1.06921535\n",
      "Iteration 14, loss = 1.07090486\n",
      "Iteration 15, loss = 1.06385854\n",
      "Iteration 16, loss = 1.06600536\n",
      "Iteration 17, loss = 1.06409297\n",
      "Iteration 18, loss = 1.06324790\n",
      "Iteration 19, loss = 1.06806670\n",
      "Iteration 20, loss = 1.06495969\n",
      "Iteration 21, loss = 1.06236434\n",
      "Iteration 22, loss = 1.06467315\n",
      "Iteration 23, loss = 1.06391946\n",
      "Iteration 24, loss = 1.06538509\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.10358559\n",
      "Iteration 2, loss = 1.29390425\n",
      "Iteration 3, loss = 1.24723563\n",
      "Iteration 4, loss = 1.22545637\n",
      "Iteration 5, loss = 1.21415860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 1.21465687\n",
      "Iteration 7, loss = 1.20390681\n",
      "Iteration 8, loss = 1.20083027\n",
      "Iteration 9, loss = 1.19772591\n",
      "Iteration 10, loss = 1.19695579\n",
      "Iteration 11, loss = 1.19338607\n",
      "Iteration 12, loss = 1.19231034\n",
      "Iteration 13, loss = 1.19232849\n",
      "Iteration 14, loss = 1.19301583\n",
      "Iteration 15, loss = 1.18959463\n",
      "Iteration 16, loss = 1.18707000\n",
      "Iteration 17, loss = 1.18965354\n",
      "Iteration 18, loss = 1.18792417\n",
      "Iteration 19, loss = 1.18390933\n",
      "Iteration 20, loss = 1.18330583\n",
      "Iteration 21, loss = 1.18422999\n",
      "Iteration 22, loss = 1.18196277\n",
      "Iteration 23, loss = 1.18416903\n",
      "Iteration 24, loss = 1.18018401\n",
      "Iteration 25, loss = 1.18310233\n",
      "Iteration 26, loss = 1.18056542\n",
      "Iteration 27, loss = 1.17952702\n",
      "Iteration 28, loss = 1.18121102\n",
      "Iteration 29, loss = 1.17854138\n",
      "Iteration 30, loss = 1.17871363\n",
      "Iteration 31, loss = 1.17639480\n",
      "Iteration 32, loss = 1.17831570\n",
      "Iteration 33, loss = 1.17936730\n",
      "Iteration 34, loss = 1.17778332\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.12201939\n",
      "Iteration 2, loss = 1.29600180\n",
      "Iteration 3, loss = 1.24649981\n",
      "Iteration 4, loss = 1.23494147\n",
      "Iteration 5, loss = 1.21855486\n",
      "Iteration 6, loss = 1.21527178\n",
      "Iteration 7, loss = 1.20683036\n",
      "Iteration 8, loss = 1.20763688\n",
      "Iteration 9, loss = 1.19893402\n",
      "Iteration 10, loss = 1.19780024\n",
      "Iteration 11, loss = 1.19773017\n",
      "Iteration 12, loss = 1.19713067\n",
      "Iteration 13, loss = 1.19249764\n",
      "Iteration 14, loss = 1.19432142\n",
      "Iteration 15, loss = 1.19353271\n",
      "Iteration 16, loss = 1.19102173\n",
      "Iteration 17, loss = 1.18961495\n",
      "Iteration 18, loss = 1.18909523\n",
      "Iteration 19, loss = 1.18790523\n",
      "Iteration 20, loss = 1.18845093\n",
      "Iteration 21, loss = 1.18540983\n",
      "Iteration 22, loss = 1.18664086\n",
      "Iteration 23, loss = 1.18528123\n",
      "Iteration 24, loss = 1.18238301\n",
      "Iteration 25, loss = 1.18450936\n",
      "Iteration 26, loss = 1.18185825\n",
      "Iteration 27, loss = 1.18085201\n",
      "Iteration 28, loss = 1.18182685\n",
      "Iteration 29, loss = 1.18282133\n",
      "Iteration 30, loss = 1.18379476\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.14948392\n",
      "Iteration 2, loss = 1.30578976\n",
      "Iteration 3, loss = 1.24778476\n",
      "Iteration 4, loss = 1.22876737\n",
      "Iteration 5, loss = 1.21639433\n",
      "Iteration 6, loss = 1.20790730\n",
      "Iteration 7, loss = 1.20421145\n",
      "Iteration 8, loss = 1.20523996\n",
      "Iteration 9, loss = 1.19832214\n",
      "Iteration 10, loss = 1.19857902\n",
      "Iteration 11, loss = 1.19561985\n",
      "Iteration 12, loss = 1.19822729\n",
      "Iteration 13, loss = 1.19649624\n",
      "Iteration 14, loss = 1.19092354\n",
      "Iteration 15, loss = 1.18835208\n",
      "Iteration 16, loss = 1.18915290\n",
      "Iteration 17, loss = 1.18868212\n",
      "Iteration 18, loss = 1.18658991\n",
      "Iteration 19, loss = 1.18516156\n",
      "Iteration 20, loss = 1.18662532\n",
      "Iteration 21, loss = 1.18404147\n",
      "Iteration 22, loss = 1.18523730\n",
      "Iteration 23, loss = 1.18665740\n",
      "Iteration 24, loss = 1.18065977\n",
      "Iteration 25, loss = 1.18052495\n",
      "Iteration 26, loss = 1.18039817\n",
      "Iteration 27, loss = 1.18095469\n",
      "Iteration 28, loss = 1.17979892\n",
      "Iteration 29, loss = 1.18267817\n",
      "Iteration 30, loss = 1.18098069\n",
      "Iteration 31, loss = 1.17872620\n",
      "Iteration 32, loss = 1.18068643\n",
      "Iteration 33, loss = 1.17943010\n",
      "Iteration 34, loss = 1.17833722\n",
      "Iteration 35, loss = 1.17996489\n",
      "Iteration 36, loss = 1.17661867\n",
      "Iteration 37, loss = 1.17930563\n",
      "Iteration 38, loss = 1.17771449\n",
      "Iteration 39, loss = 1.18106896\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.41238698\n",
      "Iteration 2, loss = 1.35069172\n",
      "Iteration 3, loss = 1.26951208\n",
      "Iteration 4, loss = 1.24242972\n",
      "Iteration 5, loss = 1.22772435\n",
      "Iteration 6, loss = 1.21641276\n",
      "Iteration 7, loss = 1.21087575\n",
      "Iteration 8, loss = 1.20665952\n",
      "Iteration 9, loss = 1.20108408\n",
      "Iteration 10, loss = 1.20078149\n",
      "Iteration 11, loss = 1.19748887\n",
      "Iteration 12, loss = 1.19688827\n",
      "Iteration 13, loss = 1.19773719\n",
      "Iteration 14, loss = 1.19292780\n",
      "Iteration 15, loss = 1.18950030\n",
      "Iteration 16, loss = 1.19304372\n",
      "Iteration 17, loss = 1.19097141\n",
      "Iteration 18, loss = 1.18630905\n",
      "Iteration 19, loss = 1.18702495\n",
      "Iteration 20, loss = 1.18714509\n",
      "Iteration 21, loss = 1.18507329\n",
      "Iteration 22, loss = 1.18401882\n",
      "Iteration 23, loss = 1.18437291\n",
      "Iteration 24, loss = 1.18587720\n",
      "Iteration 25, loss = 1.18348572\n",
      "Iteration 26, loss = 1.18329402\n",
      "Iteration 27, loss = 1.18147250\n",
      "Iteration 28, loss = 1.18301862\n",
      "Iteration 29, loss = 1.18196058\n",
      "Iteration 30, loss = 1.18182204\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.37219436\n",
      "Iteration 2, loss = 1.34423787\n",
      "Iteration 3, loss = 1.27178644\n",
      "Iteration 4, loss = 1.24450672\n",
      "Iteration 5, loss = 1.23179060\n",
      "Iteration 6, loss = 1.21667388\n",
      "Iteration 7, loss = 1.21382193\n",
      "Iteration 8, loss = 1.21023853\n",
      "Iteration 9, loss = 1.20094032\n",
      "Iteration 10, loss = 1.19899458\n",
      "Iteration 11, loss = 1.19792156\n",
      "Iteration 12, loss = 1.19708298\n",
      "Iteration 13, loss = 1.19908251\n",
      "Iteration 14, loss = 1.19505133\n",
      "Iteration 15, loss = 1.19365915\n",
      "Iteration 16, loss = 1.19438383\n",
      "Iteration 17, loss = 1.18946934\n",
      "Iteration 18, loss = 1.18823651\n",
      "Iteration 19, loss = 1.18898034\n",
      "Iteration 20, loss = 1.19154920\n",
      "Iteration 21, loss = 1.18896694\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.39473313\n",
      "Iteration 2, loss = 1.35205788\n",
      "Iteration 3, loss = 1.26924074\n",
      "Iteration 4, loss = 1.24467867\n",
      "Iteration 5, loss = 1.22782072\n",
      "Iteration 6, loss = 1.21849996\n",
      "Iteration 7, loss = 1.21093888\n",
      "Iteration 8, loss = 1.20841746\n",
      "Iteration 9, loss = 1.20055806\n",
      "Iteration 10, loss = 1.20093916\n",
      "Iteration 11, loss = 1.19713635\n",
      "Iteration 12, loss = 1.19447938\n",
      "Iteration 13, loss = 1.19614940\n",
      "Iteration 14, loss = 1.19196416\n",
      "Iteration 15, loss = 1.19249531\n",
      "Iteration 16, loss = 1.19047225\n",
      "Iteration 17, loss = 1.18777117\n",
      "Iteration 18, loss = 1.18738664\n",
      "Iteration 19, loss = 1.18766164\n",
      "Iteration 20, loss = 1.18857306\n",
      "Iteration 21, loss = 1.18915313\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.92649701\n",
      "Iteration 2, loss = 1.32627050\n",
      "Iteration 3, loss = 1.25726511\n",
      "Iteration 4, loss = 1.23610770\n",
      "Iteration 5, loss = 1.22878124\n",
      "Iteration 6, loss = 1.21609691\n",
      "Iteration 7, loss = 1.21078812\n",
      "Iteration 8, loss = 1.20462797\n",
      "Iteration 9, loss = 1.20080108\n",
      "Iteration 10, loss = 1.19789115\n",
      "Iteration 11, loss = 1.19940746\n",
      "Iteration 12, loss = 1.19836876\n",
      "Iteration 13, loss = 1.19296933\n",
      "Iteration 14, loss = 1.19255930\n",
      "Iteration 15, loss = 1.19160043\n",
      "Iteration 16, loss = 1.19237259\n",
      "Iteration 17, loss = 1.18910064\n",
      "Iteration 18, loss = 1.19081473\n",
      "Iteration 19, loss = 1.19064725\n",
      "Iteration 20, loss = 1.18665783\n",
      "Iteration 21, loss = 1.18482598\n",
      "Iteration 22, loss = 1.18524665\n",
      "Iteration 23, loss = 1.18489835\n",
      "Iteration 24, loss = 1.18168060\n",
      "Iteration 25, loss = 1.18285445\n",
      "Iteration 26, loss = 1.18580933\n",
      "Iteration 27, loss = 1.18226732\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.93027523\n",
      "Iteration 2, loss = 1.32611350\n",
      "Iteration 3, loss = 1.25950182\n",
      "Iteration 4, loss = 1.23631181\n",
      "Iteration 5, loss = 1.22834217\n",
      "Iteration 6, loss = 1.21837947\n",
      "Iteration 7, loss = 1.21255858\n",
      "Iteration 8, loss = 1.21095543\n",
      "Iteration 9, loss = 1.20634359\n",
      "Iteration 10, loss = 1.20087525\n",
      "Iteration 11, loss = 1.20094072\n",
      "Iteration 12, loss = 1.19959094\n",
      "Iteration 13, loss = 1.19543926\n",
      "Iteration 14, loss = 1.19376875\n",
      "Iteration 15, loss = 1.19428202\n",
      "Iteration 16, loss = 1.19251624\n",
      "Iteration 17, loss = 1.18934245\n",
      "Iteration 18, loss = 1.19143538\n",
      "Iteration 19, loss = 1.18993812\n",
      "Iteration 20, loss = 1.18898222\n",
      "Iteration 21, loss = 1.18836530\n",
      "Iteration 22, loss = 1.18601241\n",
      "Iteration 23, loss = 1.18975401\n",
      "Iteration 24, loss = 1.18457684\n",
      "Iteration 25, loss = 1.18451642\n",
      "Iteration 26, loss = 1.18742614\n",
      "Iteration 27, loss = 1.18515825\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.93349958\n",
      "Iteration 2, loss = 1.33745818\n",
      "Iteration 3, loss = 1.26579896\n",
      "Iteration 4, loss = 1.23894224\n",
      "Iteration 5, loss = 1.22691000\n",
      "Iteration 6, loss = 1.21677384\n",
      "Iteration 7, loss = 1.20950150\n",
      "Iteration 8, loss = 1.20557064\n",
      "Iteration 9, loss = 1.20540000\n",
      "Iteration 10, loss = 1.20444047\n",
      "Iteration 11, loss = 1.19948771\n",
      "Iteration 12, loss = 1.19656728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 1.19377987\n",
      "Iteration 14, loss = 1.19303035\n",
      "Iteration 15, loss = 1.19012110\n",
      "Iteration 16, loss = 1.19114719\n",
      "Iteration 17, loss = 1.18838427\n",
      "Iteration 18, loss = 1.18955226\n",
      "Iteration 19, loss = 1.19047672\n",
      "Iteration 20, loss = 1.18765456\n",
      "Iteration 21, loss = 1.18749798\n",
      "Iteration 22, loss = 1.18612620\n",
      "Iteration 23, loss = 1.18470441\n",
      "Iteration 24, loss = 1.18395587\n",
      "Iteration 25, loss = 1.18323822\n",
      "Iteration 26, loss = 1.18249510\n",
      "Iteration 27, loss = 1.18194363\n",
      "Iteration 28, loss = 1.18096366\n",
      "Iteration 29, loss = 1.18189813\n",
      "Iteration 30, loss = 1.18562866\n",
      "Iteration 31, loss = 1.17956758\n",
      "Iteration 32, loss = 1.17963319\n",
      "Iteration 33, loss = 1.17971505\n",
      "Iteration 34, loss = 1.18054878\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58431533\n",
      "Iteration 2, loss = 0.43753023\n",
      "Iteration 3, loss = 0.40068233\n",
      "Iteration 4, loss = 0.37685333\n",
      "Iteration 5, loss = 0.35856120\n",
      "Iteration 6, loss = 0.34578930\n",
      "Iteration 7, loss = 0.33510288\n",
      "Iteration 8, loss = 0.32574989\n",
      "Iteration 9, loss = 0.32203364\n",
      "Iteration 10, loss = 0.31069245\n",
      "Iteration 11, loss = 0.30797276\n",
      "Iteration 12, loss = 0.29913928\n",
      "Iteration 13, loss = 0.29395424\n",
      "Iteration 14, loss = 0.28972217\n",
      "Iteration 15, loss = 0.28639496\n",
      "Iteration 16, loss = 0.28272160\n",
      "Iteration 17, loss = 0.27848591\n",
      "Iteration 18, loss = 0.28044533\n",
      "Iteration 19, loss = 0.27155576\n",
      "Iteration 20, loss = 0.27129146\n",
      "Iteration 21, loss = 0.26471120\n",
      "Iteration 22, loss = 0.26345647\n",
      "Iteration 23, loss = 0.26459143\n",
      "Iteration 24, loss = 0.25892353\n",
      "Iteration 25, loss = 0.25697508\n",
      "Iteration 26, loss = 0.25351625\n",
      "Iteration 27, loss = 0.25343404\n",
      "Iteration 28, loss = 0.24879995\n",
      "Iteration 29, loss = 0.24624827\n",
      "Iteration 30, loss = 0.24422664\n",
      "Iteration 31, loss = 0.24617849\n",
      "Iteration 32, loss = 0.24062990\n",
      "Iteration 33, loss = 0.24221761\n",
      "Iteration 34, loss = 0.23917393\n",
      "Iteration 35, loss = 0.23551562\n",
      "Iteration 36, loss = 0.23569101\n",
      "Iteration 37, loss = 0.23469394\n",
      "Iteration 38, loss = 0.23101895\n",
      "Iteration 39, loss = 0.23223439\n",
      "Iteration 40, loss = 0.22851172\n",
      "Iteration 41, loss = 0.23134848\n",
      "Iteration 42, loss = 0.22664344\n",
      "Iteration 43, loss = 0.22553047\n",
      "Iteration 44, loss = 0.22821689\n",
      "Iteration 45, loss = 0.22345945\n",
      "Iteration 46, loss = 0.22112303\n",
      "Iteration 47, loss = 0.21997473\n",
      "Iteration 48, loss = 0.21982349\n",
      "Iteration 49, loss = 0.21910052\n",
      "Iteration 50, loss = 0.21806814\n",
      "Iteration 51, loss = 0.21831429\n",
      "Iteration 52, loss = 0.21239238\n",
      "Iteration 53, loss = 0.21845727\n",
      "Iteration 54, loss = 0.21415828\n",
      "Iteration 55, loss = 0.21062232\n",
      "Iteration 56, loss = 0.21269791\n",
      "Iteration 57, loss = 0.21097696\n",
      "Iteration 58, loss = 0.21087372\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,\n",
       "       warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'hidden_layer_sizes': [(280,), 140, (280, 280), (140, 140), (280, 140)], 'activation': ('logistic', 'tanh', 'relu'), 'alpha': [0.01, 0.03, 0.1, 0.3, 0.6, 1, 3, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search NN\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'hidden_layer_sizes':[(280,),(140),(280, 280), (140, 140), (280, 140)],\\\n",
    "              'activation':('logistic', 'tanh', 'relu'), 'alpha':[0.01, 0.03, 0.1, 0.3, 0.6, 1, 3, 10]}\n",
    "nn = MLPClassifier(max_iter=100, verbose=True, random_state=1)\n",
    "clf6 = GridSearchCV(nn, parameters)\n",
    "clf6.fit(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = clf6.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu', 'alpha': 0.03, 'hidden_layer_sizes': (280,)}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = clf6.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89278333333333337"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## best score: `0.8927` with `alpha=0.03`, `hidden_layer_sizes=(280,)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]0.8505\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf4 = SVC(max_iter=50000, verbose=True, kernel='rbf', random_state=1)\n",
    "clf4.fit(X_train, y_train) \n",
    "score4 = clf4.score(X_val, y_val)\n",
    "print(score4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## score: `0.8505` with `50 000` iterations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
